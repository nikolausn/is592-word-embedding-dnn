{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUID</th>\n",
       "      <th>Last</th>\n",
       "      <th>First</th>\n",
       "      <th>Ethnea</th>\n",
       "      <th>Genni</th>\n",
       "      <th>PubCountry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12872768_1</td>\n",
       "      <td>_Filho_</td>\n",
       "      <td>_Elias_Abdalla_</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>M</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12565879_3</td>\n",
       "      <td>_Bou_Abdallah_</td>\n",
       "      <td>_Jad_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17569133_1</td>\n",
       "      <td>_Abdel_Aziz_</td>\n",
       "      <td>_Ayman_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11213157_2</td>\n",
       "      <td>_Abdelmoula_</td>\n",
       "      <td>_Salma_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>F</td>\n",
       "      <td>Tunisia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11439058_1</td>\n",
       "      <td>_Abdou_</td>\n",
       "      <td>_Ibrahim_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>Egypt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10919608_3</td>\n",
       "      <td>_Abou_El_Fettouh_</td>\n",
       "      <td>_Hazem_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17314740_3</td>\n",
       "      <td>_Aboutaam_</td>\n",
       "      <td>_Rola_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>F</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16564217_2</td>\n",
       "      <td>_Alvim_de_Abreu_Silva_Rodrigues_</td>\n",
       "      <td>_Aida_Alexandra_</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>F</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6339395_2</td>\n",
       "      <td>_Cristina_Affonso_Scaletsky_</td>\n",
       "      <td>_Isabel_</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>F</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7796089_2</td>\n",
       "      <td>_Agbenyega_</td>\n",
       "      <td>_Tsiri_</td>\n",
       "      <td>AFRICAN</td>\n",
       "      <td>-</td>\n",
       "      <td>Ghana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5473023_2</td>\n",
       "      <td>_Maria_Aguado_Garcia_</td>\n",
       "      <td>_Jose_</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>M</td>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15153355_4</td>\n",
       "      <td>_Aguilar_Guisado_</td>\n",
       "      <td>_Manuela_</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>F</td>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11981185_4</td>\n",
       "      <td>_Aguillon_Luna_</td>\n",
       "      <td>_Arturo_</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>M</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12554913_2</td>\n",
       "      <td>_Ahmadzadeh_</td>\n",
       "      <td>_Ali_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>Iran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10066026_5</td>\n",
       "      <td>_Ibrahim_</td>\n",
       "      <td>_Ahmed_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>UnitedArabEmirates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17642770_2</td>\n",
       "      <td>_Letaief_</td>\n",
       "      <td>_Ahmed_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>Tunisia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14766995_2</td>\n",
       "      <td>_Shakeel_Ahmed_</td>\n",
       "      <td>_Mohammed_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16343417_1</td>\n",
       "      <td>_Aksoy_Dogan_</td>\n",
       "      <td>_Alev_</td>\n",
       "      <td>TURKISH</td>\n",
       "      <td>F</td>\n",
       "      <td>Turkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3866040_3</td>\n",
       "      <td>_Nakatsukasa_Akune_</td>\n",
       "      <td>_Mitsumi_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>-</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12663244_3</td>\n",
       "      <td>_Akyuz_</td>\n",
       "      <td>_Filiz_</td>\n",
       "      <td>TURKISH</td>\n",
       "      <td>F</td>\n",
       "      <td>Turkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2830628_1</td>\n",
       "      <td>_Al_Gohary_</td>\n",
       "      <td>_Omaimah_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>F</td>\n",
       "      <td>SaudiArabia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12113661_3</td>\n",
       "      <td>_Al_Jedah_</td>\n",
       "      <td>_Jassim_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>Qatar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3104205_7</td>\n",
       "      <td>_Al_Khatib_</td>\n",
       "      <td>_Shadi_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>-</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7520609_3</td>\n",
       "      <td>_Al_Najjar_</td>\n",
       "      <td>_Azmi_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>927746_1</td>\n",
       "      <td>_Alba_Conejo_</td>\n",
       "      <td>_Emilio_</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>M</td>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>18451155_4</td>\n",
       "      <td>_Llaguno_</td>\n",
       "      <td>_Sheila_Alcantara_</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>F</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9431676_4</td>\n",
       "      <td>_Alcocer_</td>\n",
       "      <td>_Marcos_</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>M</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3720520_1</td>\n",
       "      <td>_Grazia_Alessandri_</td>\n",
       "      <td>_Maria_</td>\n",
       "      <td>ITALIAN</td>\n",
       "      <td>F</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10480445_2</td>\n",
       "      <td>_Ali_Labib_</td>\n",
       "      <td>_Randa_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>F</td>\n",
       "      <td>Egypt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12636048_1</td>\n",
       "      <td>_Allie_Hamdulay_</td>\n",
       "      <td>_Shameez_</td>\n",
       "      <td>AFRICAN</td>\n",
       "      <td>-</td>\n",
       "      <td>SouthAfrica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44537</th>\n",
       "      <td>12641914_1</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Zhihong_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44538</th>\n",
       "      <td>16892144_4</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Zhen_ying_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44539</th>\n",
       "      <td>15566960_5</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Zhu_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44540</th>\n",
       "      <td>12828461_3</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Zhengjie_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44541</th>\n",
       "      <td>12726928_3</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Zhengchun_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44542</th>\n",
       "      <td>17802886_5</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Ziliang_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44543</th>\n",
       "      <td>18032929_3</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Zhe_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44544</th>\n",
       "      <td>16690641_1</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Zulian_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44545</th>\n",
       "      <td>17554931_2</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Zun_chun_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44546</th>\n",
       "      <td>18975769_5</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Ze_zhou_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44547</th>\n",
       "      <td>19458823_2</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Zhaochun_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44548</th>\n",
       "      <td>8734067_3</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_ziying_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>M</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44549</th>\n",
       "      <td>9706530_4</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_zhenyu_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>M</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44550</th>\n",
       "      <td>6068528_4</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_kenzo_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44551</th>\n",
       "      <td>180046_3</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_kimiko_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>F</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44552</th>\n",
       "      <td>1942669_3</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_kazuhiro_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44553</th>\n",
       "      <td>9571188_3</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_koji_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44554</th>\n",
       "      <td>11071249_4</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_Kousei_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44555</th>\n",
       "      <td>12828613_4</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_Kyoko_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>F</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44556</th>\n",
       "      <td>19073144_1</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_Kenji_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44557</th>\n",
       "      <td>8449402_2</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_keiji_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44558</th>\n",
       "      <td>10671491_5</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_keiji_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44559</th>\n",
       "      <td>18283803_5</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_Kei_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>-</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44560</th>\n",
       "      <td>8913105_4</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_kiyoshi_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44561</th>\n",
       "      <td>6678271_2</td>\n",
       "      <td>_Sasaki_</td>\n",
       "      <td>_takashi_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44562</th>\n",
       "      <td>3097163_3</td>\n",
       "      <td>_Sasaki_</td>\n",
       "      <td>_toyoshi_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44563</th>\n",
       "      <td>3800425_1</td>\n",
       "      <td>_Sasaki_</td>\n",
       "      <td>_tetsuo_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44564</th>\n",
       "      <td>9714755_2</td>\n",
       "      <td>_Sasaki_</td>\n",
       "      <td>_takayo_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>F</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44565</th>\n",
       "      <td>7880090_2</td>\n",
       "      <td>_Haddad_</td>\n",
       "      <td>_may_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>-</td>\n",
       "      <td>SaudiArabia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44566</th>\n",
       "      <td>16865091_1</td>\n",
       "      <td>_Haddad_</td>\n",
       "      <td>_Marianne_</td>\n",
       "      <td>FRENCH</td>\n",
       "      <td>F</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44567 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             AUID                              Last               First  \\\n",
       "0      12872768_1                           _Filho_     _Elias_Abdalla_   \n",
       "1      12565879_3                    _Bou_Abdallah_               _Jad_   \n",
       "2      17569133_1                      _Abdel_Aziz_             _Ayman_   \n",
       "3      11213157_2                      _Abdelmoula_             _Salma_   \n",
       "4      11439058_1                           _Abdou_           _Ibrahim_   \n",
       "5      10919608_3                 _Abou_El_Fettouh_             _Hazem_   \n",
       "6      17314740_3                        _Aboutaam_              _Rola_   \n",
       "7      16564217_2  _Alvim_de_Abreu_Silva_Rodrigues_    _Aida_Alexandra_   \n",
       "8       6339395_2      _Cristina_Affonso_Scaletsky_            _Isabel_   \n",
       "9       7796089_2                       _Agbenyega_             _Tsiri_   \n",
       "10      5473023_2             _Maria_Aguado_Garcia_              _Jose_   \n",
       "11     15153355_4                 _Aguilar_Guisado_           _Manuela_   \n",
       "12     11981185_4                   _Aguillon_Luna_            _Arturo_   \n",
       "13     12554913_2                      _Ahmadzadeh_               _Ali_   \n",
       "14     10066026_5                         _Ibrahim_             _Ahmed_   \n",
       "15     17642770_2                         _Letaief_             _Ahmed_   \n",
       "16     14766995_2                   _Shakeel_Ahmed_          _Mohammed_   \n",
       "17     16343417_1                     _Aksoy_Dogan_              _Alev_   \n",
       "18      3866040_3               _Nakatsukasa_Akune_           _Mitsumi_   \n",
       "19     12663244_3                           _Akyuz_             _Filiz_   \n",
       "20      2830628_1                       _Al_Gohary_           _Omaimah_   \n",
       "21     12113661_3                        _Al_Jedah_            _Jassim_   \n",
       "22      3104205_7                       _Al_Khatib_             _Shadi_   \n",
       "23      7520609_3                       _Al_Najjar_              _Azmi_   \n",
       "24       927746_1                     _Alba_Conejo_            _Emilio_   \n",
       "25     18451155_4                         _Llaguno_  _Sheila_Alcantara_   \n",
       "26      9431676_4                         _Alcocer_            _Marcos_   \n",
       "27      3720520_1               _Grazia_Alessandri_             _Maria_   \n",
       "28     10480445_2                       _Ali_Labib_             _Randa_   \n",
       "29     12636048_1                  _Allie_Hamdulay_           _Shameez_   \n",
       "...           ...                               ...                 ...   \n",
       "44537  12641914_1                             _Liu_           _Zhihong_   \n",
       "44538  16892144_4                             _Liu_         _Zhen_ying_   \n",
       "44539  15566960_5                             _Liu_               _Zhu_   \n",
       "44540  12828461_3                             _Liu_          _Zhengjie_   \n",
       "44541  12726928_3                             _Liu_         _Zhengchun_   \n",
       "44542  17802886_5                             _Liu_           _Ziliang_   \n",
       "44543  18032929_3                             _Liu_               _Zhe_   \n",
       "44544  16690641_1                             _Liu_            _Zulian_   \n",
       "44545  17554931_2                             _Liu_          _Zun_chun_   \n",
       "44546  18975769_5                             _Liu_           _Ze_zhou_   \n",
       "44547  19458823_2                             _Liu_          _Zhaochun_   \n",
       "44548   8734067_3                             _Liu_            _ziying_   \n",
       "44549   9706530_4                             _Liu_            _zhenyu_   \n",
       "44550   6068528_4                          _Tanaka_             _kenzo_   \n",
       "44551    180046_3                          _Tanaka_            _kimiko_   \n",
       "44552   1942669_3                          _Tanaka_          _kazuhiro_   \n",
       "44553   9571188_3                          _Tanaka_              _koji_   \n",
       "44554  11071249_4                          _Tanaka_            _Kousei_   \n",
       "44555  12828613_4                          _Tanaka_             _Kyoko_   \n",
       "44556  19073144_1                          _Tanaka_             _Kenji_   \n",
       "44557   8449402_2                          _Tanaka_             _keiji_   \n",
       "44558  10671491_5                          _Tanaka_             _keiji_   \n",
       "44559  18283803_5                          _Tanaka_               _Kei_   \n",
       "44560   8913105_4                          _Tanaka_           _kiyoshi_   \n",
       "44561   6678271_2                          _Sasaki_           _takashi_   \n",
       "44562   3097163_3                          _Sasaki_           _toyoshi_   \n",
       "44563   3800425_1                          _Sasaki_            _tetsuo_   \n",
       "44564   9714755_2                          _Sasaki_            _takayo_   \n",
       "44565   7880090_2                          _Haddad_               _may_   \n",
       "44566  16865091_1                          _Haddad_          _Marianne_   \n",
       "\n",
       "         Ethnea Genni          PubCountry  \n",
       "0      HISPANIC     M              Brazil  \n",
       "1          ARAB     M              France  \n",
       "2          ARAB     M                 USA  \n",
       "3          ARAB     F             Tunisia  \n",
       "4          ARAB     M               Egypt  \n",
       "5          ARAB     M                 USA  \n",
       "6          ARAB     F              France  \n",
       "7      HISPANIC     F              Brazil  \n",
       "8      HISPANIC     F              Brazil  \n",
       "9       AFRICAN     -               Ghana  \n",
       "10     HISPANIC     M               Spain  \n",
       "11     HISPANIC     F               Spain  \n",
       "12     HISPANIC     M                 USA  \n",
       "13         ARAB     M                Iran  \n",
       "14         ARAB     M  UnitedArabEmirates  \n",
       "15         ARAB     M             Tunisia  \n",
       "16         ARAB     M                 USA  \n",
       "17      TURKISH     F              Turkey  \n",
       "18     JAPANESE     -               Japan  \n",
       "19      TURKISH     F              Turkey  \n",
       "20         ARAB     F         SaudiArabia  \n",
       "21         ARAB     M               Qatar  \n",
       "22         ARAB     -               India  \n",
       "23         ARAB     M              France  \n",
       "24     HISPANIC     M               Spain  \n",
       "25     HISPANIC     F                 USA  \n",
       "26     HISPANIC     M                  UK  \n",
       "27      ITALIAN     F               Italy  \n",
       "28         ARAB     F               Egypt  \n",
       "29      AFRICAN     -         SouthAfrica  \n",
       "...         ...   ...                 ...  \n",
       "44537   CHINESE     -               China  \n",
       "44538   CHINESE     -               China  \n",
       "44539   CHINESE     -               China  \n",
       "44540   CHINESE     -                 USA  \n",
       "44541   CHINESE     -               China  \n",
       "44542   CHINESE     -               China  \n",
       "44543   CHINESE     -               China  \n",
       "44544   CHINESE     -                  UK  \n",
       "44545   CHINESE     -               China  \n",
       "44546   CHINESE     -               China  \n",
       "44547   CHINESE     -               China  \n",
       "44548   CHINESE     M                 USA  \n",
       "44549   CHINESE     M               China  \n",
       "44550  JAPANESE     M               Japan  \n",
       "44551  JAPANESE     F                 USA  \n",
       "44552  JAPANESE     M               Japan  \n",
       "44553  JAPANESE     M               Japan  \n",
       "44554  JAPANESE     M               Japan  \n",
       "44555  JAPANESE     F               Japan  \n",
       "44556  JAPANESE     M               Japan  \n",
       "44557  JAPANESE     M               Japan  \n",
       "44558  JAPANESE     M               Japan  \n",
       "44559  JAPANESE     -               Japan  \n",
       "44560  JAPANESE     M               Japan  \n",
       "44561  JAPANESE     M               Japan  \n",
       "44562  JAPANESE     M               Japan  \n",
       "44563  JAPANESE     M               Japan  \n",
       "44564  JAPANESE     F               Japan  \n",
       "44565      ARAB     -         SaudiArabia  \n",
       "44566    FRENCH     F              France  \n",
       "\n",
       "[44567 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethnea_df = pd.read_csv('names_ethnea_genni_country_sample.csv')\n",
    "ethnea_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First thing first, use the character feature for making the dnn model\n",
    "def extract_structure(word,n_char=2):\n",
    "    x_struct = []\n",
    "    word_len = len(word) + n_char\n",
    "    n_char-=1\n",
    "    counter = 0\n",
    "    for i in range(word_len):\n",
    "        end = i+1\n",
    "        start = (i - n_char) if (i - n_char) > 0 else 0\n",
    "        if word[start:end]!='_' and word[start:end]!='':\n",
    "        #if word[start:end]!='_':\n",
    "            x_struct.append(word[start:end])\n",
    "    return x_struct\n",
    "\n",
    "first_name_struct = ethnea_df.First.apply(lambda x: extract_structure(x.lower(),3))\n",
    "last_name_struct = ethnea_df.Last.apply(lambda x: extract_structure(x.lower(),3))\n",
    "                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [_e, _el, eli, lia, ias, as_, s_a, _ab, abd, b...\n",
       "1                                  [_j, _ja, jad, ad_, d_]\n",
       "2                        [_a, _ay, aym, yma, man, an_, n_]\n",
       "3                        [_s, _sa, sal, alm, lma, ma_, a_]\n",
       "4              [_i, _ib, ibr, bra, rah, ahi, him, im_, m_]\n",
       "5                        [_h, _ha, haz, aze, zem, em_, m_]\n",
       "6                             [_r, _ro, rol, ola, la_, a_]\n",
       "7        [_a, _ai, aid, ida, da_, a_a, _al, ale, lex, e...\n",
       "8                   [_i, _is, isa, sab, abe, bel, el_, l_]\n",
       "9                        [_t, _ts, tsi, sir, iri, ri_, i_]\n",
       "10                            [_j, _jo, jos, ose, se_, e_]\n",
       "11             [_m, _ma, man, anu, nue, uel, ela, la_, a_]\n",
       "12                  [_a, _ar, art, rtu, tur, uro, ro_, o_]\n",
       "13                                 [_a, _al, ali, li_, i_]\n",
       "14                       [_a, _ah, ahm, hme, med, ed_, d_]\n",
       "15                       [_a, _ah, ahm, hme, med, ed_, d_]\n",
       "16        [_m, _mo, moh, oha, ham, amm, mme, med, ed_, d_]\n",
       "17                            [_a, _al, ale, lev, ev_, v_]\n",
       "18             [_m, _mi, mit, its, tsu, sum, umi, mi_, i_]\n",
       "19                       [_f, _fi, fil, ili, liz, iz_, z_]\n",
       "20             [_o, _om, oma, mai, aim, ima, mah, ah_, h_]\n",
       "21                  [_j, _ja, jas, ass, ssi, sim, im_, m_]\n",
       "22                       [_s, _sh, sha, had, adi, di_, i_]\n",
       "23                            [_a, _az, azm, zmi, mi_, i_]\n",
       "24                  [_e, _em, emi, mil, ili, lio, io_, o_]\n",
       "25       [_s, _sh, she, hei, eil, ila, la_, a_a, _al, a...\n",
       "26                  [_m, _ma, mar, arc, rco, cos, os_, s_]\n",
       "27                       [_m, _ma, mar, ari, ria, ia_, a_]\n",
       "28                       [_r, _ra, ran, and, nda, da_, a_]\n",
       "29             [_s, _sh, sha, ham, ame, mee, eez, ez_, z_]\n",
       "                               ...                        \n",
       "44537          [_z, _zh, zhi, hih, iho, hon, ong, ng_, g_]\n",
       "44538    [_z, _zh, zhe, hen, en_, n_y, _yi, yin, ing, n...\n",
       "44539                              [_z, _zh, zhu, hu_, u_]\n",
       "44540     [_z, _zh, zhe, hen, eng, ngj, gji, jie, ie_, e_]\n",
       "44541    [_z, _zh, zhe, hen, eng, ngc, gch, chu, hun, u...\n",
       "44542          [_z, _zi, zil, ili, lia, ian, ang, ng_, g_]\n",
       "44543                              [_z, _zh, zhe, he_, e_]\n",
       "44544               [_z, _zu, zul, uli, lia, ian, an_, n_]\n",
       "44545     [_z, _zu, zun, un_, n_c, _ch, chu, hun, un_, n_]\n",
       "44546          [_z, _ze, ze_, e_z, _zh, zho, hou, ou_, u_]\n",
       "44547     [_z, _zh, zha, hao, aoc, och, chu, hun, un_, n_]\n",
       "44548               [_z, _zi, ziy, iyi, yin, ing, ng_, g_]\n",
       "44549               [_z, _zh, zhe, hen, eny, nyu, yu_, u_]\n",
       "44550                    [_k, _ke, ken, enz, nzo, zo_, o_]\n",
       "44551               [_k, _ki, kim, imi, mik, iko, ko_, o_]\n",
       "44552     [_k, _ka, kaz, azu, zuh, uhi, hir, iro, ro_, o_]\n",
       "44553                         [_k, _ko, koj, oji, ji_, i_]\n",
       "44554               [_k, _ko, kou, ous, use, sei, ei_, i_]\n",
       "44555                    [_k, _ky, kyo, yok, oko, ko_, o_]\n",
       "44556                    [_k, _ke, ken, enj, nji, ji_, i_]\n",
       "44557                    [_k, _ke, kei, eij, iji, ji_, i_]\n",
       "44558                    [_k, _ke, kei, eij, iji, ji_, i_]\n",
       "44559                              [_k, _ke, kei, ei_, i_]\n",
       "44560          [_k, _ki, kiy, iyo, yos, osh, shi, hi_, i_]\n",
       "44561          [_t, _ta, tak, aka, kas, ash, shi, hi_, i_]\n",
       "44562          [_t, _to, toy, oyo, yos, osh, shi, hi_, i_]\n",
       "44563               [_t, _te, tet, ets, tsu, suo, uo_, o_]\n",
       "44564               [_t, _ta, tak, aka, kay, ayo, yo_, o_]\n",
       "44565                              [_m, _ma, may, ay_, y_]\n",
       "44566     [_m, _ma, mar, ari, ria, ian, ann, nne, ne_, e_]\n",
       "Name: First, Length: 44567, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_name_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make struct dictionary\n",
    "struct_dict = {}\n",
    "for name_struct_i in first_name_struct:\n",
    "    for struct_j in name_struct_i:\n",
    "        if struct_j not in struct_dict:\n",
    "            struct_dict[struct_j]=0\n",
    "        struct_dict[struct_j]+=1\n",
    "for name_struct_i in last_name_struct:\n",
    "    for struct_j in name_struct_i:\n",
    "        if struct_j not in struct_dict:\n",
    "            struct_dict[struct_j]=0\n",
    "        struct_dict[struct_j]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "struct_dict_keys = list(struct_dict.keys())\n",
    "ethnic_series = ethnea_df['Ethnea'].str.lower()\n",
    "ethnic_keys = list(np.unique(ethnic_series.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load test train data\n",
    "with open('train_test_fix_index.pickle', 'rb') as f:\n",
    "    trainIndex,testIndex,trainX,trainY,testX,testY,ethnic_keys,tmp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform the dataset into structure\n",
    "def transform_structure(name_struct):\n",
    "    list_structure = []\n",
    "    for x in name_struct:\n",
    "        try:\n",
    "            list_structure.append(struct_dict_keys.index(x)+1)\n",
    "        except BaseException:\n",
    "            list_structure.append(0)\n",
    "    #add pading 0 for structure less than num_input\n",
    "    #for i in range(len(list_structure),timesteps):\n",
    "    #    list_structure.append(0)\n",
    "    return list_structure   \n",
    "    #return [*map(lambda x:struct_dict_keys.index(x)+1, name_struct)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_name_ds = first_name_struct.apply(lambda x:transform_structure(x))\n",
    "last_name_ds = last_name_struct.apply(lambda x:transform_structure(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8626"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(struct_dict_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(x):\n",
    "    y = np.zeros(len(ethnic_keys))\n",
    "    y[ethnic_keys.index(x)]=1\n",
    "    return y\n",
    "\n",
    "labels = np.array(list(map(lambda x: transform_labels(x),ethnic_series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...\n",
       "1                                     [16, 17, 18, 19, 20]\n",
       "2                             [21, 22, 23, 24, 25, 26, 27]\n",
       "3                             [28, 29, 30, 31, 32, 33, 15]\n",
       "4                     [34, 35, 36, 37, 38, 39, 40, 41, 42]\n",
       "5                             [43, 44, 45, 46, 47, 48, 42]\n",
       "6                                 [49, 50, 51, 52, 14, 15]\n",
       "7        [21, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 6...\n",
       "8                         [34, 67, 68, 69, 70, 71, 72, 73]\n",
       "9                             [74, 75, 76, 77, 78, 79, 80]\n",
       "10                                [16, 81, 82, 83, 84, 85]\n",
       "11                    [86, 87, 25, 88, 89, 90, 91, 14, 15]\n",
       "12                        [21, 92, 93, 94, 95, 96, 97, 98]\n",
       "13                                   [21, 58, 99, 100, 80]\n",
       "14                       [21, 101, 102, 103, 104, 105, 20]\n",
       "15                       [21, 101, 102, 103, 104, 105, 20]\n",
       "16        [86, 106, 107, 108, 109, 110, 111, 104, 105, 20]\n",
       "17                             [21, 58, 59, 112, 113, 114]\n",
       "18             [86, 115, 116, 117, 118, 119, 120, 121, 80]\n",
       "19                     [122, 123, 124, 125, 126, 127, 128]\n",
       "20           [129, 130, 131, 132, 133, 134, 135, 136, 137]\n",
       "21                    [16, 17, 138, 139, 140, 141, 41, 42]\n",
       "22                       [28, 142, 143, 144, 145, 146, 80]\n",
       "23                            [21, 147, 148, 149, 121, 80]\n",
       "24                   [1, 150, 151, 152, 125, 153, 154, 98]\n",
       "25       [28, 142, 155, 156, 157, 158, 14, 57, 58, 159,...\n",
       "26                  [86, 87, 166, 167, 168, 169, 170, 171]\n",
       "27                        [86, 87, 166, 172, 173, 174, 15]\n",
       "28                         [49, 175, 176, 63, 177, 56, 15]\n",
       "29            [28, 142, 143, 109, 178, 179, 180, 181, 128]\n",
       "                               ...                        \n",
       "44537     [875, 876, 1105, 2227, 3351, 878, 879, 811, 818]\n",
       "44538    [875, 876, 1881, 1079, 208, 1816, 292, 816, 81...\n",
       "44539                          [875, 876, 3463, 1767, 457]\n",
       "44540    [875, 876, 1881, 1079, 410, 2657, 2658, 2659, ...\n",
       "44541    [875, 876, 1881, 1079, 410, 2414, 2415, 1802, ...\n",
       "44542        [875, 2067, 3166, 125, 4, 441, 195, 811, 818]\n",
       "44543                            [875, 876, 1881, 942, 85]\n",
       "44544                [875, 1628, 479, 619, 4, 441, 26, 27]\n",
       "44545    [875, 1628, 4254, 610, 795, 776, 1802, 1803, 6...\n",
       "44546    [875, 1958, 1959, 3913, 876, 877, 1189, 1190, ...\n",
       "44547    [875, 876, 1243, 845, 5270, 2480, 1802, 1803, ...\n",
       "44548          [875, 2067, 2068, 3556, 816, 817, 811, 818]\n",
       "44549         [875, 876, 1881, 1079, 2252, 3945, 844, 457]\n",
       "44550               [238, 832, 1387, 1159, 1160, 1161, 98]\n",
       "44551         [238, 1380, 1381, 2013, 1725, 1437, 664, 98]\n",
       "44552    [238, 239, 1542, 1543, 1544, 1545, 1547, 1548,...\n",
       "44553                     [238, 638, 3948, 2553, 1639, 80]\n",
       "44554           [238, 638, 1601, 2908, 613, 1186, 548, 80]\n",
       "44555               [238, 3073, 3230, 3199, 1896, 664, 98]\n",
       "44556               [238, 832, 1387, 1746, 2523, 1639, 80]\n",
       "44557               [238, 832, 1796, 3233, 1106, 1639, 80]\n",
       "44558               [238, 832, 1796, 3233, 1106, 1639, 80]\n",
       "44559                            [238, 832, 1796, 548, 80]\n",
       "44560    [238, 1380, 3178, 2950, 1378, 1418, 988, 1419,...\n",
       "44561       [74, 935, 1581, 1582, 429, 987, 988, 1419, 80]\n",
       "44562     [74, 983, 3958, 1770, 1378, 1418, 988, 1419, 80]\n",
       "44563          [74, 1227, 3051, 2197, 118, 2860, 1120, 98]\n",
       "44564          [74, 935, 1581, 1582, 2133, 2702, 3231, 98]\n",
       "44565                            [86, 87, 1310, 2303, 332]\n",
       "44566      [86, 87, 166, 172, 173, 441, 301, 442, 443, 85]\n",
       "Name: First, Length: 44567, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_name_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "# max sequence length\n",
    "seq_length = 100\n",
    "# multi input with single output\n",
    "\n",
    "# first name input\n",
    "first_name_input = Input(shape=(len(struct_dict_keys),),name='first_name_input')\n",
    "last_name_input = Input(shape=(len(struct_dict_keys),),name='last_name_input')\n",
    "\n",
    "# first tensor for first name\n",
    "first_name_l = Dense(units=1000)(first_name_input)\n",
    "last_name_l = Dense(units=1000)(last_name_input)\n",
    "\n",
    "# merge the two layer together\n",
    "x = keras.layers.concatenate([first_name_l,last_name_l])\n",
    "\n",
    "# stack dense network for memory\n",
    "x = Dense(1000, activation='relu')(x)\n",
    "x = Dense(500, activation='relu')(x)\n",
    "output_l = Dense(len(ethnic_keys),activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[first_name_input, last_name_input], outputs=[output_l])\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_name_ds_mat = np.zeros((len(first_name_ds),len(struct_dict_keys)),dtype=np.int32)\n",
    "for i,x in enumerate(first_name_ds):\n",
    "    for y in x:\n",
    "        first_name_ds_mat[i,y-1]+=1\n",
    "last_name_ds_mat = np.zeros((len(last_name_ds),len(struct_dict_keys)),dtype=np.int32)\n",
    "for i,x in enumerate(last_name_ds):\n",
    "    for y in x:\n",
    "        last_name_ds_mat[i,y-1]+=1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "first_name_input (InputLayer)    (None, 8626)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "last_name_input (InputLayer)     (None, 8626)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1000)          8627000     first_name_input[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1000)          8627000     last_name_input[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 2000)          0           dense_1[0][0]                    \n",
      "                                                                   dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1000)          2001000     concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 500)           500500      dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 23)            11523       dense_4[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 19,767,023\n",
      "Trainable params: 19,767,023\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 296s - loss: 1.2064 - acc: 0.6731   \n",
      "Accuracy: 84.33%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 289s - loss: 0.3336 - acc: 0.9012   \n",
      "Accuracy: 86.27%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 288s - loss: 0.1517 - acc: 0.9550   \n",
      "Accuracy: 85.88%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 289s - loss: 0.0805 - acc: 0.9759   \n",
      "Accuracy: 86.18%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 292s - loss: 0.0506 - acc: 0.9842   \n",
      "Accuracy: 85.67%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 293s - loss: 0.0352 - acc: 0.9889   \n",
      "Accuracy: 85.66%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 287s - loss: 0.0229 - acc: 0.9932   \n",
      "Accuracy: 85.72%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 286s - loss: 0.0155 - acc: 0.9956   \n",
      "Accuracy: 85.33%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 287s - loss: 0.0141 - acc: 0.9963   \n",
      "Accuracy: 85.24%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 288s - loss: 0.0122 - acc: 0.9967   \n",
      "Accuracy: 85.09%\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "first_trainX = first_name_ds_mat[trainIndex]\n",
    "first_testX = first_name_ds_mat[testIndex]\n",
    "last_trainX = last_name_ds_mat[trainIndex]\n",
    "last_testX = last_name_ds_mat[testIndex]\n",
    "trainY = labels[trainIndex]\n",
    "testY = labels[testIndex] \n",
    "\n",
    "#trainX =np.array([to_categorical(x,nb_classes=len(struct_dict_keys)+1) for x in trainX])\n",
    "#testX =np.array([to_categorical(x,nb_classes=len(struct_dict_keys)+1) for x in testX])\n",
    "\n",
    "batch_size = 1000\n",
    "input_dim = 50 \n",
    "\n",
    "for x in range(10):\n",
    "    model.fit([first_trainX, last_trainX],trainY,epochs=1,batch_size=1000)\n",
    "    scores = model.evaluate([first_testX, last_testX],testY,verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" %(scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in range(10):\n",
    "    model.fit([first_trainX, last_trainX],trainY,epochs=1,batch_size=1000)\n",
    "    scores = model.evaluate([first_testX, last_testX],testY,verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" %(scores[1]*100))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save this model atlhouth there is something wrong with the features\n",
    "# I did not lowercase the text :(\n",
    "model_json = model.to_json()\n",
    "with open('model-keras-w3-trigram.json','w') as f:\n",
    "    f.write(model_json)\n",
    "#save the last weight\n",
    "model.save_weights('model-keras-w3-trigram-10.h5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trans_name(name):\n",
    "    name = name.lower()\n",
    "    # transform space into underscore\n",
    "    name = '_'+name.replace(' ','_')+'_'\n",
    "    #transform the name into sequence structure\n",
    "    ext_name = extract_structure(name)\n",
    "    trans_name = transform_structure(ext_name)\n",
    "    name_ds_mat = np.zeros((1,len(struct_dict_keys)),dtype=np.int32)\n",
    "    for i,x in enumerate(trans_name):\n",
    "        name_ds_mat[0,x-1]+=1\n",
    "    #trans_name = pad_sequences([trans_name], maxlen=50,value=0.)\n",
    "    return name_ds_mat\n",
    "\n",
    "def predict_ethnicity(fname,lname):\n",
    "    # lower case the name\n",
    "    fnamex = trans_name(fname)\n",
    "    lnamex = trans_name(lname)\n",
    "    pred = model.predict([np.array(fnamex),np.array(lnamex)])\n",
    "    pred_class = np.argsort(pred[0])[::-1]\n",
    "    return_item = []\n",
    "    for x in np.argsort(pred[0])[::-1]:\n",
    "        return_item.append((ethnic_keys[x],pred[0][x]))\n",
    "    return return_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nordic', 0.9999404),\n",
       " ('english', 4.9915998e-05),\n",
       " ('chinese', 4.4698836e-06),\n",
       " ('african', 2.1338935e-06),\n",
       " ('indonesian', 1.2289204e-06),\n",
       " ('vietnamese', 1.0045702e-06),\n",
       " ('korean', 1.7191711e-07),\n",
       " ('turkish', 1.6410125e-07),\n",
       " ('japanese', 8.7127425e-08),\n",
       " ('german', 7.1376171e-08),\n",
       " ('french', 6.1635937e-08),\n",
       " ('hispanic', 5.854336e-08),\n",
       " ('israeli', 3.0545017e-08),\n",
       " ('hungarian', 2.0371598e-08),\n",
       " ('arab', 1.7005338e-08),\n",
       " ('greek', 1.0779336e-08),\n",
       " ('romanian', 9.6871133e-09),\n",
       " ('dutch', 4.4604915e-09),\n",
       " ('baltic', 4.4088111e-09),\n",
       " ('indian', 1.0509542e-09),\n",
       " ('slav', 1.0045313e-09),\n",
       " ('italian', 5.8645444e-10),\n",
       " ('thai', 1.4990435e-10)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ethnicity('Tom','Huddlestone')\n",
    "#trans_name('Nikolaus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedd the structure vocabulary using text embedding and reduce the dimensionality\n",
    "\n",
    "# convert the names into word structure vector\n",
    "struct_dict_keys = list(struct_dict.keys())\n",
    "\n",
    "def transform_structure(name_struct):\n",
    "    list_structure = []\n",
    "    for x in name_struct:\n",
    "        try:\n",
    "            list_structure.append(struct_dict_keys.index(x)+1)\n",
    "        except BaseException:\n",
    "            list_structure.append(0)\n",
    "    #add pading 0 for structure less than num_input\n",
    "    #for i in range(len(list_structure),timesteps):\n",
    "    #    list_structure.append(0)\n",
    "    return list_structure   \n",
    "    #return [*map(lambda x:struct_dict_keys.index(x)+1, name_struct)]\n",
    "\n",
    "#data_source = full_name_struct.apply(lambda x: transform_structure(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(x):\n",
    "    y = np.zeros(len(ethnic_keys))\n",
    "    y[ethnic_keys.index(x)]=1\n",
    "    return y\n",
    "\n",
    "labels = np.array(list(map(lambda x: transform_labels(x),ethnic_series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using tflearn make the graph creation simple\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# separate train and training set\n",
    "trainX, testX, trainY, testY = train_test_split(data_source,[ethnic_keys.index(x) for x in ethnic_series],test_size = 0.2)\n",
    "\n",
    "trainX = pad_sequences(trainX, maxlen=50,value=0.)\n",
    "testX = pad_sequences(testX,maxlen=50,value=0.)\n",
    "# Converting labels to binary vectors\n",
    "trainY = to_categorical(trainY,nb_classes=len(ethnic_keys))\n",
    "testY = to_categorical(testY,nb_classes=len(ethnic_keys))    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pickle\n",
    "with open('train_test_fix.pickle','wb') as f:\n",
    "    pickle.dump((trainX,trainY,testX,testY,ethnic_keys,struct_dict_keys),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('train_test_fix.pickle', 'rb') as f:\n",
    "    trainX,trainY,testX,testY,ethnic_keys,struct_dict_keys = pickle.load(f)\n",
    "    #aha = pickle.load(f)\n",
    "\n",
    "#with open('traintest-smote.pickle','rb') as f:\n",
    "#    train_res,test_res = pickle.load(f)\n",
    "\n",
    "with open('ethnic_keys.pickle','rb') as f:\n",
    "    name_struct_keys,ethnic_keys = pickle.load(f)\n",
    "        \n",
    "embedding_vector_length = 1000\n",
    "lstm_layer = 1000\n",
    "max_sequence = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert categorical to binary crossentropy\n",
    "#trainY = np.array([np.where(x>0)[0][0] for x in trainY])\n",
    "#testY = np.array([np.where(x>0)[0][0] for x in testY])\n",
    "\n",
    "#test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 1000)          62696000  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 50, 1000)          3001000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               880800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 23)                4623      \n",
      "=================================================================\n",
      "Total params: 66,582,423\n",
      "Trainable params: 66,582,423\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 778s - loss: 1.6879 - acc: 0.5265    \n",
      "Accuracy: 75.88%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 796s - loss: 0.5197 - acc: 0.8582    \n",
      "Accuracy: 84.37%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 817s - loss: 0.2142 - acc: 0.9437    \n",
      "Accuracy: 85.12%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 779s - loss: 0.0989 - acc: 0.9755    \n",
      "Accuracy: 85.57%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 29131s - loss: 0.0476 - acc: 0.9905   \n",
      "Accuracy: 85.63%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 4804s - loss: 0.0258 - acc: 0.9953   \n",
      "Accuracy: 85.55%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 35875s - loss: 0.0138 - acc: 0.9980   \n",
      "Accuracy: 85.79%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 745s - loss: 0.0090 - acc: 0.9988    \n",
      "Accuracy: 85.69%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 742s - loss: 0.0068 - acc: 0.9992    \n",
      "Accuracy: 85.82%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 741s - loss: 0.0048 - acc: 0.9996    \n",
      "Accuracy: 85.62%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(name_struct_keys)+1,embedding_vector_length,input_length=max_sequence))\n",
    "model.add(Conv1D(filters=embedding_vector_length,kernel_size=3,padding='same',activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=2))\n",
    "#model.add(LSTM(lstm_layer,dropout=0.8))\n",
    "model.add(Bidirectional(LSTM(max_sequence*2,return_sequences=False),input_shape=(max_sequence,1)))\n",
    "#model.add(TimeDistributed(keras.layers.Dense(len(ethnic_keys),activation='softmax')))\n",
    "model.add(keras.layers.Dense(len(ethnic_keys),activation='softmax'))\n",
    "#model.add(keras.layers.Dense(len(ethnic_keys),activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "for x in range(10):\n",
    "    model.fit(trainX,trainY,epochs=1,batch_size=1000)\n",
    "    scores = model.evaluate(testX,testY,verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" %(scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  112,  1331,  1332, ...,     0,     0,     0],\n",
       "       [  251,   252,   232, ...,     0,     0,     0],\n",
       "       [ 4633, 11731, 23123, ...,     0,     0,     0],\n",
       "       ..., \n",
       "       [  696,   697,   698, ...,     0,     0,     0],\n",
       "       [   25,    26,  5195, ...,     0,     0,     0],\n",
       "       [ 8671,  8672,  6568, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# save this model atlhouth there is something wrong with the features\n",
    "# I did not lowercase the text :(\n",
    "model_json = model.to_json()\n",
    "with open('model-keras-embed-bilstm-womaxpool.json','w') as f:\n",
    "    f.write(model_json)\n",
    "#save the last weight\n",
    "model.save_weights('model-keras-embed-bilstm-womaxpool-10.h5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "\n",
    "# compute the accuracy\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "    #print(precision)\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "    #print(recall)\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    recall = c1 / c3\n",
    "\n",
    "    return recall\n",
    "\n",
    "# load model\n",
    "# load json and create model\n",
    "json_file = open('model-keras-embed-bilstm-womaxpool.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "#                               ,custom_objects= {'f1_score': f1_score})\n",
    "loaded_model.load_weights(\"model-keras-embed-bilstm-womaxpool-10.h5\")\n",
    "\n",
    "loaded_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy',f1_score,precision,recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = loaded_model.evaluate(testX,testY,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8561812878216707, F1: 0.8601400889915978, Precision: 0.8757469038779302, Recal: 0.845523895023381\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {}, F1: {}, Precision: {}, Recal: {}'.format(scores[1],scores[2],scores[3],scores[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "\n",
    "# transform prediction\n",
    "# given name compute the prediction\n",
    "def predict_ethnicity(name):\n",
    "    # lower case the name\n",
    "    name = name.lower()\n",
    "    # transform space into underscore\n",
    "    name = '_'+name.replace(' ','_')+'_'\n",
    "    #transform the name into sequence structure\n",
    "    ext_name = extract_structure(name)\n",
    "    trans_name = transform_structure(ext_name)\n",
    "    trans_name = pad_sequences([trans_name], maxlen=50,value=0.)\n",
    "    pred = loaded_model.predict(trans_name)\n",
    "    pred_class = np.argsort(pred[0])[::-1]\n",
    "    return_item = []\n",
    "    for x in np.argsort(pred[0])[::-1]:\n",
    "        return_item.append((ethnic_keys[x],pred[0][x]))\n",
    "    return return_item\n",
    "\n",
    "name='helen lamothe'\n",
    "ext_name = extract_structure(name)\n",
    "#print(ext_name)\n",
    "trans_name = transform_structure(ext_name)\n",
    "#trans_name\n",
    "pad_sequences([trans_name], maxlen=50,value=0.)\n",
    "#extract_structure('Nikolaus Nova')\n",
    "#transform_structure('Robert Nova')\n",
    "ethnic_prob = predict_ethnicity('Filho  Elias Abdalla')\n",
    "#ethnic_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.22544670e-03,   2.45175033e-05,   5.54050894e-06,\n",
       "          3.56604069e-05,   1.31730601e-04,   1.58663862e-03,\n",
       "          9.95586514e-01,   3.65224201e-04,   4.18016425e-05,\n",
       "          1.64734403e-04,   3.29397808e-05,   1.42851750e-05,\n",
       "          5.77516516e-07,   3.13429664e-05,   1.98912196e-04,\n",
       "          5.65968139e-06,   4.63458673e-06,   4.45792568e-04,\n",
       "          5.28864875e-05,   2.15678101e-05,   8.53615438e-06,\n",
       "          4.57901763e-07,   1.44505730e-05]], dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = loaded_model.predict(trainX[10].reshape(1,50))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  5,  0, 17,  7, 14,  9,  4, 18,  8,  3, 10, 13,  1, 19, 22, 11,\n",
       "       20, 15,  2, 16, 12, 21])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(test)[0][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6]),)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(trainY[10]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,  2304,  2305,     0,     0,     0,     0,\n",
       "         4088, 15559,  2523, 15102,     0,     0,     0,    24,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name='_helen__lamothe_'\n",
    "ext_name = extract_structure(name)\n",
    "#print(ext_name)\n",
    "trans_name = transform_structure(ext_name)\n",
    "#trans_name\n",
    "pad_sequences([trans_name], maxlen=50,value=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ITALIAN', 0.52735686),\n",
       " ('INDIAN', 0.30269179),\n",
       " ('KOREAN', 0.097788125),\n",
       " ('JAPANESE', 0.017470013),\n",
       " ('ROMANIAN', 0.015210837),\n",
       " ('TURKISH', 0.014220745),\n",
       " ('HISPANIC', 0.0092350421),\n",
       " ('GERMAN', 0.0046770978),\n",
       " ('BALTIC', 0.0032188322),\n",
       " ('ARAB', 0.0028918688),\n",
       " ('ISRAELI', 0.0022467086),\n",
       " ('GREEK', 0.0014537659),\n",
       " ('SLAV', 0.00046977124),\n",
       " ('NORDIC', 0.00024971511),\n",
       " ('HUNGARIAN', 0.0002173665),\n",
       " ('DUTCH', 0.00021363674),\n",
       " ('INDONESIAN', 0.00019762212),\n",
       " ('VIETNAMESE', 9.9613972e-05),\n",
       " ('AFRICAN', 3.4369114e-05),\n",
       " ('CHINESE', 3.3702971e-05),\n",
       " ('ENGLISH', 1.7328795e-05),\n",
       " ('FRENCH', 4.1550811e-06),\n",
       " ('THAI', 9.4588233e-07)]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ethnicity('harry potter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ISRAELI'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethnic_keys[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           _Elias_Abdalla__Filho_HISPANIC\n",
       "1                                  _Jad__Bou_Abdallah_ARAB\n",
       "2                                  _Ayman__Abdel_Aziz_ARAB\n",
       "3                                  _Salma__Abdelmoula_ARAB\n",
       "4                                     _Ibrahim__Abdou_ARAB\n",
       "5                             _Hazem__Abou_El_Fettouh_ARAB\n",
       "6                                     _Rola__Aboutaam_ARAB\n",
       "7        _Aida_Alexandra__Alvim_de_Abreu_Silva_Rodrigue...\n",
       "8             _Isabel__Cristina_Affonso_Scaletsky_HISPANIC\n",
       "9                                _Tsiri__Agbenyega_AFRICAN\n",
       "10                     _Jose__Maria_Aguado_Garcia_HISPANIC\n",
       "11                      _Manuela__Aguilar_Guisado_HISPANIC\n",
       "12                         _Arturo__Aguillon_Luna_HISPANIC\n",
       "13                                   _Ali__Ahmadzadeh_ARAB\n",
       "14                                    _Ahmed__Ibrahim_ARAB\n",
       "15                                    _Ahmed__Letaief_ARAB\n",
       "16                           _Mohammed__Shakeel_Ahmed_ARAB\n",
       "17                              _Alev__Aksoy_Dogan_TURKISH\n",
       "18                    _Mitsumi__Nakatsukasa_Akune_JAPANESE\n",
       "19                                   _Filiz__Akyuz_TURKISH\n",
       "20                                _Omaimah__Al_Gohary_ARAB\n",
       "21                                  _Jassim__Al_Jedah_ARAB\n",
       "22                                  _Shadi__Al_Khatib_ARAB\n",
       "23                                   _Azmi__Al_Najjar_ARAB\n",
       "24                           _Emilio__Alba_Conejo_HISPANIC\n",
       "25                     _Sheila_Alcantara__Llaguno_HISPANIC\n",
       "26                               _Marcos__Alcocer_HISPANIC\n",
       "27                       _Maria__Grazia_Alessandri_ITALIAN\n",
       "28                                  _Randa__Ali_Labib_ARAB\n",
       "29                        _Shameez__Allie_Hamdulay_AFRICAN\n",
       "                               ...                        \n",
       "44537                                _Zhihong__Liu_CHINESE\n",
       "44538                              _Zhen_ying__Liu_CHINESE\n",
       "44539                                    _Zhu__Liu_CHINESE\n",
       "44540                               _Zhengjie__Liu_CHINESE\n",
       "44541                              _Zhengchun__Liu_CHINESE\n",
       "44542                                _Ziliang__Liu_CHINESE\n",
       "44543                                    _Zhe__Liu_CHINESE\n",
       "44544                                 _Zulian__Liu_CHINESE\n",
       "44545                               _Zun_chun__Liu_CHINESE\n",
       "44546                                _Ze_zhou__Liu_CHINESE\n",
       "44547                               _Zhaochun__Liu_CHINESE\n",
       "44548                                 _ziying__Liu_CHINESE\n",
       "44549                                 _zhenyu__Liu_CHINESE\n",
       "44550                              _kenzo__Tanaka_JAPANESE\n",
       "44551                             _kimiko__Tanaka_JAPANESE\n",
       "44552                           _kazuhiro__Tanaka_JAPANESE\n",
       "44553                               _koji__Tanaka_JAPANESE\n",
       "44554                             _Kousei__Tanaka_JAPANESE\n",
       "44555                              _Kyoko__Tanaka_JAPANESE\n",
       "44556                              _Kenji__Tanaka_JAPANESE\n",
       "44557                              _keiji__Tanaka_JAPANESE\n",
       "44558                              _keiji__Tanaka_JAPANESE\n",
       "44559                                _Kei__Tanaka_JAPANESE\n",
       "44560                            _kiyoshi__Tanaka_JAPANESE\n",
       "44561                            _takashi__Sasaki_JAPANESE\n",
       "44562                            _toyoshi__Sasaki_JAPANESE\n",
       "44563                             _tetsuo__Sasaki_JAPANESE\n",
       "44564                             _takayo__Sasaki_JAPANESE\n",
       "44565                                    _may__Haddad_ARAB\n",
       "44566                             _Marianne__Haddad_FRENCH\n",
       "Length: 44567, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethnea_df['First']+ethnea_df['Last']+ethnea_df['Ethnea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
