{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ethnea_df = pd.read_csv('names_ethnea_genni_country.csv')\n",
    "#ethnea_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First thing first, use the character feature for making the dnn model\n",
    "def extract_structure(word,n_char=2):\n",
    "    x_struct = []\n",
    "    word_len = len(word) + n_char\n",
    "    n_char-=1\n",
    "    counter = 0\n",
    "    for i in range(word_len):\n",
    "        end = i+1\n",
    "        start = (i - n_char) if (i - n_char) > 0 else 0\n",
    "        if word[start:end]!='_' and word[start:end]!='':\n",
    "        #if word[start:end]!='_':\n",
    "            x_struct.append(word[start:end])\n",
    "    return x_struct\n",
    "\n",
    "first_name_struct = ethnea_df.First.apply(lambda x: extract_structure(x.lower(),2))\n",
    "last_name_struct = ethnea_df.Last.apply(lambda x: extract_structure(x.lower(),2))                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make struct dictionary\n",
    "struct_dict = {}\n",
    "for name_struct_i in first_name_struct:\n",
    "    for struct_j in name_struct_i:\n",
    "        if struct_j not in struct_dict:\n",
    "            struct_dict[struct_j]=0\n",
    "        struct_dict[struct_j]+=1\n",
    "for name_struct_i in last_name_struct:\n",
    "    for struct_j in name_struct_i:\n",
    "        if struct_j not in struct_dict:\n",
    "            struct_dict[struct_j]=0\n",
    "        struct_dict[struct_j]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "struct_dict_keys = list(struct_dict.keys())\n",
    "ethnic_series = ethnea_df['Ethnea'].str.lower()\n",
    "ethnic_keys = list(np.unique(ethnic_series.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4434085"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_name_struct)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# separate train and training set\n",
    "trainIndex, testIndex, trainY, testY = train_test_split(range(len(first_name_struct)),[ethnic_keys.index(x) for x in ethnic_series],test_size = 0.2)\n",
    "with open('train_test_full_index.pickle','wb') as f:\n",
    "    pickle.dump((trainIndex,testIndex, trainY, testY),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load test train data\n",
    "with open('train_test_full_index.pickle', 'rb') as f:\n",
    "    trainIndex,testIndex,trainY,testY = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform the dataset into structure\n",
    "def transform_structure(name_struct):\n",
    "    list_structure = []\n",
    "    for x in name_struct:\n",
    "        try:\n",
    "            list_structure.append(struct_dict_keys.index(x)+1)\n",
    "        except BaseException:\n",
    "            list_structure.append(0)\n",
    "    #add pading 0 for structure less than num_input\n",
    "    #for i in range(len(list_structure),timesteps):\n",
    "    #    list_structure.append(0)\n",
    "    return list_structure   \n",
    "    #return [*map(lambda x:struct_dict_keys.index(x)+1, name_struct)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# make data source creation more efficient\n",
    "batch_size = 10000\n",
    "#first_name_ds_mat = np.zeros((len(ethnic_series),len(struct_dict_keys)),dtype=np.int32)\n",
    "#last_name_ds_mat = np.zeros((len(ethnic_series),len(struct_dict_keys)),dtype=np.int32)\n",
    "first_name_ds_mat = np.zeros((batch_size,len(struct_dict_keys)),dtype=np.int32)\n",
    "last_name_ds_mat = np.zeros((batch_size,len(struct_dict_keys)),dtype=np.int32)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    x = first_name_struct.iloc[i]\n",
    "    for y in x:\n",
    "        first_name_ds_mat[i,struct_dict_keys.index(y)]+=1\n",
    "    x = last_name_struct.iloc[i]\n",
    "    for y in x:\n",
    "        last_name_ds_mat[i,struct_dict_keys.index(y)]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_structure(name_struct):\n",
    "    list_structure = []\n",
    "    for x in name_struct:\n",
    "        try:\n",
    "            list_structure.append(struct_dict_keys.index(x)+1)\n",
    "        except BaseException:\n",
    "            list_structure.append(0)\n",
    "    #add pading 0 for structure less than num_input\n",
    "    #for i in range(len(list_structure),timesteps):\n",
    "    #    list_structure.append(0)\n",
    "    return list_structure   \n",
    "    #return [*map(lambda x:struct_dict_keys.index(x)+1, name_struct)]\n",
    "\n",
    "#data_source = full_name_struct.apply(lambda x: transform_structure(x))\n",
    "\n",
    "def generate_batch(first_name, last_name, i, batch_size=10000):\n",
    "    len_name = len(first_name)\n",
    "    start = i*batch_size\n",
    "    end = start+batch_size if start+batch_size < len_name else len_name\n",
    "    len_mat = end - start\n",
    "    #first_name_ds_mat = np.zeros((len_mat,50),dtype=np.int32)\n",
    "    #last_name_ds_mat = np.zeros((len_mat,50),dtype=np.int32)\n",
    "    first_name_ds_mat = first_name[start:end]\n",
    "    last_name_ds_mat = last_name[start:end]\n",
    "    first_name_ds_mat = pad_sequences(first_name_ds_mat.apply(lambda x: transform_structure(x)),maxlen=50,value=0.)\n",
    "    first_name_ds_mat = first_name_ds_mat.reshape(first_name_ds_mat.shape[0],1,first_name_ds_mat.shape[1])\n",
    "    last_name_ds_mat = pad_sequences(last_name_ds_mat.apply(lambda x:transform_structure(x)),maxlen=50,value=0.)\n",
    "    last_name_ds_mat = last_name_ds_mat.reshape(last_name_ds_mat.shape[0],1,last_name_ds_mat.shape[1])\n",
    "\n",
    "    \"\"\"\n",
    "    for i in range(len_mat):\n",
    "        x = first_name_struct.iloc[i]\n",
    "        for y in x:\n",
    "            first_name_ds_mat[i,struct_dict_keys.index(y)]+=1\n",
    "        x = last_name_struct.iloc[i]\n",
    "        for y in x:\n",
    "            last_name_ds_mat[i,struct_dict_keys.index(y)]+=1\n",
    "    \"\"\"\n",
    "    return first_name_ds_mat,last_name_ds_mat, range(start,end)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "first_name_ds = first_name_struct.apply(lambda x:transform_structure(x))\n",
    "last_name_ds = last_name_struct.apply(lambda x:transform_structure(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(5,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(x):\n",
    "    y = np.zeros(len(ethnic_keys))\n",
    "    y[ethnic_keys.index(x)]=1\n",
    "    return y\n",
    "\n",
    "labels = np.array(list(map(lambda x: transform_labels(x),ethnic_series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "\"\"\"\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(name_struct_keys)+1,embedding_vector_length,input_length=max_sequence))\n",
    "model.add(Conv1D(filters=embedding_vector_length,kernel_size=3,padding='same',activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=2))\n",
    "#model.add(LSTM(lstm_layer,dropout=0.8))\n",
    "model.add(Bidirectional(LSTM(max_sequence*2,return_sequences=False),input_shape=(max_sequence,1)))\n",
    "#model.add(TimeDistributed(keras.layers.Dense(len(ethnic_keys),activation='softmax')))\n",
    "model.add(keras.layers.Dense(len(ethnic_keys),activation='softmax'))\n",
    "#model.add(keras.layers.Dense(len(ethnic_keys),activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\"\"\"\n",
    "\n",
    "# max sequence length\n",
    "seq_length = 100\n",
    "# multi input with single output\n",
    "\n",
    "# first name input\n",
    "first_name_input = Input(shape=(1,50),name='first_name_input')\n",
    "last_name_input = Input(shape=(1,50),name='last_name_input')\n",
    "\n",
    "# first tensor for first name\n",
    "first_name_l = Bidirectional(LSTM(1000,return_sequences=False))(first_name_input)\n",
    "last_name_l = Bidirectional(LSTM(1000,return_sequences=False))(last_name_input)\n",
    "\n",
    "# merge the two layer together\n",
    "x = keras.layers.concatenate([first_name_l,last_name_l])\n",
    "\n",
    "# stack dense network for memory\n",
    "x = Dense(1000, activation='relu')(x)\n",
    "x = Dense(500, activation='relu')(x)\n",
    "output_l = Dense(len(ethnic_keys),activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[first_name_input, last_name_input], outputs=[output_l])\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "first_name_input (InputLayer)    (None, 1, 50)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "last_name_input (InputLayer)     (None, 1, 50)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional)  (None, 2000)          8408000     first_name_input[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional)  (None, 2000)          8408000     last_name_input[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 4000)          0           bidirectional_4[0][0]            \n",
      "                                                                   bidirectional_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1000)          4001000     concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 500)           500500      dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 26)            13026       dense_5[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 21,330,526\n",
      "Trainable params: 21,330,526\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "first_name_ds_mat = np.zeros((len(first_name_ds),len(struct_dict_keys)),dtype=np.int32)\n",
    "for i,x in enumerate(first_name_ds):\n",
    "    for y in x:\n",
    "        first_name_ds_mat[i,y-1]+=1\n",
    "last_name_ds_mat = np.zeros((len(last_name_ds),len(struct_dict_keys)),dtype=np.int32)\n",
    "for i,x in enumerate(last_name_ds):\n",
    "    for y in x:\n",
    "        last_name_ds_mat[i,y-1]+=1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "first_name_input (InputLayer)    (None, 1, 50)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "last_name_input (InputLayer)     (None, 1, 50)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional)  (None, 2000)          8408000     first_name_input[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional)  (None, 2000)          8408000     last_name_input[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 4000)          0           bidirectional_4[0][0]            \n",
      "                                                                   bidirectional_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1000)          4001000     concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 500)           500500      dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 26)            13026       dense_5[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 21,330,526\n",
      "Trainable params: 21,330,526\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 26s - loss: 2.4946 - acc: 0.2947    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 22s - loss: 1.9588 - acc: 0.4391    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 22s - loss: 1.7741 - acc: 0.4917    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 22s - loss: 1.6398 - acc: 0.5261    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 22s - loss: 1.5288 - acc: 0.5550    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 22s - loss: 1.4780 - acc: 0.5739    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 22s - loss: 1.4564 - acc: 0.5754    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 22s - loss: 1.4259 - acc: 0.5894    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 22s - loss: 1.3739 - acc: 0.6056    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 22s - loss: 1.3246 - acc: 0.6174    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 22s - loss: 1.3154 - acc: 0.6245    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 22s - loss: 1.3029 - acc: 0.6242    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 22s - loss: 1.2882 - acc: 0.6333    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 22s - loss: 1.2732 - acc: 0.6280    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 22s - loss: 1.2127 - acc: 0.6466    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 80s - loss: 1.2401 - acc: 0.6429    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 22s - loss: 1.2035 - acc: 0.6558    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 22s - loss: 1.1930 - acc: 0.6555    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 37s - loss: 1.2206 - acc: 0.6506    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 1.1996 - acc: 0.6537    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 1.1457 - acc: 0.6655    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 74s - loss: 1.1527 - acc: 0.6669    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 97s - loss: 1.1516 - acc: 0.6717    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 1.1579 - acc: 0.6669    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 89s - loss: 1.1224 - acc: 0.6777    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 85s - loss: 1.1099 - acc: 0.6853    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 1.1089 - acc: 0.6851    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 1.0955 - acc: 0.6869    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 70s - loss: 1.1232 - acc: 0.6765    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 1.0978 - acc: 0.6869    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 1.1317 - acc: 0.6743    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 67s - loss: 1.0768 - acc: 0.6906    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 1.0949 - acc: 0.6816    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 106s - loss: 1.0446 - acc: 0.7012    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 67s - loss: 1.0685 - acc: 0.6943    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 1.0652 - acc: 0.6960    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 1.0591 - acc: 0.6969    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 71s - loss: 1.0660 - acc: 0.6909    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 101s - loss: 1.0316 - acc: 0.7029    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 1.0367 - acc: 0.6984    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 84s - loss: 1.0378 - acc: 0.6984    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 89s - loss: 1.0340 - acc: 0.7001    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 1.0052 - acc: 0.7101    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 99s - loss: 1.0181 - acc: 0.7020     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 75s - loss: 1.0287 - acc: 0.7070    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 1.0005 - acc: 0.7078    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 1.0148 - acc: 0.7093    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 67s - loss: 1.0205 - acc: 0.7077    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 1.0132 - acc: 0.7096    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 1.0123 - acc: 0.7079    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 1.0028 - acc: 0.7098    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9788 - acc: 0.7216    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9659 - acc: 0.7227    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9657 - acc: 0.7246    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9984 - acc: 0.7133    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.9767 - acc: 0.7150    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9818 - acc: 0.7172    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9615 - acc: 0.7205    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9523 - acc: 0.7211    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.9811 - acc: 0.7189    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9596 - acc: 0.7154    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9941 - acc: 0.7104    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.9394 - acc: 0.7309    \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 104s - loss: 0.9709 - acc: 0.7167    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.9688 - acc: 0.7175    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9795 - acc: 0.7199    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9297 - acc: 0.7294    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 68s - loss: 0.9477 - acc: 0.7223    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9425 - acc: 0.7267    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 106s - loss: 0.9135 - acc: 0.7361    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9275 - acc: 0.7310    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.9294 - acc: 0.7329    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9451 - acc: 0.7276    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9221 - acc: 0.7325    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9327 - acc: 0.7325    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9026 - acc: 0.7389    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9299 - acc: 0.7313    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.9295 - acc: 0.7328    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.9291 - acc: 0.7362    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9039 - acc: 0.7411    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9298 - acc: 0.7298    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8675 - acc: 0.7470    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.9147 - acc: 0.7338    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.9034 - acc: 0.7381    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.8986 - acc: 0.7431    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.9088 - acc: 0.7382    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.9007 - acc: 0.7374    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 81s - loss: 0.8949 - acc: 0.7384    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 92s - loss: 0.8732 - acc: 0.7522    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8819 - acc: 0.7458    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.9045 - acc: 0.7358    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.9046 - acc: 0.7387    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8944 - acc: 0.7377    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8805 - acc: 0.7437    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8790 - acc: 0.7469    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8708 - acc: 0.7497    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.8632 - acc: 0.7526    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8670 - acc: 0.7481    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8721 - acc: 0.7454    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8563 - acc: 0.7522    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8850 - acc: 0.7418    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.8712 - acc: 0.7453    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.8683 - acc: 0.7449    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8715 - acc: 0.7476    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 106s - loss: 0.8620 - acc: 0.7463    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8578 - acc: 0.7499    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8634 - acc: 0.7492    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8587 - acc: 0.7493    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 68s - loss: 0.8712 - acc: 0.7485    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8490 - acc: 0.7490    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8473 - acc: 0.7581    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8384 - acc: 0.7542    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8681 - acc: 0.7473    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8616 - acc: 0.7525    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.8524 - acc: 0.7568    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8649 - acc: 0.7496    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.8279 - acc: 0.7618    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.8533 - acc: 0.7547    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.8358 - acc: 0.7612    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8360 - acc: 0.7626    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8537 - acc: 0.7552    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.8506 - acc: 0.7573    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8335 - acc: 0.7591    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.8119 - acc: 0.7635    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8339 - acc: 0.7580    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.8441 - acc: 0.7499    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 105s - loss: 0.8080 - acc: 0.7628    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 110s - loss: 0.8401 - acc: 0.7558    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8292 - acc: 0.7604    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8468 - acc: 0.7566    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8256 - acc: 0.7628    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 67s - loss: 0.8349 - acc: 0.7602    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8220 - acc: 0.7614    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8184 - acc: 0.7625    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8396 - acc: 0.7551    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8199 - acc: 0.7615    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8484 - acc: 0.7505    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.7972 - acc: 0.7654    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8239 - acc: 0.7629    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8119 - acc: 0.7621    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 104s - loss: 0.8049 - acc: 0.7666    \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 103s - loss: 0.8371 - acc: 0.7559    \n",
      "Epoch 1/1\n",
      " 5000/10000 [==============>...............] - ETA: 52s - loss: 0.8062 - acc: 0.7676"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1e4e3e8593ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_mini_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0my_first_trainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_last_trainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_range\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_trainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlast_trainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_first_trainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_last_trainX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m#scores = model.evaluate([first_testX, last_testX],testY,verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#print(\"Accuracy: %.2f%%\" %(scores[1]*100))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "first_trainX = first_name_struct[trainIndex]\n",
    "first_testX = first_name_struct[testIndex]\n",
    "last_trainX = last_name_struct[trainIndex]\n",
    "last_testX = last_name_struct[testIndex]\n",
    "trainY = labels[trainIndex]\n",
    "testY = labels[testIndex]\n",
    "\n",
    "#trainX =np.array([to_categorical(x,nb_classes=len(struct_dict_keys)+1) for x in trainX])\n",
    "#testX =np.array([to_categorical(x,nb_classes=len(struct_dict_keys)+1) for x in testX])\n",
    "\n",
    "mini_batch_size = 10000\n",
    "len_mini_batch = round(len(trainY)/mini_batch_size)\n",
    "batch_size = 1000\n",
    "\n",
    "for x in range(10):\n",
    "    for y in range(len_mini_batch):\n",
    "        y_first_trainX, y_last_trainX, batch_range =  generate_batch(first_trainX,last_trainX,y,mini_batch_size)\n",
    "        model.fit([y_first_trainX, y_last_trainX],trainY[batch_range],epochs=1,batch_size=batch_size)\n",
    "        #scores = model.evaluate([first_testX, last_testX],testY,verbose=0)\n",
    "        #print(\"Accuracy: %.2f%%\" %(scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3415 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3498 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3490 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3557 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3430 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3428 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3461 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3501 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3491 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3422 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3318 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3446 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3456 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3455 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3472 - acc: 0.3236    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3430 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3367 - acc: 0.3294    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3515 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3367 - acc: 0.3292    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 15s - loss: 2.3417 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3449 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3443 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3426 - acc: 0.3251    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3411 - acc: 0.3276    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3455 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3505 - acc: 0.3220    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3414 - acc: 0.3269    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3437 - acc: 0.3252    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3439 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3535 - acc: 0.3212    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3427 - acc: 0.3271    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3431 - acc: 0.3284    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3463 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3493 - acc: 0.3242    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3397 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3363 - acc: 0.3241    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3406 - acc: 0.3279    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3457 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3443 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3426 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3412 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3468 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3409 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3474 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3476 - acc: 0.3222    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3478 - acc: 0.3227    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3488 - acc: 0.3230    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3471 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3446 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3408 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3407 - acc: 0.3286    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3415 - acc: 0.3287    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3397 - acc: 0.3280    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3402 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3401 - acc: 0.3295    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3486 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3482 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3439 - acc: 0.3254    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3497 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3451 - acc: 0.3245    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3438 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3430 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3430 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3338 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3470 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3507 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3464 - acc: 0.3239    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3403 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3474 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3374 - acc: 0.3297    \n",
      "Epoch 1/1\n",
      "47268/47268 [==============================] - 13s - loss: 2.3398 - acc: 0.3272    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3411 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3497 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3487 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3555 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3427 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3426 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3461 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3500 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3490 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3422 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3317 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3446 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3456 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3453 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3472 - acc: 0.3236    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3430 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3367 - acc: 0.3294    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3515 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3365 - acc: 0.3292    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3416 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3448 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3441 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3426 - acc: 0.3251    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3410 - acc: 0.3276    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3455 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3504 - acc: 0.3220    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3414 - acc: 0.3269    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3437 - acc: 0.3252    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3439 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3535 - acc: 0.3212    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3426 - acc: 0.3271    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3431 - acc: 0.3284    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3461 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3493 - acc: 0.3242    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3397 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3363 - acc: 0.3241    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3406 - acc: 0.3279    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3456 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3442 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3425 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3412 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3468 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3410 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3474 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3475 - acc: 0.3222    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3478 - acc: 0.3227    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3489 - acc: 0.3230    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3472 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3445 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3408 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3406 - acc: 0.3286    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3414 - acc: 0.3287    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3396 - acc: 0.3280    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3402 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3400 - acc: 0.3295    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3486 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3482 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3438 - acc: 0.3254    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3496 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3450 - acc: 0.3245    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3436 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 15s - loss: 2.3430 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3429 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3339 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3470 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3507 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3464 - acc: 0.3239    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3403 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3473 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3372 - acc: 0.3297    \n",
      "Epoch 1/1\n",
      "47268/47268 [==============================] - 13s - loss: 2.3399 - acc: 0.3272    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 15s - loss: 2.3412 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3497 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3486 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3556 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3426 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3427 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3459 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3500 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3489 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3420 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3316 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3445 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3455 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3452 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3471 - acc: 0.3236    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3429 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3366 - acc: 0.3294    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3514 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3365 - acc: 0.3292    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3416 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3448 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3441 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3425 - acc: 0.3251    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3410 - acc: 0.3276    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3455 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3504 - acc: 0.3220    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3415 - acc: 0.3269    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 15s - loss: 2.3436 - acc: 0.3252    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3437 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 62s - loss: 2.3534 - acc: 0.3212    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 59s - loss: 2.3427 - acc: 0.3271    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3430 - acc: 0.3284    \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 14s - loss: 2.3462 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 32s - loss: 2.3493 - acc: 0.3242    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3398 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3365 - acc: 0.3241    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 45s - loss: 2.3405 - acc: 0.3279    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 48s - loss: 2.3457 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 70s - loss: 2.3442 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3426 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3412 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 51s - loss: 2.3469 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 65s - loss: 2.3407 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3475 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3475 - acc: 0.3222    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 56s - loss: 2.3476 - acc: 0.3227    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3488 - acc: 0.3230    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3471 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3445 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 55s - loss: 2.3406 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3406 - acc: 0.3286    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3416 - acc: 0.3287    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3394 - acc: 0.3280    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 58s - loss: 2.3401 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3399 - acc: 0.3295    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3486 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3482 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 56s - loss: 2.3438 - acc: 0.3254    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 65s - loss: 2.3497 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3449 - acc: 0.3245    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3437 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 58s - loss: 2.3430 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 68s - loss: 2.3431 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3336 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3471 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 54s - loss: 2.3507 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3464 - acc: 0.3239    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3404 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3474 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 56s - loss: 2.3372 - acc: 0.3297    \n",
      "Epoch 1/1\n",
      "47268/47268 [==============================] - 65s - loss: 2.3399 - acc: 0.3272    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3410 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3498 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 56s - loss: 2.3487 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3555 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3425 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3428 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 59s - loss: 2.3461 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3501 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3489 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3420 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 56s - loss: 2.3310 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3445 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3454 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3453 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 58s - loss: 2.3471 - acc: 0.3236    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3429 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3367 - acc: 0.3294    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3514 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 56s - loss: 2.3365 - acc: 0.3292    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3415 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3447 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3444 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 58s - loss: 2.3426 - acc: 0.3251    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3410 - acc: 0.3276    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3455 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3504 - acc: 0.3220    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 55s - loss: 2.3414 - acc: 0.3269    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3438 - acc: 0.3252    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3439 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3535 - acc: 0.3212    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 59s - loss: 2.3427 - acc: 0.3271    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3428 - acc: 0.3285    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3462 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3492 - acc: 0.3242    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 55s - loss: 2.3396 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 64s - loss: 2.3362 - acc: 0.3241    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3405 - acc: 0.3278    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3455 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 59s - loss: 2.3440 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 64s - loss: 2.3425 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3413 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3467 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 57s - loss: 2.3408 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3473 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3474 - acc: 0.3222    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3477 - acc: 0.3227    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 58s - loss: 2.3487 - acc: 0.3230    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3470 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3444 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3407 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 59s - loss: 2.3407 - acc: 0.3286    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 62s - loss: 2.3416 - acc: 0.3287    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3397 - acc: 0.3280    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3403 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 60s - loss: 2.3396 - acc: 0.3295    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 62s - loss: 2.3487 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3483 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3437 - acc: 0.3254    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 60s - loss: 2.3497 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 61s - loss: 2.3451 - acc: 0.3245    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3438 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3430 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 60s - loss: 2.3432 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 62s - loss: 2.3337 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3470 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3506 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 60s - loss: 2.3463 - acc: 0.3239    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 62s - loss: 2.3404 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3474 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3372 - acc: 0.3297    \n",
      "Epoch 1/1\n",
      "47268/47268 [==============================] - 63s - loss: 2.3398 - acc: 0.3272    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3408 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3496 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3486 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 55s - loss: 2.3555 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3426 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3426 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3459 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3501 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3489 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3420 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3316 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 45s - loss: 2.3444 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3454 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3454 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3468 - acc: 0.3236    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3428 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3364 - acc: 0.3294    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3510 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 69s - loss: 2.3364 - acc: 0.3292    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3412 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3448 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3441 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3425 - acc: 0.3251    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3409 - acc: 0.3276    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3453 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3503 - acc: 0.3220    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3414 - acc: 0.3269    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 30s - loss: 2.3436 - acc: 0.3252    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3436 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3534 - acc: 0.3212    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3424 - acc: 0.3271    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3427 - acc: 0.3285    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3462 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3491 - acc: 0.3242    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3395 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3364 - acc: 0.3241    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3405 - acc: 0.3278    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3456 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3440 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3422 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3410 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3466 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3407 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3476 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3473 - acc: 0.3222    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3478 - acc: 0.3227    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3486 - acc: 0.3230    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3471 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3443 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3407 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3405 - acc: 0.3286    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3416 - acc: 0.3287    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3394 - acc: 0.3280    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3402 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 51s - loss: 2.3399 - acc: 0.3295    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 65s - loss: 2.3487 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3481 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3435 - acc: 0.3254    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3498 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3449 - acc: 0.3245    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3438 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3428 - acc: 0.3264    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3432 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3336 - acc: 0.3282    \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 66s - loss: 2.3469 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3504 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 36s - loss: 2.3459 - acc: 0.3239    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3405 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3475 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3372 - acc: 0.3297    \n",
      "Epoch 1/1\n",
      "47268/47268 [==============================] - 63s - loss: 2.3396 - acc: 0.3272    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 65s - loss: 2.3407 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 65s - loss: 2.3495 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3485 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3552 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3426 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3424 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3462 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3500 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3491 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3421 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3315 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3445 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3451 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3453 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3468 - acc: 0.3236    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3429 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3364 - acc: 0.3294    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3510 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3364 - acc: 0.3292    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 48s - loss: 2.3413 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3449 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3439 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3427 - acc: 0.3251    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3409 - acc: 0.3276    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3453 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3500 - acc: 0.3220    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3412 - acc: 0.3269    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3435 - acc: 0.3252    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3437 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3532 - acc: 0.3212    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3422 - acc: 0.3271    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3428 - acc: 0.3284    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3461 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3493 - acc: 0.3242    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 50s - loss: 2.3396 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 65s - loss: 2.3363 - acc: 0.3241    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3405 - acc: 0.3279    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3456 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3440 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3423 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3411 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3465 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 69s - loss: 2.3408 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3471 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3474 - acc: 0.3222    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3480 - acc: 0.3227    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 36s - loss: 2.3487 - acc: 0.3230    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3469 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3444 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3410 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3405 - acc: 0.3286    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3414 - acc: 0.3287    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3394 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3401 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3397 - acc: 0.3295    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3485 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3480 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3437 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3496 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3448 - acc: 0.3245    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3435 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3430 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3431 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3335 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3469 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3504 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3460 - acc: 0.3239    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3401 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3474 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3373 - acc: 0.3297    \n",
      "Epoch 1/1\n",
      "47268/47268 [==============================] - 55s - loss: 2.3395 - acc: 0.3272    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3407 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3494 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3488 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3555 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3424 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3425 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 59s - loss: 2.3459 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 62s - loss: 2.3497 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3491 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3422 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3318 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3443 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3456 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3452 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3470 - acc: 0.3236    \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-9dcd0069fede>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_mini_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0my_first_trainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_last_trainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_range\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_trainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlast_trainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_first_trainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_last_trainX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m#scores = model.evaluate([first_testX, last_testX],testY,verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#print(\"Accuracy: %.2f%%\" %(scores[1]*100))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mini_batch_size = 50000\n",
    "len_mini_batch = round(len(trainY)/mini_batch_size)\n",
    "batch_size = 5000\n",
    "\n",
    "for x in range(10):\n",
    "    for y in range(len_mini_batch):\n",
    "        y_first_trainX, y_last_trainX, batch_range =  generate_batch(first_trainX,last_trainX,y,mini_batch_size)\n",
    "        model.fit([y_first_trainX, y_last_trainX],trainY[batch_range],epochs=1,batch_size=batch_size)\n",
    "        #scores = model.evaluate([first_testX, last_testX],testY,verbose=0)\n",
    "        #print(\"Accuracy: %.2f%%\" %(scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1, 50)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_first_trainX.reshape(y_first_trainX.shape[0],1,y_first_trainX.shape[1]).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save this model atlhouth there is something wrong with the features\n",
    "# I did not lowercase the text :(\n",
    "model_json = model.to_json()\n",
    "with open('model-keras-w3-bigram.json','w') as f:\n",
    "    f.write(model_json)\n",
    "#save the last weight\n",
    "model.save_weights('model-keras-w3-bigram-10.h5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trans_name(name):\n",
    "    name = name.lower()\n",
    "    # transform space into underscore\n",
    "    name = '_'+name.replace(' ','_')+'_'\n",
    "    #transform the name into sequence structure\n",
    "    ext_name = extract_structure(name)\n",
    "    trans_name = transform_structure(ext_name)\n",
    "    #name_ds_mat = np.zeros((1,len(struct_dict_keys)),dtype=np.int32)\n",
    "    #for i,x in enumerate(trans_name):\n",
    "    #    name_ds_mat[0,x-1]+=1    \n",
    "    trans_name = pad_sequences([trans_name], maxlen=50,value=0.)\n",
    "    trans_name = trans_name.reshape(trans_name.shape[0],1,trans_name.shape[1])\n",
    "\n",
    "    return trans_name\n",
    "\n",
    "def predict_ethnicity(fname,lname):\n",
    "    # lower case the name\n",
    "    fnamex = trans_name(fname)\n",
    "    lnamex = trans_name(lname)\n",
    "    pred = model.predict([np.array(fnamex),np.array(lnamex)])\n",
    "    pred_class = np.argsort(pred[0])[::-1]\n",
    "    return_item = []\n",
    "    for x in np.argsort(pred[0])[::-1]:\n",
    "        return_item.append((ethnic_keys[x],pred[0][x]))\n",
    "    return return_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arab', 0.69823658),\n",
       " ('hispanic', 0.057908442),\n",
       " ('french', 0.055705793),\n",
       " ('english', 0.048168365),\n",
       " ('israeli', 0.02471127),\n",
       " ('slav', 0.021401808),\n",
       " ('italian', 0.018654032),\n",
       " ('nordic', 0.017046874),\n",
       " ('indian', 0.016055189),\n",
       " ('turkish', 0.014107338),\n",
       " ('german', 0.007924147),\n",
       " ('chinese', 0.0059029222),\n",
       " ('romanian', 0.0056299986),\n",
       " ('african', 0.0029451125),\n",
       " ('japanese', 0.0026210765),\n",
       " ('baltic', 0.00069294788),\n",
       " ('dutch', 0.00057027361),\n",
       " ('thai', 0.00046238682),\n",
       " ('vietnamese', 0.00037071199),\n",
       " ('indonesian', 0.00027243383),\n",
       " ('greek', 0.00022111264),\n",
       " ('hungarian', 0.00022082729),\n",
       " ('korean', 0.000101094),\n",
       " ('caribbean', 3.0067344e-05),\n",
       " ('mongolian', 2.4259991e-05),\n",
       " ('polynesian', 1.4907315e-05)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ethnicity('Hari','Cahyono')\n",
    "#trans_name('Nikolaus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedd the structure vocabulary using text embedding and reduce the dimensionality\n",
    "\n",
    "# convert the names into word structure vector\n",
    "struct_dict_keys = list(struct_dict.keys())\n",
    "\n",
    "def transform_structure(name_struct):\n",
    "    list_structure = []\n",
    "    for x in name_struct:\n",
    "        try:\n",
    "            list_structure.append(struct_dict_keys.index(x)+1)\n",
    "        except BaseException:\n",
    "            list_structure.append(0)\n",
    "    #add pading 0 for structure less than num_input\n",
    "    #for i in range(len(list_structure),timesteps):\n",
    "    #    list_structure.append(0)\n",
    "    return list_structure   \n",
    "    #return [*map(lambda x:struct_dict_keys.index(x)+1, name_struct)]\n",
    "\n",
    "#data_source = full_name_struct.apply(lambda x: transform_structure(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(x):\n",
    "    y = np.zeros(len(ethnic_keys))\n",
    "    y[ethnic_keys.index(x)]=1\n",
    "    return y\n",
    "\n",
    "labels = np.array(list(map(lambda x: transform_labels(x),ethnic_series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using tflearn make the graph creation simple\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# separate train and training set\n",
    "trainX, testX, trainY, testY = train_test_split(data_source,[ethnic_keys.index(x) for x in ethnic_series],test_size = 0.2)\n",
    "\n",
    "trainX = pad_sequences(trainX, maxlen=50,value=0.)\n",
    "testX = pad_sequences(testX,maxlen=50,value=0.)\n",
    "# Converting labels to binary vectors\n",
    "trainY = to_categorical(trainY,nb_classes=len(ethnic_keys))\n",
    "testY = to_categorical(testY,nb_classes=len(ethnic_keys))    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pickle\n",
    "with open('train_test_fix.pickle','wb') as f:\n",
    "    pickle.dump((trainX,trainY,testX,testY,ethnic_keys,struct_dict_keys),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('train_test_fix.pickle', 'rb') as f:\n",
    "    trainX,trainY,testX,testY,ethnic_keys,struct_dict_keys = pickle.load(f)\n",
    "    #aha = pickle.load(f)\n",
    "\n",
    "#with open('traintest-smote.pickle','rb') as f:\n",
    "#    train_res,test_res = pickle.load(f)\n",
    "\n",
    "with open('ethnic_keys.pickle','rb') as f:\n",
    "    name_struct_keys,ethnic_keys = pickle.load(f)\n",
    "        \n",
    "embedding_vector_length = 1000\n",
    "lstm_layer = 1000\n",
    "max_sequence = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert categorical to binary crossentropy\n",
    "#trainY = np.array([np.where(x>0)[0][0] for x in trainY])\n",
    "#testY = np.array([np.where(x>0)[0][0] for x in testY])\n",
    "\n",
    "#test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 1000)          62696000  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 50, 1000)          3001000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               880800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 23)                4623      \n",
      "=================================================================\n",
      "Total params: 66,582,423\n",
      "Trainable params: 66,582,423\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 778s - loss: 1.6879 - acc: 0.5265    \n",
      "Accuracy: 75.88%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 796s - loss: 0.5197 - acc: 0.8582    \n",
      "Accuracy: 84.37%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 817s - loss: 0.2142 - acc: 0.9437    \n",
      "Accuracy: 85.12%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 779s - loss: 0.0989 - acc: 0.9755    \n",
      "Accuracy: 85.57%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 29131s - loss: 0.0476 - acc: 0.9905   \n",
      "Accuracy: 85.63%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 4804s - loss: 0.0258 - acc: 0.9953   \n",
      "Accuracy: 85.55%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 35875s - loss: 0.0138 - acc: 0.9980   \n",
      "Accuracy: 85.79%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 745s - loss: 0.0090 - acc: 0.9988    \n",
      "Accuracy: 85.69%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 742s - loss: 0.0068 - acc: 0.9992    \n",
      "Accuracy: 85.82%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 741s - loss: 0.0048 - acc: 0.9996    \n",
      "Accuracy: 85.62%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(name_struct_keys)+1,embedding_vector_length,input_length=max_sequence))\n",
    "model.add(Conv1D(filters=embedding_vector_length,kernel_size=3,padding='same',activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=2))\n",
    "#model.add(LSTM(lstm_layer,dropout=0.8))\n",
    "model.add(Bidirectional(LSTM(max_sequence*2,return_sequences=False),input_shape=(max_sequence,1)))\n",
    "#model.add(TimeDistributed(keras.layers.Dense(len(ethnic_keys),activation='softmax')))\n",
    "model.add(keras.layers.Dense(len(ethnic_keys),activation='softmax'))\n",
    "#model.add(keras.layers.Dense(len(ethnic_keys),activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "for x in range(10):\n",
    "    model.fit(trainX,trainY,epochs=1,batch_size=1000)\n",
    "    scores = model.evaluate(testX,testY,verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" %(scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  112,  1331,  1332, ...,     0,     0,     0],\n",
       "       [  251,   252,   232, ...,     0,     0,     0],\n",
       "       [ 4633, 11731, 23123, ...,     0,     0,     0],\n",
       "       ..., \n",
       "       [  696,   697,   698, ...,     0,     0,     0],\n",
       "       [   25,    26,  5195, ...,     0,     0,     0],\n",
       "       [ 8671,  8672,  6568, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# save this model atlhouth there is something wrong with the features\n",
    "# I did not lowercase the text :(\n",
    "model_json = model.to_json()\n",
    "with open('model-keras-embed-bilstm-womaxpool.json','w') as f:\n",
    "    f.write(model_json)\n",
    "#save the last weight\n",
    "model.save_weights('model-keras-embed-bilstm-womaxpool-10.h5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "\n",
    "# compute the accuracy\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "    #print(precision)\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "    #print(recall)\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    recall = c1 / c3\n",
    "\n",
    "    return recall\n",
    "\n",
    "# load model\n",
    "# load json and create model\n",
    "json_file = open('model-keras-embed-bilstm-womaxpool.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "#                               ,custom_objects= {'f1_score': f1_score})\n",
    "loaded_model.load_weights(\"model-keras-embed-bilstm-womaxpool-10.h5\")\n",
    "\n",
    "loaded_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy',f1_score,precision,recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = loaded_model.evaluate(testX,testY,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8561812878216707, F1: 0.8601400889915978, Precision: 0.8757469038779302, Recal: 0.845523895023381\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {}, F1: {}, Precision: {}, Recal: {}'.format(scores[1],scores[2],scores[3],scores[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "\n",
    "# transform prediction\n",
    "# given name compute the prediction\n",
    "def predict_ethnicity(name):\n",
    "    # lower case the name\n",
    "    name = name.lower()\n",
    "    # transform space into underscore\n",
    "    name = '_'+name.replace(' ','_')+'_'\n",
    "    #transform the name into sequence structure\n",
    "    ext_name = extract_structure(name)\n",
    "    trans_name = transform_structure(ext_name)\n",
    "    trans_name = pad_sequences([trans_name], maxlen=50,value=0.)\n",
    "    pred = loaded_model.predict(trans_name)\n",
    "    pred_class = np.argsort(pred[0])[::-1]\n",
    "    return_item = []\n",
    "    for x in np.argsort(pred[0])[::-1]:\n",
    "        return_item.append((ethnic_keys[x],pred[0][x]))\n",
    "    return return_item\n",
    "\n",
    "name='helen lamothe'\n",
    "ext_name = extract_structure(name)\n",
    "#print(ext_name)\n",
    "trans_name = transform_structure(ext_name)\n",
    "#trans_name\n",
    "pad_sequences([trans_name], maxlen=50,value=0.)\n",
    "#extract_structure('Nikolaus Nova')\n",
    "#transform_structure('Robert Nova')\n",
    "ethnic_prob = predict_ethnicity('Filho  Elias Abdalla')\n",
    "#ethnic_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.22544670e-03,   2.45175033e-05,   5.54050894e-06,\n",
       "          3.56604069e-05,   1.31730601e-04,   1.58663862e-03,\n",
       "          9.95586514e-01,   3.65224201e-04,   4.18016425e-05,\n",
       "          1.64734403e-04,   3.29397808e-05,   1.42851750e-05,\n",
       "          5.77516516e-07,   3.13429664e-05,   1.98912196e-04,\n",
       "          5.65968139e-06,   4.63458673e-06,   4.45792568e-04,\n",
       "          5.28864875e-05,   2.15678101e-05,   8.53615438e-06,\n",
       "          4.57901763e-07,   1.44505730e-05]], dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = loaded_model.predict(trainX[10].reshape(1,50))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  5,  0, 17,  7, 14,  9,  4, 18,  8,  3, 10, 13,  1, 19, 22, 11,\n",
       "       20, 15,  2, 16, 12, 21])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(test)[0][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6]),)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(trainY[10]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,  2304,  2305,     0,     0,     0,     0,\n",
       "         4088, 15559,  2523, 15102,     0,     0,     0,    24,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name='_helen__lamothe_'\n",
    "ext_name = extract_structure(name)\n",
    "#print(ext_name)\n",
    "trans_name = transform_structure(ext_name)\n",
    "#trans_name\n",
    "pad_sequences([trans_name], maxlen=50,value=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ITALIAN', 0.52735686),\n",
       " ('INDIAN', 0.30269179),\n",
       " ('KOREAN', 0.097788125),\n",
       " ('JAPANESE', 0.017470013),\n",
       " ('ROMANIAN', 0.015210837),\n",
       " ('TURKISH', 0.014220745),\n",
       " ('HISPANIC', 0.0092350421),\n",
       " ('GERMAN', 0.0046770978),\n",
       " ('BALTIC', 0.0032188322),\n",
       " ('ARAB', 0.0028918688),\n",
       " ('ISRAELI', 0.0022467086),\n",
       " ('GREEK', 0.0014537659),\n",
       " ('SLAV', 0.00046977124),\n",
       " ('NORDIC', 0.00024971511),\n",
       " ('HUNGARIAN', 0.0002173665),\n",
       " ('DUTCH', 0.00021363674),\n",
       " ('INDONESIAN', 0.00019762212),\n",
       " ('VIETNAMESE', 9.9613972e-05),\n",
       " ('AFRICAN', 3.4369114e-05),\n",
       " ('CHINESE', 3.3702971e-05),\n",
       " ('ENGLISH', 1.7328795e-05),\n",
       " ('FRENCH', 4.1550811e-06),\n",
       " ('THAI', 9.4588233e-07)]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ethnicity('harry potter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ISRAELI'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethnic_keys[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           _Elias_Abdalla__Filho_HISPANIC\n",
       "1                                  _Jad__Bou_Abdallah_ARAB\n",
       "2                                  _Ayman__Abdel_Aziz_ARAB\n",
       "3                                  _Salma__Abdelmoula_ARAB\n",
       "4                                     _Ibrahim__Abdou_ARAB\n",
       "5                             _Hazem__Abou_El_Fettouh_ARAB\n",
       "6                                     _Rola__Aboutaam_ARAB\n",
       "7        _Aida_Alexandra__Alvim_de_Abreu_Silva_Rodrigue...\n",
       "8             _Isabel__Cristina_Affonso_Scaletsky_HISPANIC\n",
       "9                                _Tsiri__Agbenyega_AFRICAN\n",
       "10                     _Jose__Maria_Aguado_Garcia_HISPANIC\n",
       "11                      _Manuela__Aguilar_Guisado_HISPANIC\n",
       "12                         _Arturo__Aguillon_Luna_HISPANIC\n",
       "13                                   _Ali__Ahmadzadeh_ARAB\n",
       "14                                    _Ahmed__Ibrahim_ARAB\n",
       "15                                    _Ahmed__Letaief_ARAB\n",
       "16                           _Mohammed__Shakeel_Ahmed_ARAB\n",
       "17                              _Alev__Aksoy_Dogan_TURKISH\n",
       "18                    _Mitsumi__Nakatsukasa_Akune_JAPANESE\n",
       "19                                   _Filiz__Akyuz_TURKISH\n",
       "20                                _Omaimah__Al_Gohary_ARAB\n",
       "21                                  _Jassim__Al_Jedah_ARAB\n",
       "22                                  _Shadi__Al_Khatib_ARAB\n",
       "23                                   _Azmi__Al_Najjar_ARAB\n",
       "24                           _Emilio__Alba_Conejo_HISPANIC\n",
       "25                     _Sheila_Alcantara__Llaguno_HISPANIC\n",
       "26                               _Marcos__Alcocer_HISPANIC\n",
       "27                       _Maria__Grazia_Alessandri_ITALIAN\n",
       "28                                  _Randa__Ali_Labib_ARAB\n",
       "29                        _Shameez__Allie_Hamdulay_AFRICAN\n",
       "                               ...                        \n",
       "44537                                _Zhihong__Liu_CHINESE\n",
       "44538                              _Zhen_ying__Liu_CHINESE\n",
       "44539                                    _Zhu__Liu_CHINESE\n",
       "44540                               _Zhengjie__Liu_CHINESE\n",
       "44541                              _Zhengchun__Liu_CHINESE\n",
       "44542                                _Ziliang__Liu_CHINESE\n",
       "44543                                    _Zhe__Liu_CHINESE\n",
       "44544                                 _Zulian__Liu_CHINESE\n",
       "44545                               _Zun_chun__Liu_CHINESE\n",
       "44546                                _Ze_zhou__Liu_CHINESE\n",
       "44547                               _Zhaochun__Liu_CHINESE\n",
       "44548                                 _ziying__Liu_CHINESE\n",
       "44549                                 _zhenyu__Liu_CHINESE\n",
       "44550                              _kenzo__Tanaka_JAPANESE\n",
       "44551                             _kimiko__Tanaka_JAPANESE\n",
       "44552                           _kazuhiro__Tanaka_JAPANESE\n",
       "44553                               _koji__Tanaka_JAPANESE\n",
       "44554                             _Kousei__Tanaka_JAPANESE\n",
       "44555                              _Kyoko__Tanaka_JAPANESE\n",
       "44556                              _Kenji__Tanaka_JAPANESE\n",
       "44557                              _keiji__Tanaka_JAPANESE\n",
       "44558                              _keiji__Tanaka_JAPANESE\n",
       "44559                                _Kei__Tanaka_JAPANESE\n",
       "44560                            _kiyoshi__Tanaka_JAPANESE\n",
       "44561                            _takashi__Sasaki_JAPANESE\n",
       "44562                            _toyoshi__Sasaki_JAPANESE\n",
       "44563                             _tetsuo__Sasaki_JAPANESE\n",
       "44564                             _takayo__Sasaki_JAPANESE\n",
       "44565                                    _may__Haddad_ARAB\n",
       "44566                             _Marianne__Haddad_FRENCH\n",
       "Length: 44567, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethnea_df['First']+ethnea_df['Last']+ethnea_df['Ethnea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
