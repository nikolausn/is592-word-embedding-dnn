{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ethnea_df = pd.read_csv('names_ethnea_genni_country.csv')\n",
    "#ethnea_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First thing first, use the character feature for making the dnn model\n",
    "def extract_structure(word,n_char=2):\n",
    "    x_struct = []\n",
    "    word_len = len(word) + n_char\n",
    "    n_char-=1\n",
    "    counter = 0\n",
    "    for i in range(word_len):\n",
    "        end = i+1\n",
    "        start = (i - n_char) if (i - n_char) > 0 else 0\n",
    "        if word[start:end]!='_' and word[start:end]!='':\n",
    "        #if word[start:end]!='_':\n",
    "            x_struct.append(word[start:end])\n",
    "    return x_struct\n",
    "\n",
    "first_name_struct = ethnea_df.First.apply(lambda x: extract_structure(x.lower(),2))\n",
    "last_name_struct = ethnea_df.Last.apply(lambda x: extract_structure(x.lower(),2))                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make struct dictionary\n",
    "struct_dict = {}\n",
    "for name_struct_i in first_name_struct:\n",
    "    for struct_j in name_struct_i:\n",
    "        if struct_j not in struct_dict:\n",
    "            struct_dict[struct_j]=0\n",
    "        struct_dict[struct_j]+=1\n",
    "for name_struct_i in last_name_struct:\n",
    "    for struct_j in name_struct_i:\n",
    "        if struct_j not in struct_dict:\n",
    "            struct_dict[struct_j]=0\n",
    "        struct_dict[struct_j]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "struct_dict_keys = list(struct_dict.keys())\n",
    "ethnic_series = ethnea_df['Ethnea'].str.lower()\n",
    "ethnic_keys = list(np.unique(ethnic_series.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4434085"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_name_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# separate train and training set\n",
    "trainIndex, testIndex, trainY, testY = train_test_split(range(len(first_name_struct)),[ethnic_keys.index(x) for x in ethnic_series],test_size = 0.2)\n",
    "with open('train_test_full_index.pickle','wb') as f:\n",
    "    pickle.dump((trainIndex,testIndex, trainY, testY),f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# load test train data\n",
    "with open('train_test_full_index.pickle', 'rb') as f:\n",
    "    trainIndex,testIndex,trainY,testY = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform the dataset into structure\n",
    "def transform_structure(name_struct):\n",
    "    list_structure = []\n",
    "    for x in name_struct:\n",
    "        try:\n",
    "            list_structure.append(struct_dict_keys.index(x)+1)\n",
    "        except BaseException:\n",
    "            list_structure.append(0)\n",
    "    #add pading 0 for structure less than num_input\n",
    "    #for i in range(len(list_structure),timesteps):\n",
    "    #    list_structure.append(0)\n",
    "    return list_structure   \n",
    "    #return [*map(lambda x:struct_dict_keys.index(x)+1, name_struct)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data source creation more efficient\n",
    "batch_size = 10000\n",
    "#first_name_ds_mat = np.zeros((len(ethnic_series),len(struct_dict_keys)),dtype=np.int32)\n",
    "#last_name_ds_mat = np.zeros((len(ethnic_series),len(struct_dict_keys)),dtype=np.int32)\n",
    "first_name_ds_mat = np.zeros((batch_size,len(struct_dict_keys)),dtype=np.int32)\n",
    "last_name_ds_mat = np.zeros((batch_size,len(struct_dict_keys)),dtype=np.int32)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    x = first_name_struct.iloc[i]\n",
    "    for y in x:\n",
    "        first_name_ds_mat[i,struct_dict_keys.index(y)]+=1\n",
    "    x = last_name_struct.iloc[i]\n",
    "    for y in x:\n",
    "        last_name_ds_mat[i,struct_dict_keys.index(y)]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(first_name, last_name, i, batch_size=10000):\n",
    "    len_name = len(first_name)\n",
    "    start = i*batch_size\n",
    "    end = start+batch_size if start+batch_size < len_name else len_name\n",
    "    len_mat = end - start\n",
    "    first_name_ds_mat = np.zeros((len_mat,len(struct_dict_keys)),dtype=np.int32)\n",
    "    last_name_ds_mat = np.zeros((len_mat,len(struct_dict_keys)),dtype=np.int32)\n",
    "\n",
    "    for i in range(len_mat):\n",
    "        x = first_name_struct.iloc[i]\n",
    "        for y in x:\n",
    "            first_name_ds_mat[i,struct_dict_keys.index(y)]+=1\n",
    "        x = last_name_struct.iloc[i]\n",
    "        for y in x:\n",
    "            last_name_ds_mat[i,struct_dict_keys.index(y)]+=1\n",
    "    return first_name_ds_mat,last_name_ds_mat, range(start,end)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "first_name_ds = first_name_struct.apply(lambda x:transform_structure(x))\n",
    "last_name_ds = last_name_struct.apply(lambda x:transform_structure(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(5,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(x):\n",
    "    y = np.zeros(len(ethnic_keys))\n",
    "    y[ethnic_keys.index(x)]=1\n",
    "    return y\n",
    "\n",
    "labels = np.array(list(map(lambda x: transform_labels(x),ethnic_series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
       "1                                         [15, 16, 17, 18]\n",
       "2                                  [7, 19, 20, 21, 22, 23]\n",
       "3                                 [24, 25, 11, 26, 21, 14]\n",
       "4                         [27, 28, 29, 30, 31, 32, 33, 34]\n",
       "5                                 [35, 36, 37, 38, 39, 34]\n",
       "6                                     [40, 41, 42, 13, 14]\n",
       "7        [7, 43, 44, 10, 14, 7, 11, 45, 46, 47, 22, 48,...\n",
       "8                               [27, 50, 25, 8, 51, 2, 52]\n",
       "9                                 [53, 54, 55, 56, 57, 58]\n",
       "10                                    [15, 59, 60, 61, 62]\n",
       "11                         [63, 21, 22, 64, 65, 2, 13, 14]\n",
       "12                             [7, 66, 67, 68, 69, 41, 70]\n",
       "13                                          [7, 11, 3, 58]\n",
       "14                                 [7, 31, 71, 72, 73, 18]\n",
       "15                                 [7, 31, 71, 72, 73, 18]\n",
       "16                    [63, 74, 75, 36, 76, 77, 72, 73, 18]\n",
       "17                                     [7, 11, 45, 78, 79]\n",
       "18                        [63, 80, 81, 54, 82, 83, 80, 58]\n",
       "19                                 [84, 85, 86, 3, 87, 88]\n",
       "20                        [89, 90, 21, 43, 33, 21, 31, 91]\n",
       "21                             [15, 16, 5, 92, 55, 33, 34]\n",
       "22                                [24, 93, 36, 17, 94, 58]\n",
       "23                                     [7, 37, 95, 80, 58]\n",
       "24                              [1, 39, 80, 86, 3, 96, 70]\n",
       "25       [24, 93, 97, 98, 86, 13, 14, 7, 11, 99, 100, 2...\n",
       "26                           [63, 21, 66, 103, 104, 60, 6]\n",
       "27                                 [63, 21, 66, 57, 4, 14]\n",
       "28                                [40, 30, 22, 48, 10, 14]\n",
       "29                      [24, 93, 36, 76, 72, 105, 106, 88]\n",
       "                               ...                        \n",
       "44537              [295, 296, 32, 381, 128, 113, 115, 286]\n",
       "44538     [295, 296, 97, 120, 23, 158, 159, 168, 115, 286]\n",
       "44539                                 [295, 296, 287, 212]\n",
       "44540          [295, 296, 97, 120, 115, 449, 270, 153, 62]\n",
       "44541     [295, 296, 97, 120, 115, 433, 230, 287, 249, 23]\n",
       "44542                   [295, 413, 86, 3, 4, 22, 115, 286]\n",
       "44543                                   [295, 296, 97, 62]\n",
       "44544                        [295, 218, 198, 3, 4, 22, 23]\n",
       "44545          [295, 218, 249, 23, 125, 230, 287, 249, 23]\n",
       "44546               [295, 38, 62, 295, 296, 128, 202, 212]\n",
       "44547          [295, 296, 36, 245, 385, 230, 287, 249, 23]\n",
       "44548                  [295, 413, 367, 159, 168, 115, 286]\n",
       "44549                   [295, 296, 97, 120, 309, 248, 212]\n",
       "44550                        [136, 133, 120, 200, 330, 70]\n",
       "44551                     [136, 355, 33, 80, 326, 257, 70]\n",
       "44552             [136, 137, 37, 218, 371, 32, 56, 41, 70]\n",
       "44553                             [136, 257, 443, 270, 58]\n",
       "44554                     [136, 257, 202, 204, 61, 98, 58]\n",
       "44555                        [136, 410, 300, 129, 257, 70]\n",
       "44556                        [136, 133, 120, 362, 270, 58]\n",
       "44557                         [136, 133, 98, 255, 270, 58]\n",
       "44558                         [136, 133, 98, 255, 270, 58]\n",
       "44559                                   [136, 133, 98, 58]\n",
       "44560                 [136, 355, 367, 300, 60, 93, 32, 58]\n",
       "44561                   [53, 102, 282, 137, 5, 93, 32, 58]\n",
       "44562                  [53, 143, 390, 300, 60, 93, 32, 58]\n",
       "44563                      [53, 210, 199, 54, 82, 321, 70]\n",
       "44564                     [53, 102, 282, 137, 19, 300, 70]\n",
       "44565                                    [63, 21, 19, 175]\n",
       "44566                [63, 21, 66, 57, 4, 22, 163, 207, 62]\n",
       "Name: First, Length: 44567, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_name_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "# max sequence length\n",
    "seq_length = 100\n",
    "# multi input with single output\n",
    "\n",
    "# first name input\n",
    "first_name_input = Input(shape=(len(struct_dict_keys),),name='first_name_input')\n",
    "last_name_input = Input(shape=(len(struct_dict_keys),),name='last_name_input')\n",
    "\n",
    "# first tensor for first name\n",
    "first_name_l = Dense(units=1000)(first_name_input)\n",
    "last_name_l = Dense(units=1000)(last_name_input)\n",
    "\n",
    "# merge the two layer together\n",
    "x = keras.layers.concatenate([first_name_l,last_name_l])\n",
    "\n",
    "# stack dense network for memory\n",
    "x = Dense(1000, activation='relu')(x)\n",
    "x = Dense(500, activation='relu')(x)\n",
    "output_l = Dense(len(ethnic_keys),activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[first_name_input, last_name_input], outputs=[output_l])\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "first_name_ds_mat = np.zeros((len(first_name_ds),len(struct_dict_keys)),dtype=np.int32)\n",
    "for i,x in enumerate(first_name_ds):\n",
    "    for y in x:\n",
    "        first_name_ds_mat[i,y-1]+=1\n",
    "last_name_ds_mat = np.zeros((len(last_name_ds),len(struct_dict_keys)),dtype=np.int32)\n",
    "for i,x in enumerate(last_name_ds):\n",
    "    for y in x:\n",
    "        last_name_ds_mat[i,y-1]+=1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "first_name_input (InputLayer)    (None, 725)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "last_name_input (InputLayer)     (None, 725)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1000)          726000      first_name_input[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1000)          726000      last_name_input[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 2000)          0           dense_1[0][0]                    \n",
      "                                                                   dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1000)          2001000     concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 500)           500500      dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 26)            13026       dense_4[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 3,966,526\n",
      "Trainable params: 3,966,526\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.5871 - acc: 0.3001     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3856 - acc: 0.3338     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3843 - acc: 0.3257     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3692 - acc: 0.3264     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3684 - acc: 0.3217     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3578 - acc: 0.3335     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3852 - acc: 0.3139     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3721 - acc: 0.3212     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3575 - acc: 0.3295     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3673 - acc: 0.3260     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3570 - acc: 0.3291     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3658 - acc: 0.3306     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3673 - acc: 0.3198     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3785 - acc: 0.3189     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3566 - acc: 0.3294     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3641 - acc: 0.3240     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3719 - acc: 0.3218     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3712 - acc: 0.3181     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3819 - acc: 0.3238     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3590 - acc: 0.3248     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3374 - acc: 0.3342     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3775 - acc: 0.3180     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3557 - acc: 0.3236     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3563 - acc: 0.3278     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3479 - acc: 0.3263     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3554 - acc: 0.3263     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3572 - acc: 0.3261     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3397 - acc: 0.3336     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3580 - acc: 0.3277     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3585 - acc: 0.3246     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3551 - acc: 0.3266     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3579 - acc: 0.3228     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3662 - acc: 0.3198     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3471 - acc: 0.3294     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3538 - acc: 0.3265     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3539 - acc: 0.3266     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3505 - acc: 0.3297     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3636 - acc: 0.3247     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3547 - acc: 0.3249     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3717 - acc: 0.3110     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3589 - acc: 0.3205     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3603 - acc: 0.3228     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3542 - acc: 0.3261     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3462 - acc: 0.3283     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3758 - acc: 0.3180     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3323 - acc: 0.3350     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3376 - acc: 0.3341     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3710 - acc: 0.3135     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3555 - acc: 0.3263     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3563 - acc: 0.3244     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3413 - acc: 0.3257     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3492 - acc: 0.3252     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3392 - acc: 0.3279     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3311 - acc: 0.3304     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3320 - acc: 0.3284     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3667 - acc: 0.3189     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3427 - acc: 0.3293     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3500 - acc: 0.3246     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3456 - acc: 0.3222     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3548 - acc: 0.3294     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3383 - acc: 0.3235     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3835 - acc: 0.3184     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3319 - acc: 0.3338     \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s - loss: 2.3557 - acc: 0.3250     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3551 - acc: 0.3324     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3732 - acc: 0.3196     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3300 - acc: 0.3257     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3494 - acc: 0.3243     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3567 - acc: 0.3236     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3516 - acc: 0.3284     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3577 - acc: 0.3239     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3561 - acc: 0.3220     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3418 - acc: 0.3334     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3530 - acc: 0.3200     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3581 - acc: 0.3186     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3424 - acc: 0.3302     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3649 - acc: 0.3238     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3311 - acc: 0.3316     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3568 - acc: 0.3287     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3499 - acc: 0.3269     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3577 - acc: 0.3273     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3243 - acc: 0.3333     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3541 - acc: 0.3255     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3594 - acc: 0.3261     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3173 - acc: 0.3348     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3712 - acc: 0.3151     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3615 - acc: 0.3171     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3403 - acc: 0.3267     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3492 - acc: 0.3270     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3617 - acc: 0.3235     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3330 - acc: 0.3296     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3401 - acc: 0.3319     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3408 - acc: 0.3293     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3447 - acc: 0.3257     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3475 - acc: 0.3297     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3322 - acc: 0.3332     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3539 - acc: 0.3172     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3559 - acc: 0.3255     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3505 - acc: 0.3220     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3399 - acc: 0.3306     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3591 - acc: 0.3229     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3374 - acc: 0.3302     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3427 - acc: 0.3308     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3553 - acc: 0.3242     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3537 - acc: 0.3245     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3438 - acc: 0.3239     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3472 - acc: 0.3242     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3432 - acc: 0.3298     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3489 - acc: 0.3251     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3591 - acc: 0.3234     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3432 - acc: 0.3243     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3349 - acc: 0.3300     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3479 - acc: 0.3268     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3579 - acc: 0.3199     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3514 - acc: 0.3246     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3586 - acc: 0.3259     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3257 - acc: 0.3340     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3553 - acc: 0.3235     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3449 - acc: 0.3259     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3414 - acc: 0.3287     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3645 - acc: 0.3208     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3467 - acc: 0.3243     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3447 - acc: 0.3259     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3334 - acc: 0.3320     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3536 - acc: 0.3205     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3475 - acc: 0.3204     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3492 - acc: 0.3273     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3436 - acc: 0.3279     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3576 - acc: 0.3201     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3715 - acc: 0.3142     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3483 - acc: 0.3249     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3379 - acc: 0.3304     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3399 - acc: 0.3274     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3590 - acc: 0.3240     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3386 - acc: 0.3278     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3230 - acc: 0.3280     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3682 - acc: 0.3245     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3385 - acc: 0.3241     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3625 - acc: 0.3206     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3437 - acc: 0.3290     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3463 - acc: 0.3281     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3671 - acc: 0.3119     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3384 - acc: 0.3326     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3378 - acc: 0.3290     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3465 - acc: 0.3219     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3566 - acc: 0.3246     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3554 - acc: 0.3245     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3624 - acc: 0.3159     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3642 - acc: 0.3183     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3496 - acc: 0.3226     \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s - loss: 2.3434 - acc: 0.3273     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3439 - acc: 0.3288     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3581 - acc: 0.3195     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3426 - acc: 0.3277     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3389 - acc: 0.3320     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3559 - acc: 0.3278     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3366 - acc: 0.3312     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3426 - acc: 0.3303     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3474 - acc: 0.3237     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3465 - acc: 0.3292     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3558 - acc: 0.3216     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3409 - acc: 0.3235     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3481 - acc: 0.3263     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3542 - acc: 0.3290     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3455 - acc: 0.3261     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3444 - acc: 0.3252     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3710 - acc: 0.3188     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3374 - acc: 0.3309     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3423 - acc: 0.3307     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3645 - acc: 0.3155     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3463 - acc: 0.3238     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3400 - acc: 0.3283     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3320 - acc: 0.3299     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3437 - acc: 0.3209     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3483 - acc: 0.3253     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3499 - acc: 0.3189     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3346 - acc: 0.3260     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3224 - acc: 0.3298     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3428 - acc: 0.3210     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3461 - acc: 0.3249     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3363 - acc: 0.3285     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3420 - acc: 0.3272     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3548 - acc: 0.3251     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3386 - acc: 0.3299     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3433 - acc: 0.3287     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3426 - acc: 0.3324     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3577 - acc: 0.3200     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3600 - acc: 0.3214     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3443 - acc: 0.3272     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3383 - acc: 0.3318     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3539 - acc: 0.3255     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3506 - acc: 0.3244     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3334 - acc: 0.3295     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3504 - acc: 0.3243     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3448 - acc: 0.3278     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3659 - acc: 0.3162     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3477 - acc: 0.3230     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3328 - acc: 0.3320     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3390 - acc: 0.3261     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3388 - acc: 0.3262     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3551 - acc: 0.3215     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3259 - acc: 0.3345     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3338 - acc: 0.3311     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3542 - acc: 0.3223     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3487 - acc: 0.3221     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3491 - acc: 0.3219     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3519 - acc: 0.3228     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3416 - acc: 0.3302     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3535 - acc: 0.3222     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3460 - acc: 0.3274     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3478 - acc: 0.3209     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3314 - acc: 0.3345     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3475 - acc: 0.3269     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3516 - acc: 0.3238     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3357 - acc: 0.3262     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3538 - acc: 0.3202     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3534 - acc: 0.3237     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3599 - acc: 0.3247     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3308 - acc: 0.3347     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3504 - acc: 0.3268     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3573 - acc: 0.3198     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3445 - acc: 0.3226     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3521 - acc: 0.3209     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3615 - acc: 0.3174     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3308 - acc: 0.3303     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3565 - acc: 0.3223     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3543 - acc: 0.3206     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3435 - acc: 0.3258     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3411 - acc: 0.3202     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3496 - acc: 0.3247     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3589 - acc: 0.3219     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3434 - acc: 0.3284     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3331 - acc: 0.3288     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3349 - acc: 0.3264     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3823 - acc: 0.3096     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3413 - acc: 0.3330     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3391 - acc: 0.3308     \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s - loss: 2.3560 - acc: 0.3175     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3587 - acc: 0.3226     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3507 - acc: 0.3247     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3344 - acc: 0.3340     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3490 - acc: 0.3276     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3431 - acc: 0.3277     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3659 - acc: 0.3168     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3401 - acc: 0.3268     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3391 - acc: 0.3264     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3560 - acc: 0.3163     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3479 - acc: 0.3253     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3321 - acc: 0.3331     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3355 - acc: 0.3305     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3432 - acc: 0.3255     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3386 - acc: 0.3298     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3502 - acc: 0.3281     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3343 - acc: 0.3318     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3431 - acc: 0.3278     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3437 - acc: 0.3316     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3449 - acc: 0.3262     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3544 - acc: 0.3208     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3433 - acc: 0.3334     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3295 - acc: 0.3315     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3486 - acc: 0.3253     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3262 - acc: 0.3323     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3352 - acc: 0.3283     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3658 - acc: 0.3164     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3279 - acc: 0.3378     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3468 - acc: 0.3221     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3410 - acc: 0.3292     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3386 - acc: 0.3284     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3472 - acc: 0.3226     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3354 - acc: 0.3326     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3372 - acc: 0.3289     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3341 - acc: 0.3336     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3280 - acc: 0.3373     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3585 - acc: 0.3228     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3474 - acc: 0.3250     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3479 - acc: 0.3234     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3412 - acc: 0.3306     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3426 - acc: 0.3266     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3551 - acc: 0.3240     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3603 - acc: 0.3194     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3361 - acc: 0.3276     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3480 - acc: 0.3237     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3592 - acc: 0.3222     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3480 - acc: 0.3220     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3546 - acc: 0.3208     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3260 - acc: 0.3314     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3482 - acc: 0.3246     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3522 - acc: 0.3226     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3433 - acc: 0.3263     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3535 - acc: 0.3220     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3627 - acc: 0.3227     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3520 - acc: 0.3278     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3361 - acc: 0.3283     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3426 - acc: 0.3286     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3598 - acc: 0.3173     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3468 - acc: 0.3253     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3543 - acc: 0.3216     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3570 - acc: 0.3184     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3313 - acc: 0.3276     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3419 - acc: 0.3295     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3450 - acc: 0.3245     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3529 - acc: 0.3255     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3412 - acc: 0.3308     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3304 - acc: 0.3302     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3547 - acc: 0.3299     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3546 - acc: 0.3241     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3304 - acc: 0.3284     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3431 - acc: 0.3289     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3506 - acc: 0.3237     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3409 - acc: 0.3272     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3484 - acc: 0.3305     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3472 - acc: 0.3250     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3483 - acc: 0.3227     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3336 - acc: 0.3300     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3426 - acc: 0.3269     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3102 - acc: 0.3375     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3352 - acc: 0.3301     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3362 - acc: 0.3302     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3621 - acc: 0.3149     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3310 - acc: 0.3283     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3403 - acc: 0.3244     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3553 - acc: 0.3178     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3417 - acc: 0.3240     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3441 - acc: 0.3236     \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s - loss: 2.3590 - acc: 0.3195     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3578 - acc: 0.3222     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3360 - acc: 0.3327     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3635 - acc: 0.3145     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3322 - acc: 0.3276     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3685 - acc: 0.3155     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3644 - acc: 0.3168     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3348 - acc: 0.3265     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3417 - acc: 0.3261     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3320 - acc: 0.3310     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3654 - acc: 0.3191     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3411 - acc: 0.3279     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3288 - acc: 0.3274     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3478 - acc: 0.3253     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3363 - acc: 0.3282     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3511 - acc: 0.3238     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3437 - acc: 0.3256     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3404 - acc: 0.3259     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3590 - acc: 0.3167     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3489 - acc: 0.3207     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3472 - acc: 0.3274     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3332 - acc: 0.3287     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3252 - acc: 0.3336     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3426 - acc: 0.3281     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3492 - acc: 0.3272     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3396 - acc: 0.3309     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3401 - acc: 0.3259     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3407 - acc: 0.3267     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3462 - acc: 0.3233     \n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s - loss: 2.3333 - acc: 0.3324     \n",
      "Epoch 1/1\n",
      "7268/7268 [==============================] - 2s - loss: 2.3423 - acc: 0.3279     \n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "first_trainX = first_name_struct[trainIndex]\n",
    "first_testX = first_name_struct[testIndex]\n",
    "last_trainX = last_name_struct[trainIndex]\n",
    "last_testX = last_name_struct[testIndex]\n",
    "trainY = labels[trainIndex]\n",
    "testY = labels[testIndex]\n",
    "\n",
    "#trainX =np.array([to_categorical(x,nb_classes=len(struct_dict_keys)+1) for x in trainX])\n",
    "#testX =np.array([to_categorical(x,nb_classes=len(struct_dict_keys)+1) for x in testX])\n",
    "\n",
    "mini_batch_size = 10000\n",
    "len_mini_batch = round(len(trainY)/mini_batch_size)\n",
    "batch_size = 1000\n",
    "\n",
    "for x in range(10):\n",
    "    for y in range(len_mini_batch):\n",
    "        y_first_trainX, y_last_trainX, batch_range =  generate_batch(first_trainX,last_trainX,y,mini_batch_size)\n",
    "        model.fit([y_first_trainX, y_last_trainX],trainY[batch_range],epochs=1,batch_size=batch_size)\n",
    "        #scores = model.evaluate([first_testX, last_testX],testY,verbose=0)\n",
    "        #print(\"Accuracy: %.2f%%\" %(scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3415 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3498 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3490 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3557 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3430 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3428 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3461 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3501 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3491 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3422 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3318 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3446 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3456 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3455 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3472 - acc: 0.3236    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3430 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3367 - acc: 0.3294    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3515 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3367 - acc: 0.3292    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 15s - loss: 2.3417 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3449 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3443 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3426 - acc: 0.3251    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3411 - acc: 0.3276    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3455 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3505 - acc: 0.3220    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3414 - acc: 0.3269    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3437 - acc: 0.3252    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3439 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3535 - acc: 0.3212    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3427 - acc: 0.3271    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3431 - acc: 0.3284    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3463 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3493 - acc: 0.3242    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3397 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3363 - acc: 0.3241    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3406 - acc: 0.3279    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3457 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3443 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3426 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3412 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3468 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3409 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3474 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3476 - acc: 0.3222    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3478 - acc: 0.3227    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3488 - acc: 0.3230    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3471 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3446 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3408 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3407 - acc: 0.3286    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3415 - acc: 0.3287    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3397 - acc: 0.3280    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3402 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3401 - acc: 0.3295    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3486 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3482 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3439 - acc: 0.3254    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3497 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3451 - acc: 0.3245    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3438 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3430 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3430 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3338 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3470 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3507 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3464 - acc: 0.3239    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3403 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3474 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3374 - acc: 0.3297    \n",
      "Epoch 1/1\n",
      "47268/47268 [==============================] - 13s - loss: 2.3398 - acc: 0.3272    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3411 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3497 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3487 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3555 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3427 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3426 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3461 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3500 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3490 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3422 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3317 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3446 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3456 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3453 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3472 - acc: 0.3236    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3430 - acc: 0.3282    \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 14s - loss: 2.3367 - acc: 0.3294    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3515 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3365 - acc: 0.3292    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3416 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3448 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3441 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3426 - acc: 0.3251    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3410 - acc: 0.3276    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3455 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3504 - acc: 0.3220    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3414 - acc: 0.3269    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3437 - acc: 0.3252    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3439 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3535 - acc: 0.3212    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3426 - acc: 0.3271    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3431 - acc: 0.3284    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3461 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3493 - acc: 0.3242    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3397 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3363 - acc: 0.3241    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3406 - acc: 0.3279    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3456 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3442 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3425 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3412 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3468 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3410 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3474 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3475 - acc: 0.3222    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3478 - acc: 0.3227    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3489 - acc: 0.3230    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3472 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3445 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3408 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3406 - acc: 0.3286    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3414 - acc: 0.3287    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3396 - acc: 0.3280    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3402 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3400 - acc: 0.3295    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3486 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3482 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3438 - acc: 0.3254    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3496 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3450 - acc: 0.3245    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3436 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 15s - loss: 2.3430 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3429 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3339 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3470 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3507 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3464 - acc: 0.3239    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3403 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3473 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3372 - acc: 0.3297    \n",
      "Epoch 1/1\n",
      "47268/47268 [==============================] - 13s - loss: 2.3399 - acc: 0.3272    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 15s - loss: 2.3412 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3497 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3486 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3556 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3426 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3427 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3459 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3500 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3489 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3420 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3316 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3445 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3455 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3452 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3471 - acc: 0.3236    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3429 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3366 - acc: 0.3294    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3514 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3365 - acc: 0.3292    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3416 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3448 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3441 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3425 - acc: 0.3251    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3410 - acc: 0.3276    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3455 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3504 - acc: 0.3220    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3415 - acc: 0.3269    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 15s - loss: 2.3436 - acc: 0.3252    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3437 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 62s - loss: 2.3534 - acc: 0.3212    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 59s - loss: 2.3427 - acc: 0.3271    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3430 - acc: 0.3284    \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 14s - loss: 2.3462 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 32s - loss: 2.3493 - acc: 0.3242    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3398 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3365 - acc: 0.3241    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 45s - loss: 2.3405 - acc: 0.3279    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 48s - loss: 2.3457 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 70s - loss: 2.3442 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3426 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3412 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 51s - loss: 2.3469 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 65s - loss: 2.3407 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3475 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3475 - acc: 0.3222    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 56s - loss: 2.3476 - acc: 0.3227    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3488 - acc: 0.3230    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3471 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3445 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 55s - loss: 2.3406 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3406 - acc: 0.3286    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3416 - acc: 0.3287    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3394 - acc: 0.3280    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 58s - loss: 2.3401 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3399 - acc: 0.3295    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3486 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3482 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 56s - loss: 2.3438 - acc: 0.3254    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 65s - loss: 2.3497 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3449 - acc: 0.3245    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3437 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 58s - loss: 2.3430 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 68s - loss: 2.3431 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3336 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3471 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 54s - loss: 2.3507 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3464 - acc: 0.3239    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3404 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3474 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 56s - loss: 2.3372 - acc: 0.3297    \n",
      "Epoch 1/1\n",
      "47268/47268 [==============================] - 65s - loss: 2.3399 - acc: 0.3272    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3410 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3498 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 56s - loss: 2.3487 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3555 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3425 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3428 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 59s - loss: 2.3461 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3501 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3489 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3420 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 56s - loss: 2.3310 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3445 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3454 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3453 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 58s - loss: 2.3471 - acc: 0.3236    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3429 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3367 - acc: 0.3294    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3514 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 56s - loss: 2.3365 - acc: 0.3292    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3415 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3447 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3444 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 58s - loss: 2.3426 - acc: 0.3251    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3410 - acc: 0.3276    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3455 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3504 - acc: 0.3220    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 55s - loss: 2.3414 - acc: 0.3269    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3438 - acc: 0.3252    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3439 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3535 - acc: 0.3212    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 59s - loss: 2.3427 - acc: 0.3271    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3428 - acc: 0.3285    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3462 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3492 - acc: 0.3242    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 55s - loss: 2.3396 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 64s - loss: 2.3362 - acc: 0.3241    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3405 - acc: 0.3278    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3455 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 59s - loss: 2.3440 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 64s - loss: 2.3425 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3413 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3467 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 57s - loss: 2.3408 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3473 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3474 - acc: 0.3222    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3477 - acc: 0.3227    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 58s - loss: 2.3487 - acc: 0.3230    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3470 - acc: 0.3257    \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 66s - loss: 2.3444 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3407 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 59s - loss: 2.3407 - acc: 0.3286    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 62s - loss: 2.3416 - acc: 0.3287    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3397 - acc: 0.3280    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3403 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 60s - loss: 2.3396 - acc: 0.3295    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 62s - loss: 2.3487 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3483 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3437 - acc: 0.3254    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 60s - loss: 2.3497 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 61s - loss: 2.3451 - acc: 0.3245    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3438 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3430 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 60s - loss: 2.3432 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 62s - loss: 2.3337 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3470 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3506 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 60s - loss: 2.3463 - acc: 0.3239    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 62s - loss: 2.3404 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3474 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3372 - acc: 0.3297    \n",
      "Epoch 1/1\n",
      "47268/47268 [==============================] - 63s - loss: 2.3398 - acc: 0.3272    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3408 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3496 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3486 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 55s - loss: 2.3555 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3426 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3426 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3459 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3501 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3489 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3420 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3316 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 45s - loss: 2.3444 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3454 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3454 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3468 - acc: 0.3236    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3428 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3364 - acc: 0.3294    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3510 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 69s - loss: 2.3364 - acc: 0.3292    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3412 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3448 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3441 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3425 - acc: 0.3251    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3409 - acc: 0.3276    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3453 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3503 - acc: 0.3220    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3414 - acc: 0.3269    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 30s - loss: 2.3436 - acc: 0.3252    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3436 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3534 - acc: 0.3212    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3424 - acc: 0.3271    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3427 - acc: 0.3285    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3462 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3491 - acc: 0.3242    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3395 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3364 - acc: 0.3241    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3405 - acc: 0.3278    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3456 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3440 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3422 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3410 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3466 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3407 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3476 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3473 - acc: 0.3222    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3478 - acc: 0.3227    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3486 - acc: 0.3230    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3471 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3443 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3407 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3405 - acc: 0.3286    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3416 - acc: 0.3287    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3394 - acc: 0.3280    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3402 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 51s - loss: 2.3399 - acc: 0.3295    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 65s - loss: 2.3487 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3481 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3435 - acc: 0.3254    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3498 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3449 - acc: 0.3245    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3438 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3428 - acc: 0.3264    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3432 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3336 - acc: 0.3282    \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 66s - loss: 2.3469 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3504 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 36s - loss: 2.3459 - acc: 0.3239    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3405 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3475 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3372 - acc: 0.3297    \n",
      "Epoch 1/1\n",
      "47268/47268 [==============================] - 63s - loss: 2.3396 - acc: 0.3272    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 65s - loss: 2.3407 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 65s - loss: 2.3495 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3485 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3552 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3426 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3424 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3462 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3500 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3491 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3421 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3315 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3445 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3451 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3453 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3468 - acc: 0.3236    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3429 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3364 - acc: 0.3294    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3510 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3364 - acc: 0.3292    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 48s - loss: 2.3413 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3449 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3439 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3427 - acc: 0.3251    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3409 - acc: 0.3276    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3453 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3500 - acc: 0.3220    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3412 - acc: 0.3269    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3435 - acc: 0.3252    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3437 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3532 - acc: 0.3212    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3422 - acc: 0.3271    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3428 - acc: 0.3284    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3461 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3493 - acc: 0.3242    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 50s - loss: 2.3396 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 65s - loss: 2.3363 - acc: 0.3241    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3405 - acc: 0.3279    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3456 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3440 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3423 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3411 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3465 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 69s - loss: 2.3408 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3471 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3474 - acc: 0.3222    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3480 - acc: 0.3227    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 36s - loss: 2.3487 - acc: 0.3230    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3469 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3444 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3410 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3405 - acc: 0.3286    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3414 - acc: 0.3287    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3394 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3401 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3397 - acc: 0.3295    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3485 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3480 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3437 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3496 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3448 - acc: 0.3245    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3435 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3430 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3431 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3335 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3469 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3504 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3460 - acc: 0.3239    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3401 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3474 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3373 - acc: 0.3297    \n",
      "Epoch 1/1\n",
      "47268/47268 [==============================] - 55s - loss: 2.3395 - acc: 0.3272    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3407 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3494 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3488 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3555 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3424 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3425 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 59s - loss: 2.3459 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 62s - loss: 2.3497 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3491 - acc: 0.3231    \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 66s - loss: 2.3422 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3318 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3443 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3456 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3452 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3470 - acc: 0.3236    \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-9dcd0069fede>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_mini_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0my_first_trainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_last_trainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_range\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_trainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlast_trainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_first_trainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_last_trainX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m#scores = model.evaluate([first_testX, last_testX],testY,verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#print(\"Accuracy: %.2f%%\" %(scores[1]*100))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mini_batch_size = 50000\n",
    "len_mini_batch = round(len(trainY)/mini_batch_size)\n",
    "batch_size = 5000\n",
    "\n",
    "for x in range(10):\n",
    "    for y in range(len_mini_batch):\n",
    "        y_first_trainX, y_last_trainX, batch_range =  generate_batch(first_trainX,last_trainX,y,mini_batch_size)\n",
    "        model.fit([y_first_trainX, y_last_trainX],trainY[batch_range],epochs=1,batch_size=batch_size)\n",
    "        #scores = model.evaluate([first_testX, last_testX],testY,verbose=0)\n",
    "        #print(\"Accuracy: %.2f%%\" %(scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 186s - loss: 0.0116 - acc: 0.9991   \n",
      "Accuracy: 85.43%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 184s - loss: 0.0069 - acc: 0.9994   \n",
      "Accuracy: 85.30%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 184s - loss: 0.0040 - acc: 0.9998   \n",
      "Accuracy: 85.37%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 187s - loss: 0.0023 - acc: 1.0000   \n",
      "Accuracy: 85.66%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 186s - loss: 0.0016 - acc: 1.0000   \n",
      "Accuracy: 85.66%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 182s - loss: 0.0013 - acc: 1.0000   \n",
      "Accuracy: 85.70%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 190s - loss: 0.0011 - acc: 1.0000   \n",
      "Accuracy: 85.69%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 186s - loss: 9.3359e-04 - acc: 1.0000   \n",
      "Accuracy: 85.75%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 189s - loss: 8.2290e-04 - acc: 1.0000   \n",
      "Accuracy: 85.74%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 186s - loss: 7.3335e-04 - acc: 1.0000   \n",
      "Accuracy: 85.76%\n"
     ]
    }
   ],
   "source": [
    "for x in range(10):\n",
    "    model.fit([first_trainX, last_trainX],trainY,epochs=1,batch_size=1000)\n",
    "    scores = model.evaluate([first_testX, last_testX],testY,verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" %(scores[1]*100))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save this model atlhouth there is something wrong with the features\n",
    "# I did not lowercase the text :(\n",
    "model_json = model.to_json()\n",
    "with open('model-keras-w3-bigram.json','w') as f:\n",
    "    f.write(model_json)\n",
    "#save the last weight\n",
    "model.save_weights('model-keras-w3-bigram-10.h5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trans_name(name):\n",
    "    name = name.lower()\n",
    "    # transform space into underscore\n",
    "    name = '_'+name.replace(' ','_')+'_'\n",
    "    #transform the name into sequence structure\n",
    "    ext_name = extract_structure(name)\n",
    "    trans_name = transform_structure(ext_name)\n",
    "    name_ds_mat = np.zeros((1,len(struct_dict_keys)),dtype=np.int32)\n",
    "    for i,x in enumerate(trans_name):\n",
    "        name_ds_mat[0,x-1]+=1\n",
    "    #trans_name = pad_sequences([trans_name], maxlen=50,value=0.)\n",
    "    return name_ds_mat\n",
    "\n",
    "def predict_ethnicity(fname,lname):\n",
    "    # lower case the name\n",
    "    fnamex = trans_name(fname)\n",
    "    lnamex = trans_name(lname)\n",
    "    pred = model.predict([np.array(fnamex),np.array(lnamex)])\n",
    "    pred_class = np.argsort(pred[0])[::-1]\n",
    "    return_item = []\n",
    "    for x in np.argsort(pred[0])[::-1]:\n",
    "        return_item.append((ethnic_keys[x],pred[0][x]))\n",
    "    return return_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('japanese', 1.0),\n",
       " ('indian', 5.0325966e-08),\n",
       " ('african', 1.4800873e-08),\n",
       " ('indonesian', 8.8743637e-11),\n",
       " ('arab', 8.7172121e-13),\n",
       " ('korean', 3.7096197e-13),\n",
       " ('slav', 1.1345596e-13),\n",
       " ('german', 5.7383182e-14),\n",
       " ('thai', 5.4225884e-14),\n",
       " ('chinese', 2.72746e-14),\n",
       " ('english', 3.5170739e-15),\n",
       " ('israeli', 1.5735749e-15),\n",
       " ('french', 2.2933817e-17),\n",
       " ('turkish', 1.2016868e-17),\n",
       " ('greek', 1.1056446e-17),\n",
       " ('nordic', 7.038771e-18),\n",
       " ('hispanic', 6.3994195e-18),\n",
       " ('hungarian', 5.473129e-18),\n",
       " ('vietnamese', 5.199001e-18),\n",
       " ('dutch', 4.4446176e-18),\n",
       " ('baltic', 4.1480262e-18),\n",
       " ('romanian', 1.9028031e-18),\n",
       " ('italian', 1.5801921e-22)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ethnicity('Shinji','Kagawa')\n",
    "#trans_name('Nikolaus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedd the structure vocabulary using text embedding and reduce the dimensionality\n",
    "\n",
    "# convert the names into word structure vector\n",
    "struct_dict_keys = list(struct_dict.keys())\n",
    "\n",
    "def transform_structure(name_struct):\n",
    "    list_structure = []\n",
    "    for x in name_struct:\n",
    "        try:\n",
    "            list_structure.append(struct_dict_keys.index(x)+1)\n",
    "        except BaseException:\n",
    "            list_structure.append(0)\n",
    "    #add pading 0 for structure less than num_input\n",
    "    #for i in range(len(list_structure),timesteps):\n",
    "    #    list_structure.append(0)\n",
    "    return list_structure   \n",
    "    #return [*map(lambda x:struct_dict_keys.index(x)+1, name_struct)]\n",
    "\n",
    "#data_source = full_name_struct.apply(lambda x: transform_structure(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(x):\n",
    "    y = np.zeros(len(ethnic_keys))\n",
    "    y[ethnic_keys.index(x)]=1\n",
    "    return y\n",
    "\n",
    "labels = np.array(list(map(lambda x: transform_labels(x),ethnic_series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using tflearn make the graph creation simple\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# separate train and training set\n",
    "trainX, testX, trainY, testY = train_test_split(data_source,[ethnic_keys.index(x) for x in ethnic_series],test_size = 0.2)\n",
    "\n",
    "trainX = pad_sequences(trainX, maxlen=50,value=0.)\n",
    "testX = pad_sequences(testX,maxlen=50,value=0.)\n",
    "# Converting labels to binary vectors\n",
    "trainY = to_categorical(trainY,nb_classes=len(ethnic_keys))\n",
    "testY = to_categorical(testY,nb_classes=len(ethnic_keys))    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pickle\n",
    "with open('train_test_fix.pickle','wb') as f:\n",
    "    pickle.dump((trainX,trainY,testX,testY,ethnic_keys,struct_dict_keys),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('train_test_fix.pickle', 'rb') as f:\n",
    "    trainX,trainY,testX,testY,ethnic_keys,struct_dict_keys = pickle.load(f)\n",
    "    #aha = pickle.load(f)\n",
    "\n",
    "#with open('traintest-smote.pickle','rb') as f:\n",
    "#    train_res,test_res = pickle.load(f)\n",
    "\n",
    "with open('ethnic_keys.pickle','rb') as f:\n",
    "    name_struct_keys,ethnic_keys = pickle.load(f)\n",
    "        \n",
    "embedding_vector_length = 1000\n",
    "lstm_layer = 1000\n",
    "max_sequence = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert categorical to binary crossentropy\n",
    "#trainY = np.array([np.where(x>0)[0][0] for x in trainY])\n",
    "#testY = np.array([np.where(x>0)[0][0] for x in testY])\n",
    "\n",
    "#test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 1000)          62696000  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 50, 1000)          3001000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               880800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 23)                4623      \n",
      "=================================================================\n",
      "Total params: 66,582,423\n",
      "Trainable params: 66,582,423\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 778s - loss: 1.6879 - acc: 0.5265    \n",
      "Accuracy: 75.88%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 796s - loss: 0.5197 - acc: 0.8582    \n",
      "Accuracy: 84.37%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 817s - loss: 0.2142 - acc: 0.9437    \n",
      "Accuracy: 85.12%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 779s - loss: 0.0989 - acc: 0.9755    \n",
      "Accuracy: 85.57%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 29131s - loss: 0.0476 - acc: 0.9905   \n",
      "Accuracy: 85.63%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 4804s - loss: 0.0258 - acc: 0.9953   \n",
      "Accuracy: 85.55%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 35875s - loss: 0.0138 - acc: 0.9980   \n",
      "Accuracy: 85.79%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 745s - loss: 0.0090 - acc: 0.9988    \n",
      "Accuracy: 85.69%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 742s - loss: 0.0068 - acc: 0.9992    \n",
      "Accuracy: 85.82%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 741s - loss: 0.0048 - acc: 0.9996    \n",
      "Accuracy: 85.62%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(name_struct_keys)+1,embedding_vector_length,input_length=max_sequence))\n",
    "model.add(Conv1D(filters=embedding_vector_length,kernel_size=3,padding='same',activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=2))\n",
    "#model.add(LSTM(lstm_layer,dropout=0.8))\n",
    "model.add(Bidirectional(LSTM(max_sequence*2,return_sequences=False),input_shape=(max_sequence,1)))\n",
    "#model.add(TimeDistributed(keras.layers.Dense(len(ethnic_keys),activation='softmax')))\n",
    "model.add(keras.layers.Dense(len(ethnic_keys),activation='softmax'))\n",
    "#model.add(keras.layers.Dense(len(ethnic_keys),activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "for x in range(10):\n",
    "    model.fit(trainX,trainY,epochs=1,batch_size=1000)\n",
    "    scores = model.evaluate(testX,testY,verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" %(scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  112,  1331,  1332, ...,     0,     0,     0],\n",
       "       [  251,   252,   232, ...,     0,     0,     0],\n",
       "       [ 4633, 11731, 23123, ...,     0,     0,     0],\n",
       "       ..., \n",
       "       [  696,   697,   698, ...,     0,     0,     0],\n",
       "       [   25,    26,  5195, ...,     0,     0,     0],\n",
       "       [ 8671,  8672,  6568, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# save this model atlhouth there is something wrong with the features\n",
    "# I did not lowercase the text :(\n",
    "model_json = model.to_json()\n",
    "with open('model-keras-embed-bilstm-womaxpool.json','w') as f:\n",
    "    f.write(model_json)\n",
    "#save the last weight\n",
    "model.save_weights('model-keras-embed-bilstm-womaxpool-10.h5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "\n",
    "# compute the accuracy\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "    #print(precision)\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "    #print(recall)\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    recall = c1 / c3\n",
    "\n",
    "    return recall\n",
    "\n",
    "# load model\n",
    "# load json and create model\n",
    "json_file = open('model-keras-embed-bilstm-womaxpool.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "#                               ,custom_objects= {'f1_score': f1_score})\n",
    "loaded_model.load_weights(\"model-keras-embed-bilstm-womaxpool-10.h5\")\n",
    "\n",
    "loaded_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy',f1_score,precision,recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = loaded_model.evaluate(testX,testY,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8561812878216707, F1: 0.8601400889915978, Precision: 0.8757469038779302, Recal: 0.845523895023381\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {}, F1: {}, Precision: {}, Recal: {}'.format(scores[1],scores[2],scores[3],scores[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "\n",
    "# transform prediction\n",
    "# given name compute the prediction\n",
    "def predict_ethnicity(name):\n",
    "    # lower case the name\n",
    "    name = name.lower()\n",
    "    # transform space into underscore\n",
    "    name = '_'+name.replace(' ','_')+'_'\n",
    "    #transform the name into sequence structure\n",
    "    ext_name = extract_structure(name)\n",
    "    trans_name = transform_structure(ext_name)\n",
    "    trans_name = pad_sequences([trans_name], maxlen=50,value=0.)\n",
    "    pred = loaded_model.predict(trans_name)\n",
    "    pred_class = np.argsort(pred[0])[::-1]\n",
    "    return_item = []\n",
    "    for x in np.argsort(pred[0])[::-1]:\n",
    "        return_item.append((ethnic_keys[x],pred[0][x]))\n",
    "    return return_item\n",
    "\n",
    "name='helen lamothe'\n",
    "ext_name = extract_structure(name)\n",
    "#print(ext_name)\n",
    "trans_name = transform_structure(ext_name)\n",
    "#trans_name\n",
    "pad_sequences([trans_name], maxlen=50,value=0.)\n",
    "#extract_structure('Nikolaus Nova')\n",
    "#transform_structure('Robert Nova')\n",
    "ethnic_prob = predict_ethnicity('Filho  Elias Abdalla')\n",
    "#ethnic_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.22544670e-03,   2.45175033e-05,   5.54050894e-06,\n",
       "          3.56604069e-05,   1.31730601e-04,   1.58663862e-03,\n",
       "          9.95586514e-01,   3.65224201e-04,   4.18016425e-05,\n",
       "          1.64734403e-04,   3.29397808e-05,   1.42851750e-05,\n",
       "          5.77516516e-07,   3.13429664e-05,   1.98912196e-04,\n",
       "          5.65968139e-06,   4.63458673e-06,   4.45792568e-04,\n",
       "          5.28864875e-05,   2.15678101e-05,   8.53615438e-06,\n",
       "          4.57901763e-07,   1.44505730e-05]], dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = loaded_model.predict(trainX[10].reshape(1,50))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  5,  0, 17,  7, 14,  9,  4, 18,  8,  3, 10, 13,  1, 19, 22, 11,\n",
       "       20, 15,  2, 16, 12, 21])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(test)[0][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6]),)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(trainY[10]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,  2304,  2305,     0,     0,     0,     0,\n",
       "         4088, 15559,  2523, 15102,     0,     0,     0,    24,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name='_helen__lamothe_'\n",
    "ext_name = extract_structure(name)\n",
    "#print(ext_name)\n",
    "trans_name = transform_structure(ext_name)\n",
    "#trans_name\n",
    "pad_sequences([trans_name], maxlen=50,value=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ITALIAN', 0.52735686),\n",
       " ('INDIAN', 0.30269179),\n",
       " ('KOREAN', 0.097788125),\n",
       " ('JAPANESE', 0.017470013),\n",
       " ('ROMANIAN', 0.015210837),\n",
       " ('TURKISH', 0.014220745),\n",
       " ('HISPANIC', 0.0092350421),\n",
       " ('GERMAN', 0.0046770978),\n",
       " ('BALTIC', 0.0032188322),\n",
       " ('ARAB', 0.0028918688),\n",
       " ('ISRAELI', 0.0022467086),\n",
       " ('GREEK', 0.0014537659),\n",
       " ('SLAV', 0.00046977124),\n",
       " ('NORDIC', 0.00024971511),\n",
       " ('HUNGARIAN', 0.0002173665),\n",
       " ('DUTCH', 0.00021363674),\n",
       " ('INDONESIAN', 0.00019762212),\n",
       " ('VIETNAMESE', 9.9613972e-05),\n",
       " ('AFRICAN', 3.4369114e-05),\n",
       " ('CHINESE', 3.3702971e-05),\n",
       " ('ENGLISH', 1.7328795e-05),\n",
       " ('FRENCH', 4.1550811e-06),\n",
       " ('THAI', 9.4588233e-07)]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ethnicity('harry potter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ISRAELI'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethnic_keys[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           _Elias_Abdalla__Filho_HISPANIC\n",
       "1                                  _Jad__Bou_Abdallah_ARAB\n",
       "2                                  _Ayman__Abdel_Aziz_ARAB\n",
       "3                                  _Salma__Abdelmoula_ARAB\n",
       "4                                     _Ibrahim__Abdou_ARAB\n",
       "5                             _Hazem__Abou_El_Fettouh_ARAB\n",
       "6                                     _Rola__Aboutaam_ARAB\n",
       "7        _Aida_Alexandra__Alvim_de_Abreu_Silva_Rodrigue...\n",
       "8             _Isabel__Cristina_Affonso_Scaletsky_HISPANIC\n",
       "9                                _Tsiri__Agbenyega_AFRICAN\n",
       "10                     _Jose__Maria_Aguado_Garcia_HISPANIC\n",
       "11                      _Manuela__Aguilar_Guisado_HISPANIC\n",
       "12                         _Arturo__Aguillon_Luna_HISPANIC\n",
       "13                                   _Ali__Ahmadzadeh_ARAB\n",
       "14                                    _Ahmed__Ibrahim_ARAB\n",
       "15                                    _Ahmed__Letaief_ARAB\n",
       "16                           _Mohammed__Shakeel_Ahmed_ARAB\n",
       "17                              _Alev__Aksoy_Dogan_TURKISH\n",
       "18                    _Mitsumi__Nakatsukasa_Akune_JAPANESE\n",
       "19                                   _Filiz__Akyuz_TURKISH\n",
       "20                                _Omaimah__Al_Gohary_ARAB\n",
       "21                                  _Jassim__Al_Jedah_ARAB\n",
       "22                                  _Shadi__Al_Khatib_ARAB\n",
       "23                                   _Azmi__Al_Najjar_ARAB\n",
       "24                           _Emilio__Alba_Conejo_HISPANIC\n",
       "25                     _Sheila_Alcantara__Llaguno_HISPANIC\n",
       "26                               _Marcos__Alcocer_HISPANIC\n",
       "27                       _Maria__Grazia_Alessandri_ITALIAN\n",
       "28                                  _Randa__Ali_Labib_ARAB\n",
       "29                        _Shameez__Allie_Hamdulay_AFRICAN\n",
       "                               ...                        \n",
       "44537                                _Zhihong__Liu_CHINESE\n",
       "44538                              _Zhen_ying__Liu_CHINESE\n",
       "44539                                    _Zhu__Liu_CHINESE\n",
       "44540                               _Zhengjie__Liu_CHINESE\n",
       "44541                              _Zhengchun__Liu_CHINESE\n",
       "44542                                _Ziliang__Liu_CHINESE\n",
       "44543                                    _Zhe__Liu_CHINESE\n",
       "44544                                 _Zulian__Liu_CHINESE\n",
       "44545                               _Zun_chun__Liu_CHINESE\n",
       "44546                                _Ze_zhou__Liu_CHINESE\n",
       "44547                               _Zhaochun__Liu_CHINESE\n",
       "44548                                 _ziying__Liu_CHINESE\n",
       "44549                                 _zhenyu__Liu_CHINESE\n",
       "44550                              _kenzo__Tanaka_JAPANESE\n",
       "44551                             _kimiko__Tanaka_JAPANESE\n",
       "44552                           _kazuhiro__Tanaka_JAPANESE\n",
       "44553                               _koji__Tanaka_JAPANESE\n",
       "44554                             _Kousei__Tanaka_JAPANESE\n",
       "44555                              _Kyoko__Tanaka_JAPANESE\n",
       "44556                              _Kenji__Tanaka_JAPANESE\n",
       "44557                              _keiji__Tanaka_JAPANESE\n",
       "44558                              _keiji__Tanaka_JAPANESE\n",
       "44559                                _Kei__Tanaka_JAPANESE\n",
       "44560                            _kiyoshi__Tanaka_JAPANESE\n",
       "44561                            _takashi__Sasaki_JAPANESE\n",
       "44562                            _toyoshi__Sasaki_JAPANESE\n",
       "44563                             _tetsuo__Sasaki_JAPANESE\n",
       "44564                             _takayo__Sasaki_JAPANESE\n",
       "44565                                    _may__Haddad_ARAB\n",
       "44566                             _Marianne__Haddad_FRENCH\n",
       "Length: 44567, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethnea_df['First']+ethnea_df['Last']+ethnea_df['Ethnea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
