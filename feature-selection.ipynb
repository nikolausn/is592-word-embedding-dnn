{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('traintest.pickle', 'rb') as f:\n",
    "    trainX,testX,trainY,testY = pickle.load(f)\n",
    "\n",
    "#with open('traintest-smote.pickle','rb') as f:\n",
    "#    train_res,test_res = pickle.load(f)\n",
    "\n",
    "with open('ethnic_keys.pickle','rb') as f:\n",
    "    name_struct_keys,ethnic_keys = pickle.load(f)\n",
    "        \n",
    "embedding_vector_length = 1000\n",
    "lstm_layer = 1000\n",
    "max_sequence = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainY = np.array([np.where(x>0)[0][0] for x in trainY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  207,  3513,  6997, ...,     0,     0,     0],\n",
       "       [   59,  2867,  9923, ...,     0,     0,     0],\n",
       "       [  837,  1918,  1396, ...,     0,     0,     0],\n",
       "       ..., \n",
       "       [10016, 11765, 14255, ...,     0,     0,     0],\n",
       "       [   59,   428,  5269, ...,     0,     0,     0],\n",
       "       [ 1498,  2819,  2848, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feat_rows = np.zeros([len(trainX),len(name_struct_keys)],dtype=np.bool)\n",
    "feat_rows = np.zeros([len(trainX),len(name_struct_keys)],dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,x in enumerate(trainX):\n",
    "    for j in x:\n",
    "        #feat_rows[i,j-1]=True\n",
    "        feat_rows[i,j-1]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# use linear SVC for feature selection\n",
    "lsvc = LinearSVC(C=0.01,penalty='l1',dual=False).fit(feat_rows,trainY)\n",
    "model =SelectFromModel(lsvc,prefit=True)\n",
    "X_new = model.transform(feat_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35653, 62695)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35653, 591)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(915,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(model.estimator.coef_!=0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = model.estimator.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 62695)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected = np.array([np.sum(test[:,x]!=0) for x in range(test.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_coef = np.where(selected>0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_E',\n",
       " '_Abd',\n",
       " 'lla_',\n",
       " 'la__',\n",
       " 'o_',\n",
       " '',\n",
       " '_J',\n",
       " 'ad__',\n",
       " '__Bo',\n",
       " 'h_',\n",
       " '_A',\n",
       " 'man_',\n",
       " 'an__',\n",
       " '__Ab',\n",
       " 'z_',\n",
       " '_S',\n",
       " '_Sa',\n",
       " 'a__A',\n",
       " 'la_',\n",
       " 'a_',\n",
       " '_I',\n",
       " 'ou_',\n",
       " 'u_',\n",
       " '_H',\n",
       " '_Ha',\n",
       " '_El_',\n",
       " '_R',\n",
       " 'ola_',\n",
       " 'm_',\n",
       " 'ndra',\n",
       " 'ra__',\n",
       " '__Al',\n",
       " '_de_',\n",
       " '_Sil',\n",
       " 'Silv',\n",
       " 'lva_',\n",
       " '_Rod',\n",
       " 'igue',\n",
       " 'ues_',\n",
       " 'es_',\n",
       " 's_',\n",
       " 'el__',\n",
       " 'isti',\n",
       " 'tina',\n",
       " 'ina_',\n",
       " 'y_',\n",
       " '_T',\n",
       " 'ri__',\n",
       " '_Jo',\n",
       " '_Jos',\n",
       " 'Jose',\n",
       " 'ose_',\n",
       " 'se__',\n",
       " 'e__M',\n",
       " '_Mar',\n",
       " 'aria',\n",
       " 'ria_',\n",
       " '_Gar',\n",
       " 'arci',\n",
       " 'cia_',\n",
       " '_M',\n",
       " '_Ma',\n",
       " 'ela_',\n",
       " 'do_',\n",
       " 'ro__',\n",
       " 'na_',\n",
       " '_Ali',\n",
       " 'eh_',\n",
       " 'med_',\n",
       " 'ed__',\n",
       " 'im_',\n",
       " 'f_',\n",
       " '_Mo',\n",
       " '_Moh',\n",
       " '__Sh',\n",
       " '_Sha',\n",
       " 'd_',\n",
       " 'gan_',\n",
       " 'an_',\n",
       " 'n_',\n",
       " 'umi_',\n",
       " 'mi__',\n",
       " '__Na',\n",
       " 'atsu',\n",
       " 'ne_',\n",
       " 'e_',\n",
       " '_F',\n",
       " '_O',\n",
       " 'ah__',\n",
       " '_Al_',\n",
       " '_Sh',\n",
       " '_Kha',\n",
       " 'ar_',\n",
       " 'r_',\n",
       " 'io__',\n",
       " 'no_',\n",
       " 'os__',\n",
       " 'er_',\n",
       " 'ia__',\n",
       " '__Gr',\n",
       " 'less',\n",
       " 'i_',\n",
       " '_Ra',\n",
       " 'nda_',\n",
       " 'ez__',\n",
       " 'er__',\n",
       " '__Mo',\n",
       " '_Mor',\n",
       " 'eira',\n",
       " 'ira_',\n",
       " 'da_',\n",
       " 'e__L',\n",
       " '_Lui',\n",
       " 'ella',\n",
       " 'a__S',\n",
       " '__Sa',\n",
       " '_B',\n",
       " 'na__',\n",
       " 'ero_',\n",
       " 'ro_',\n",
       " '_An',\n",
       " 'es__',\n",
       " '_N',\n",
       " 'sen_',\n",
       " 'en__',\n",
       " 'in_',\n",
       " 'th__',\n",
       " 'sche',\n",
       " 'her_',\n",
       " '_C',\n",
       " '_Car',\n",
       " 'rlos',\n",
       " 'osa_',\n",
       " '_Tho',\n",
       " '_Bha',\n",
       " 'ker_',\n",
       " 'va_',\n",
       " '_K',\n",
       " '_V',\n",
       " 'rez_',\n",
       " '_San',\n",
       " 'ez_',\n",
       " 'ler_',\n",
       " '__Ro',\n",
       " 'ard_',\n",
       " 'rd_',\n",
       " 'a__R',\n",
       " 'ra_',\n",
       " '_L',\n",
       " '_Fer',\n",
       " 'erna',\n",
       " 'rnan',\n",
       " 'do__',\n",
       " 'te_',\n",
       " 'io_',\n",
       " 'ani_',\n",
       " 'ni_',\n",
       " '_P',\n",
       " 'ier_',\n",
       " 'nces',\n",
       " 'cesc',\n",
       " '_Y',\n",
       " 'son_',\n",
       " 'ey_',\n",
       " 'rma_',\n",
       " 'a__M',\n",
       " '__Mc',\n",
       " 'ane_',\n",
       " 'on_',\n",
       " 'sson',\n",
       " '_Ber',\n",
       " 'ski_',\n",
       " 'ki_',\n",
       " 'el_',\n",
       " 'l_',\n",
       " 'ce__',\n",
       " 'ke__',\n",
       " 'homa',\n",
       " 'as_',\n",
       " '_G',\n",
       " 't_',\n",
       " 'cisc',\n",
       " 'co__',\n",
       " 'erto',\n",
       " 'to__',\n",
       " '__Ar',\n",
       " 'o__S',\n",
       " '__St',\n",
       " 'Rosa',\n",
       " 'sa__',\n",
       " 'uan_',\n",
       " 'Mart',\n",
       " 'rtin',\n",
       " 'on__',\n",
       " '__Ra',\n",
       " 'ano_',\n",
       " 'san_',\n",
       " 're__',\n",
       " 'chi_',\n",
       " 'ha__',\n",
       " 'arle',\n",
       " 'al_',\n",
       " 'que_',\n",
       " 'e__G',\n",
       " 'k_',\n",
       " '__Mi',\n",
       " 'ai_',\n",
       " 'ley_',\n",
       " 'w_',\n",
       " 'in__',\n",
       " 'c_',\n",
       " 'tin_',\n",
       " 'lo__',\n",
       " 'o__M',\n",
       " '_Mon',\n",
       " 'as__',\n",
       " 'is_',\n",
       " 'ner_',\n",
       " 'anne',\n",
       " 'ne__',\n",
       " 'e__B',\n",
       " '_Jon',\n",
       " 'ong_',\n",
       " 'ng_',\n",
       " 'g_',\n",
       " 'en_',\n",
       " '_Rob',\n",
       " '__Hu',\n",
       " 'uel_',\n",
       " 'o__C',\n",
       " 'lan_',\n",
       " '_U',\n",
       " 'a__B',\n",
       " 'cz_',\n",
       " 'ton_',\n",
       " '_Na',\n",
       " 'ger_',\n",
       " '_Bro',\n",
       " 'ini_',\n",
       " '_Gi',\n",
       " 'o__B',\n",
       " 'ca__',\n",
       " 'ardo',\n",
       " 'icha',\n",
       " 'ille',\n",
       " 'lle_',\n",
       " 'le__',\n",
       " 'ie__',\n",
       " '_Li',\n",
       " 'mann',\n",
       " 'ann_',\n",
       " 'nn_',\n",
       " 'ei__',\n",
       " 'ine_',\n",
       " 'ck_',\n",
       " 'elli',\n",
       " 'ana_',\n",
       " 'y__B',\n",
       " 'rre_',\n",
       " 'ova_',\n",
       " 'dro_',\n",
       " 'chae',\n",
       " 'hael',\n",
       " 'las_',\n",
       " '__Ca',\n",
       " 'Lope',\n",
       " 'nald',\n",
       " 'os_',\n",
       " 'ierr',\n",
       " 'ry__',\n",
       " '_van',\n",
       " 'ian_',\n",
       " 'n__W',\n",
       " '_Wu_',\n",
       " 'ta__',\n",
       " '_Ste',\n",
       " 'Stef',\n",
       " 'tefa',\n",
       " 'efan',\n",
       " 'li_',\n",
       " 'a__C',\n",
       " '__Ch',\n",
       " '_Chi',\n",
       " 'Paol',\n",
       " 'a__D',\n",
       " '__De',\n",
       " '_Su',\n",
       " 'un__',\n",
       " 'eng_',\n",
       " 'a__P',\n",
       " '__Pa',\n",
       " 'ti_',\n",
       " '_l',\n",
       " 'dt_',\n",
       " 'rs__',\n",
       " 'n__M',\n",
       " '_Br',\n",
       " 'ko__',\n",
       " '__Br',\n",
       " 'ey__',\n",
       " 'Brow',\n",
       " 'Ana_',\n",
       " '__Ha',\n",
       " '_Har',\n",
       " 'eth_',\n",
       " 'thy_',\n",
       " 'ins_',\n",
       " '_Ji',\n",
       " 'll__',\n",
       " 'aure',\n",
       " 'ter_',\n",
       " 'ert_',\n",
       " 'rt__',\n",
       " 'rs_',\n",
       " 'a__K',\n",
       " '__Ko',\n",
       " 'era_',\n",
       " '__de',\n",
       " 'res_',\n",
       " 'e__C',\n",
       " 'e__D',\n",
       " '_X',\n",
       " 'to_',\n",
       " 'stia',\n",
       " 'tian',\n",
       " 'Davi',\n",
       " 'avid',\n",
       " 'vid_',\n",
       " 'ell_',\n",
       " 'll_',\n",
       " 'lli_',\n",
       " '_Van',\n",
       " '_Ch',\n",
       " 'hris',\n",
       " 'pher',\n",
       " '_Mac',\n",
       " '_Wol',\n",
       " 'ke_',\n",
       " 'ang_',\n",
       " 'ui__',\n",
       " '_Yin',\n",
       " 'ing_',\n",
       " 'ng__',\n",
       " 'ska_',\n",
       " 'ung_',\n",
       " '_Cha',\n",
       " 'atha',\n",
       " '__Co',\n",
       " 'eau_',\n",
       " 'au_',\n",
       " 'vin_',\n",
       " 'ss_',\n",
       " 'han_',\n",
       " '_Yu_',\n",
       " 'ua__',\n",
       " 'ao__',\n",
       " 'is__',\n",
       " '_Pr',\n",
       " '_Pra',\n",
       " '__Ku',\n",
       " 'umar',\n",
       " 'mar_',\n",
       " '_Che',\n",
       " '_We',\n",
       " '_Wei',\n",
       " 'Chen',\n",
       " 'hen_',\n",
       " '_Z',\n",
       " '_Zh',\n",
       " '_Xia',\n",
       " '_Min',\n",
       " 'Lee_',\n",
       " 'ee_',\n",
       " '_Fa',\n",
       " '_Seo',\n",
       " '_Cho',\n",
       " 'Sun_',\n",
       " '__Su',\n",
       " 'fer_',\n",
       " '_Jia',\n",
       " '_Chu',\n",
       " 'ith_',\n",
       " 'tt_',\n",
       " 'John',\n",
       " 'ohn_',\n",
       " 'hn__',\n",
       " '_Jea',\n",
       " 'ean_',\n",
       " 'iro_',\n",
       " '_c',\n",
       " 'us__',\n",
       " '__Sc',\n",
       " '_Sch',\n",
       " 'vann',\n",
       " 'tta_',\n",
       " 'ns__',\n",
       " 'ya__',\n",
       " '_Yan',\n",
       " 'ao_',\n",
       " '__Fo',\n",
       " 'ta_',\n",
       " '__So',\n",
       " 'ony_',\n",
       " 'ashi',\n",
       " 'teve',\n",
       " 'ick_',\n",
       " 'arol',\n",
       " 'yn__',\n",
       " 'ark_',\n",
       " 'rk_',\n",
       " 're_',\n",
       " 'ma_',\n",
       " 'e__P',\n",
       " 'anie',\n",
       " '__He',\n",
       " 'no__',\n",
       " 'ori_',\n",
       " '__Va',\n",
       " 'Van_',\n",
       " 'ka__',\n",
       " 'enni',\n",
       " 'niqu',\n",
       " 'ux_',\n",
       " 'x_',\n",
       " 'ura_',\n",
       " '_Mas',\n",
       " 'aus_',\n",
       " 'eter',\n",
       " '_Hu',\n",
       " 'izab',\n",
       " 'ka_',\n",
       " 'lu_',\n",
       " 'ther',\n",
       " 'ko_',\n",
       " 'uo__',\n",
       " 'ar__',\n",
       " 'arie',\n",
       " 'ward',\n",
       " 'rd__',\n",
       " 'ich_',\n",
       " 'ch_',\n",
       " 'rg_',\n",
       " 'rk__',\n",
       " 'mad_',\n",
       " 'mes_',\n",
       " 'om_',\n",
       " 'ic__',\n",
       " '__Li',\n",
       " '_Li_',\n",
       " '__Ka',\n",
       " '_p',\n",
       " 'olo_',\n",
       " 'aul_',\n",
       " 'ly__',\n",
       " 'hell',\n",
       " 'nn__',\n",
       " 'usan',\n",
       " 'am__',\n",
       " 'ford',\n",
       " 'ncoi',\n",
       " 'cois',\n",
       " 'ent_',\n",
       " '_Shi',\n",
       " '__Fu',\n",
       " 'moto',\n",
       " 'oto_',\n",
       " 'Masa',\n",
       " 'oshi',\n",
       " 'aka_',\n",
       " 'ama_',\n",
       " 'ain_',\n",
       " 'v_',\n",
       " 'one_',\n",
       " 'ones',\n",
       " 'ishn',\n",
       " 'shna',\n",
       " 'tz_',\n",
       " 'illi',\n",
       " 'lip_',\n",
       " 'char',\n",
       " 'hard',\n",
       " 'een_',\n",
       " 'ce_',\n",
       " '_Hua',\n",
       " 'len_',\n",
       " 'uo_',\n",
       " 'rah_',\n",
       " 'ci_',\n",
       " 'nen_',\n",
       " 'ham_',\n",
       " 'y__S',\n",
       " 'ew__',\n",
       " 'ns_',\n",
       " 'sh__',\n",
       " '_Hi',\n",
       " 'ki__',\n",
       " 'shi_',\n",
       " 'hi__',\n",
       " '__Ke',\n",
       " '__Zh',\n",
       " 'ic_',\n",
       " 'mura',\n",
       " '__Ya',\n",
       " 'cott',\n",
       " 'ott_',\n",
       " 'llia',\n",
       " 'o__I',\n",
       " 'aki_',\n",
       " 'uki_',\n",
       " 'drea',\n",
       " 'ames',\n",
       " '_Wan',\n",
       " 'ore_',\n",
       " 'Kim_',\n",
       " 'th_',\n",
       " 'hiro',\n",
       " '_Yam',\n",
       " 'Yama',\n",
       " '_Hyu',\n",
       " '__Ta',\n",
       " '_Tak',\n",
       " 'i__K',\n",
       " 'hili',\n",
       " '__Kr',\n",
       " 'o__K',\n",
       " 'ans_',\n",
       " 'rg__',\n",
       " 'ight',\n",
       " '_y',\n",
       " 'et_',\n",
       " 'Lin_',\n",
       " 'lf__',\n",
       " 'mith',\n",
       " 'or_',\n",
       " 'aret',\n",
       " 'gen_',\n",
       " 'Park',\n",
       " '__Wi',\n",
       " 'rris',\n",
       " 'my__',\n",
       " '_Hir',\n",
       " 'ichi',\n",
       " 'dy__',\n",
       " '__O_',\n",
       " 'kazu',\n",
       " 'oon_',\n",
       " '_s',\n",
       " '_Q',\n",
       " 'ifer',\n",
       " '_Raj',\n",
       " 'ch__',\n",
       " '_Sin',\n",
       " '_b',\n",
       " '_j',\n",
       " 'lipp',\n",
       " 'aman',\n",
       " 'ingh',\n",
       " '_Hei',\n",
       " '_a',\n",
       " '_r',\n",
       " '_t',\n",
       " 'hel_',\n",
       " 'cy__',\n",
       " '_d',\n",
       " '_h',\n",
       " 'wa_',\n",
       " 'suke',\n",
       " 'ov_',\n",
       " '_ra',\n",
       " '_k',\n",
       " '_ro',\n",
       " 'ad_',\n",
       " 'be_',\n",
       " 'awa_',\n",
       " 'ru__',\n",
       " 'ji__',\n",
       " '_Aki',\n",
       " 'Mill',\n",
       " 'kar_',\n",
       " 'yama',\n",
       " 'masa',\n",
       " 'phen',\n",
       " 'ig_',\n",
       " 'ippe',\n",
       " '__Wh',\n",
       " 'aude',\n",
       " '__Xi',\n",
       " 'ae__',\n",
       " '_gi',\n",
       " 'arc_',\n",
       " 'ziyi']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name_struct_keys[x] for x in selected_coef]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(591,)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_coef.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
