{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/HD-500GB/Users/nikolausn/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:280: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUID</th>\n",
       "      <th>Last</th>\n",
       "      <th>First</th>\n",
       "      <th>Ethnea</th>\n",
       "      <th>Genni</th>\n",
       "      <th>PubCountry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12872768_1</td>\n",
       "      <td>_Filho_</td>\n",
       "      <td>_Elias_Abdalla_</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>M</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12565879_3</td>\n",
       "      <td>_Bou_Abdallah_</td>\n",
       "      <td>_Jad_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17569133_1</td>\n",
       "      <td>_Abdel_Aziz_</td>\n",
       "      <td>_Ayman_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11213157_2</td>\n",
       "      <td>_Abdelmoula_</td>\n",
       "      <td>_Salma_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>F</td>\n",
       "      <td>Tunisia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11439058_1</td>\n",
       "      <td>_Abdou_</td>\n",
       "      <td>_Ibrahim_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>Egypt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10919608_3</td>\n",
       "      <td>_Abou_El_Fettouh_</td>\n",
       "      <td>_Hazem_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17314740_3</td>\n",
       "      <td>_Aboutaam_</td>\n",
       "      <td>_Rola_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>F</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16564217_2</td>\n",
       "      <td>_Alvim_de_Abreu_Silva_Rodrigues_</td>\n",
       "      <td>_Aida_Alexandra_</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>F</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6339395_2</td>\n",
       "      <td>_Cristina_Affonso_Scaletsky_</td>\n",
       "      <td>_Isabel_</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>F</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7796089_2</td>\n",
       "      <td>_Agbenyega_</td>\n",
       "      <td>_Tsiri_</td>\n",
       "      <td>AFRICAN</td>\n",
       "      <td>-</td>\n",
       "      <td>Ghana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5473023_2</td>\n",
       "      <td>_Maria_Aguado_Garcia_</td>\n",
       "      <td>_Jose_</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>M</td>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15153355_4</td>\n",
       "      <td>_Aguilar_Guisado_</td>\n",
       "      <td>_Manuela_</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>F</td>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11981185_4</td>\n",
       "      <td>_Aguillon_Luna_</td>\n",
       "      <td>_Arturo_</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>M</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12554913_2</td>\n",
       "      <td>_Ahmadzadeh_</td>\n",
       "      <td>_Ali_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>Iran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10066026_5</td>\n",
       "      <td>_Ibrahim_</td>\n",
       "      <td>_Ahmed_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>UnitedArabEmirates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17642770_2</td>\n",
       "      <td>_Letaief_</td>\n",
       "      <td>_Ahmed_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>Tunisia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14766995_2</td>\n",
       "      <td>_Shakeel_Ahmed_</td>\n",
       "      <td>_Mohammed_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16343417_1</td>\n",
       "      <td>_Aksoy_Dogan_</td>\n",
       "      <td>_Alev_</td>\n",
       "      <td>TURKISH</td>\n",
       "      <td>F</td>\n",
       "      <td>Turkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3866040_3</td>\n",
       "      <td>_Nakatsukasa_Akune_</td>\n",
       "      <td>_Mitsumi_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>-</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12663244_3</td>\n",
       "      <td>_Akyuz_</td>\n",
       "      <td>_Filiz_</td>\n",
       "      <td>TURKISH</td>\n",
       "      <td>F</td>\n",
       "      <td>Turkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2830628_1</td>\n",
       "      <td>_Al_Gohary_</td>\n",
       "      <td>_Omaimah_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>F</td>\n",
       "      <td>SaudiArabia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12113661_3</td>\n",
       "      <td>_Al_Jedah_</td>\n",
       "      <td>_Jassim_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>Qatar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3104205_7</td>\n",
       "      <td>_Al_Khatib_</td>\n",
       "      <td>_Shadi_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>-</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7520609_3</td>\n",
       "      <td>_Al_Najjar_</td>\n",
       "      <td>_Azmi_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>M</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>927746_1</td>\n",
       "      <td>_Alba_Conejo_</td>\n",
       "      <td>_Emilio_</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>M</td>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>18451155_4</td>\n",
       "      <td>_Llaguno_</td>\n",
       "      <td>_Sheila_Alcantara_</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>F</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9431676_4</td>\n",
       "      <td>_Alcocer_</td>\n",
       "      <td>_Marcos_</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>M</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3720520_1</td>\n",
       "      <td>_Grazia_Alessandri_</td>\n",
       "      <td>_Maria_</td>\n",
       "      <td>ITALIAN</td>\n",
       "      <td>F</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10480445_2</td>\n",
       "      <td>_Ali_Labib_</td>\n",
       "      <td>_Randa_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>F</td>\n",
       "      <td>Egypt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12636048_1</td>\n",
       "      <td>_Allie_Hamdulay_</td>\n",
       "      <td>_Shameez_</td>\n",
       "      <td>AFRICAN</td>\n",
       "      <td>-</td>\n",
       "      <td>SouthAfrica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44537</th>\n",
       "      <td>12641914_1</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Zhihong_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44538</th>\n",
       "      <td>16892144_4</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Zhen_ying_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44539</th>\n",
       "      <td>15566960_5</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Zhu_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44540</th>\n",
       "      <td>12828461_3</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Zhengjie_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44541</th>\n",
       "      <td>12726928_3</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Zhengchun_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44542</th>\n",
       "      <td>17802886_5</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Ziliang_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44543</th>\n",
       "      <td>18032929_3</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Zhe_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44544</th>\n",
       "      <td>16690641_1</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Zulian_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44545</th>\n",
       "      <td>17554931_2</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Zun_chun_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44546</th>\n",
       "      <td>18975769_5</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Ze_zhou_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44547</th>\n",
       "      <td>19458823_2</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_Zhaochun_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>-</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44548</th>\n",
       "      <td>8734067_3</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_ziying_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>M</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44549</th>\n",
       "      <td>9706530_4</td>\n",
       "      <td>_Liu_</td>\n",
       "      <td>_zhenyu_</td>\n",
       "      <td>CHINESE</td>\n",
       "      <td>M</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44550</th>\n",
       "      <td>6068528_4</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_kenzo_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44551</th>\n",
       "      <td>180046_3</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_kimiko_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>F</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44552</th>\n",
       "      <td>1942669_3</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_kazuhiro_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44553</th>\n",
       "      <td>9571188_3</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_koji_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44554</th>\n",
       "      <td>11071249_4</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_Kousei_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44555</th>\n",
       "      <td>12828613_4</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_Kyoko_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>F</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44556</th>\n",
       "      <td>19073144_1</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_Kenji_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44557</th>\n",
       "      <td>8449402_2</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_keiji_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44558</th>\n",
       "      <td>10671491_5</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_keiji_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44559</th>\n",
       "      <td>18283803_5</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_Kei_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>-</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44560</th>\n",
       "      <td>8913105_4</td>\n",
       "      <td>_Tanaka_</td>\n",
       "      <td>_kiyoshi_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44561</th>\n",
       "      <td>6678271_2</td>\n",
       "      <td>_Sasaki_</td>\n",
       "      <td>_takashi_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44562</th>\n",
       "      <td>3097163_3</td>\n",
       "      <td>_Sasaki_</td>\n",
       "      <td>_toyoshi_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44563</th>\n",
       "      <td>3800425_1</td>\n",
       "      <td>_Sasaki_</td>\n",
       "      <td>_tetsuo_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>M</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44564</th>\n",
       "      <td>9714755_2</td>\n",
       "      <td>_Sasaki_</td>\n",
       "      <td>_takayo_</td>\n",
       "      <td>JAPANESE</td>\n",
       "      <td>F</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44565</th>\n",
       "      <td>7880090_2</td>\n",
       "      <td>_Haddad_</td>\n",
       "      <td>_may_</td>\n",
       "      <td>ARAB</td>\n",
       "      <td>-</td>\n",
       "      <td>SaudiArabia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44566</th>\n",
       "      <td>16865091_1</td>\n",
       "      <td>_Haddad_</td>\n",
       "      <td>_Marianne_</td>\n",
       "      <td>FRENCH</td>\n",
       "      <td>F</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44567 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             AUID                              Last               First  \\\n",
       "0      12872768_1                           _Filho_     _Elias_Abdalla_   \n",
       "1      12565879_3                    _Bou_Abdallah_               _Jad_   \n",
       "2      17569133_1                      _Abdel_Aziz_             _Ayman_   \n",
       "3      11213157_2                      _Abdelmoula_             _Salma_   \n",
       "4      11439058_1                           _Abdou_           _Ibrahim_   \n",
       "5      10919608_3                 _Abou_El_Fettouh_             _Hazem_   \n",
       "6      17314740_3                        _Aboutaam_              _Rola_   \n",
       "7      16564217_2  _Alvim_de_Abreu_Silva_Rodrigues_    _Aida_Alexandra_   \n",
       "8       6339395_2      _Cristina_Affonso_Scaletsky_            _Isabel_   \n",
       "9       7796089_2                       _Agbenyega_             _Tsiri_   \n",
       "10      5473023_2             _Maria_Aguado_Garcia_              _Jose_   \n",
       "11     15153355_4                 _Aguilar_Guisado_           _Manuela_   \n",
       "12     11981185_4                   _Aguillon_Luna_            _Arturo_   \n",
       "13     12554913_2                      _Ahmadzadeh_               _Ali_   \n",
       "14     10066026_5                         _Ibrahim_             _Ahmed_   \n",
       "15     17642770_2                         _Letaief_             _Ahmed_   \n",
       "16     14766995_2                   _Shakeel_Ahmed_          _Mohammed_   \n",
       "17     16343417_1                     _Aksoy_Dogan_              _Alev_   \n",
       "18      3866040_3               _Nakatsukasa_Akune_           _Mitsumi_   \n",
       "19     12663244_3                           _Akyuz_             _Filiz_   \n",
       "20      2830628_1                       _Al_Gohary_           _Omaimah_   \n",
       "21     12113661_3                        _Al_Jedah_            _Jassim_   \n",
       "22      3104205_7                       _Al_Khatib_             _Shadi_   \n",
       "23      7520609_3                       _Al_Najjar_              _Azmi_   \n",
       "24       927746_1                     _Alba_Conejo_            _Emilio_   \n",
       "25     18451155_4                         _Llaguno_  _Sheila_Alcantara_   \n",
       "26      9431676_4                         _Alcocer_            _Marcos_   \n",
       "27      3720520_1               _Grazia_Alessandri_             _Maria_   \n",
       "28     10480445_2                       _Ali_Labib_             _Randa_   \n",
       "29     12636048_1                  _Allie_Hamdulay_           _Shameez_   \n",
       "...           ...                               ...                 ...   \n",
       "44537  12641914_1                             _Liu_           _Zhihong_   \n",
       "44538  16892144_4                             _Liu_         _Zhen_ying_   \n",
       "44539  15566960_5                             _Liu_               _Zhu_   \n",
       "44540  12828461_3                             _Liu_          _Zhengjie_   \n",
       "44541  12726928_3                             _Liu_         _Zhengchun_   \n",
       "44542  17802886_5                             _Liu_           _Ziliang_   \n",
       "44543  18032929_3                             _Liu_               _Zhe_   \n",
       "44544  16690641_1                             _Liu_            _Zulian_   \n",
       "44545  17554931_2                             _Liu_          _Zun_chun_   \n",
       "44546  18975769_5                             _Liu_           _Ze_zhou_   \n",
       "44547  19458823_2                             _Liu_          _Zhaochun_   \n",
       "44548   8734067_3                             _Liu_            _ziying_   \n",
       "44549   9706530_4                             _Liu_            _zhenyu_   \n",
       "44550   6068528_4                          _Tanaka_             _kenzo_   \n",
       "44551    180046_3                          _Tanaka_            _kimiko_   \n",
       "44552   1942669_3                          _Tanaka_          _kazuhiro_   \n",
       "44553   9571188_3                          _Tanaka_              _koji_   \n",
       "44554  11071249_4                          _Tanaka_            _Kousei_   \n",
       "44555  12828613_4                          _Tanaka_             _Kyoko_   \n",
       "44556  19073144_1                          _Tanaka_             _Kenji_   \n",
       "44557   8449402_2                          _Tanaka_             _keiji_   \n",
       "44558  10671491_5                          _Tanaka_             _keiji_   \n",
       "44559  18283803_5                          _Tanaka_               _Kei_   \n",
       "44560   8913105_4                          _Tanaka_           _kiyoshi_   \n",
       "44561   6678271_2                          _Sasaki_           _takashi_   \n",
       "44562   3097163_3                          _Sasaki_           _toyoshi_   \n",
       "44563   3800425_1                          _Sasaki_            _tetsuo_   \n",
       "44564   9714755_2                          _Sasaki_            _takayo_   \n",
       "44565   7880090_2                          _Haddad_               _may_   \n",
       "44566  16865091_1                          _Haddad_          _Marianne_   \n",
       "\n",
       "         Ethnea Genni          PubCountry  \n",
       "0      HISPANIC     M              Brazil  \n",
       "1          ARAB     M              France  \n",
       "2          ARAB     M                 USA  \n",
       "3          ARAB     F             Tunisia  \n",
       "4          ARAB     M               Egypt  \n",
       "5          ARAB     M                 USA  \n",
       "6          ARAB     F              France  \n",
       "7      HISPANIC     F              Brazil  \n",
       "8      HISPANIC     F              Brazil  \n",
       "9       AFRICAN     -               Ghana  \n",
       "10     HISPANIC     M               Spain  \n",
       "11     HISPANIC     F               Spain  \n",
       "12     HISPANIC     M                 USA  \n",
       "13         ARAB     M                Iran  \n",
       "14         ARAB     M  UnitedArabEmirates  \n",
       "15         ARAB     M             Tunisia  \n",
       "16         ARAB     M                 USA  \n",
       "17      TURKISH     F              Turkey  \n",
       "18     JAPANESE     -               Japan  \n",
       "19      TURKISH     F              Turkey  \n",
       "20         ARAB     F         SaudiArabia  \n",
       "21         ARAB     M               Qatar  \n",
       "22         ARAB     -               India  \n",
       "23         ARAB     M              France  \n",
       "24     HISPANIC     M               Spain  \n",
       "25     HISPANIC     F                 USA  \n",
       "26     HISPANIC     M                  UK  \n",
       "27      ITALIAN     F               Italy  \n",
       "28         ARAB     F               Egypt  \n",
       "29      AFRICAN     -         SouthAfrica  \n",
       "...         ...   ...                 ...  \n",
       "44537   CHINESE     -               China  \n",
       "44538   CHINESE     -               China  \n",
       "44539   CHINESE     -               China  \n",
       "44540   CHINESE     -                 USA  \n",
       "44541   CHINESE     -               China  \n",
       "44542   CHINESE     -               China  \n",
       "44543   CHINESE     -               China  \n",
       "44544   CHINESE     -                  UK  \n",
       "44545   CHINESE     -               China  \n",
       "44546   CHINESE     -               China  \n",
       "44547   CHINESE     -               China  \n",
       "44548   CHINESE     M                 USA  \n",
       "44549   CHINESE     M               China  \n",
       "44550  JAPANESE     M               Japan  \n",
       "44551  JAPANESE     F                 USA  \n",
       "44552  JAPANESE     M               Japan  \n",
       "44553  JAPANESE     M               Japan  \n",
       "44554  JAPANESE     M               Japan  \n",
       "44555  JAPANESE     F               Japan  \n",
       "44556  JAPANESE     M               Japan  \n",
       "44557  JAPANESE     M               Japan  \n",
       "44558  JAPANESE     M               Japan  \n",
       "44559  JAPANESE     -               Japan  \n",
       "44560  JAPANESE     M               Japan  \n",
       "44561  JAPANESE     M               Japan  \n",
       "44562  JAPANESE     M               Japan  \n",
       "44563  JAPANESE     M               Japan  \n",
       "44564  JAPANESE     F               Japan  \n",
       "44565      ARAB     -         SaudiArabia  \n",
       "44566    FRENCH     F              France  \n",
       "\n",
       "[44567 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethnea_df = pd.read_csv('names_ethnea_genni_country_sample.csv')\n",
    "ethnea_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start doing DNN for the name embedding\n",
    "# combine first name and last name\n",
    "full_name = ethnea_df['First'].str.lower()+ethnea_df['Last'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_structure(word,n_char=4):\n",
    "    x_struct = []\n",
    "    word_len = len(word) + n_char\n",
    "    n_char-=1\n",
    "    counter = 0\n",
    "    for i in range(word_len):\n",
    "        end = i+1\n",
    "        start = (i - n_char) if (i - n_char) > 0 else 0\n",
    "        if word[start:end]!='_':\n",
    "            x_struct.append(word[start:end])\n",
    "    return x_struct\n",
    "\n",
    "#extract_structure('_Elias_Abdalla__Filho_')\n",
    "full_name_struct = full_name.apply(lambda x: extract_structure(x))\n",
    "\n",
    "struct_dict = {}\n",
    "for name_struct_i in full_name_struct:\n",
    "    for struct_j in name_struct_i:\n",
    "        if struct_j not in struct_dict:\n",
    "            struct_dict[struct_j]=0\n",
    "        struct_dict[struct_j]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48972"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(struct_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_dict_keys = list(struct_dict.keys())\n",
    "ethnic_series = ethnea_df['Ethnea'].str.lower()\n",
    "ethnic_keys = list(np.unique(ethnic_series.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedd the structure vocabulary using text embedding and reduce the dimensionality\n",
    "\n",
    "# convert the names into word structure vector\n",
    "struct_dict_keys = list(struct_dict.keys())\n",
    "\n",
    "def transform_structure(name_struct):\n",
    "    list_structure = []\n",
    "    for x in name_struct:\n",
    "        list_structure.append(struct_dict_keys.index(x)+1)\n",
    "    #add pading 0 for structure less than num_input\n",
    "    #for i in range(len(list_structure),timesteps):\n",
    "    #    list_structure.append(0)\n",
    "    return list_structure   \n",
    "    #return [*map(lambda x:struct_dict_keys.index(x)+1, name_struct)]\n",
    "\n",
    "data_source = full_name_struct.apply(lambda x: transform_structure(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels(x):\n",
    "    y = np.zeros(len(ethnic_keys))\n",
    "    y[ethnic_keys.index(x)]=1\n",
    "    return y\n",
    "\n",
    "labels = np.array(list(map(lambda x: transform_labels(x),ethnic_series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tflearn make the graph creation simple\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# separate train and training set\n",
    "trainX, testX, trainY, testY = train_test_split(data_source,[ethnic_keys.index(x) for x in ethnic_series],test_size = 0.2)\n",
    "\n",
    "trainX = pad_sequences(trainX, maxlen=50,value=0.)\n",
    "testX = pad_sequences(testX,maxlen=50,value=0.)\n",
    "# Converting labels to binary vectors\n",
    "trainY = to_categorical(trainY,nb_classes=len(ethnic_keys))\n",
    "testY = to_categorical(testY,nb_classes=len(ethnic_keys))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train_test_fix.pickle','wb') as f:\n",
    "    pickle.dump((trainX,trainY,testX,testY,ethnic_keys,struct_dict_keys),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('train_test_fix.pickle', 'rb') as f:\n",
    "    trainX,trainY,testX,testY,ethnic_keys,struct_dict_keys = pickle.load(f)\n",
    "\n",
    "#with open('traintest-smote.pickle','rb') as f:\n",
    "#    train_res,test_res = pickle.load(f)\n",
    "\n",
    "with open('ethnic_keys.pickle','rb') as f:\n",
    "    name_struct_keys,ethnic_keys = pickle.load(f)\n",
    "        \n",
    "embedding_vector_length = 1000\n",
    "lstm_layer = 1000\n",
    "max_sequence = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert categorical to binary crossentropy\n",
    "#trainY = np.array([np.where(x>0)[0][0] for x in trainY])\n",
    "#testY = np.array([np.where(x>0)[0][0] for x in testY])\n",
    "\n",
    "#test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 1000)          62696000  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 50, 1000)          3001000   \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 25, 1000)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               880800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 23)                4623      \n",
      "=================================================================\n",
      "Total params: 66,582,423\n",
      "Trainable params: 66,582,423\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 662s - loss: 1.6547 - acc: 0.5389    \n",
      "Accuracy: 76.87%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 661s - loss: 0.4786 - acc: 0.8715    \n",
      "Accuracy: 85.26%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 650s - loss: 0.1770 - acc: 0.9562    \n",
      "Accuracy: 85.60%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 649s - loss: 0.0735 - acc: 0.9841    \n",
      "Accuracy: 86.13%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 648s - loss: 0.0327 - acc: 0.9942    \n",
      "Accuracy: 86.01%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 659s - loss: 0.0165 - acc: 0.9973    \n",
      "Accuracy: 86.20%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 635s - loss: 0.0096 - acc: 0.9989    \n",
      "Accuracy: 86.28%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 648s - loss: 0.0062 - acc: 0.9994    \n",
      "Accuracy: 86.39%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 637s - loss: 0.0043 - acc: 0.9997    \n",
      "Accuracy: 86.35%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 650s - loss: 0.0031 - acc: 0.9999    \n",
      "Accuracy: 86.26%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(name_struct_keys)+1,embedding_vector_length,input_length=max_sequence))\n",
    "model.add(Conv1D(filters=embedding_vector_length,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "#model.add(LSTM(lstm_layer,dropout=0.8))\n",
    "model.add(Bidirectional(LSTM(max_sequence*2,return_sequences=False),input_shape=(max_sequence,1)))\n",
    "#model.add(TimeDistributed(keras.layers.Dense(len(ethnic_keys),activation='softmax')))\n",
    "model.add(keras.layers.Dense(len(ethnic_keys),activation='softmax'))\n",
    "#model.add(keras.layers.Dense(len(ethnic_keys),activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "for x in range(10):\n",
    "    model.fit(trainX,trainY,epochs=1,batch_size=1000)\n",
    "    scores = model.evaluate(testX,testY,verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" %(scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  112,  1331,  1332, ...,     0,     0,     0],\n",
       "       [  251,   252,   232, ...,     0,     0,     0],\n",
       "       [ 4633, 11731, 23123, ...,     0,     0,     0],\n",
       "       ..., \n",
       "       [  696,   697,   698, ...,     0,     0,     0],\n",
       "       [   25,    26,  5195, ...,     0,     0,     0],\n",
       "       [ 8671,  8672,  6568, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save this model atlhouth there is something wrong with the features\n",
    "# I did not lowercase the text :(\n",
    "model_json = model.to_json()\n",
    "with open('model-keras-embed-bilstm-fixlower.json','w') as f:\n",
    "    f.write(model_json)\n",
    "#save the last weight\n",
    "model.save_weights('model-keras-embed-bilstm-fixlower-10.h5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
