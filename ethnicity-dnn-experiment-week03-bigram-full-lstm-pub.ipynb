{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ethnea_df = pd.read_csv('names_ethnea_genni_country.csv')\n",
    "#ethnea_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First thing first, use the character feature for making the dnn model\n",
    "def extract_structure(word,n_char=2):\n",
    "    x_struct = []\n",
    "    word_len = len(word) + n_char\n",
    "    n_char-=1\n",
    "    counter = 0\n",
    "    for i in range(word_len):\n",
    "        end = i+1\n",
    "        start = (i - n_char) if (i - n_char) > 0 else 0\n",
    "        if word[start:end]!='_' and word[start:end]!='':\n",
    "        #if word[start:end]!='_':\n",
    "            x_struct.append(word[start:end])\n",
    "    return x_struct\n",
    "\n",
    "first_name_struct = ethnea_df.First.apply(lambda x: extract_structure(x.lower(),2))\n",
    "last_name_struct = ethnea_df.Last.apply(lambda x: extract_structure(x.lower(),2))                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make struct dictionary\n",
    "struct_dict = {}\n",
    "for name_struct_i in first_name_struct:\n",
    "    for struct_j in name_struct_i:\n",
    "        if struct_j not in struct_dict:\n",
    "            struct_dict[struct_j]=0\n",
    "        struct_dict[struct_j]+=1\n",
    "for name_struct_i in last_name_struct:\n",
    "    for struct_j in name_struct_i:\n",
    "        if struct_j not in struct_dict:\n",
    "            struct_dict[struct_j]=0\n",
    "        struct_dict[struct_j]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "struct_dict_keys = list(struct_dict.keys())\n",
    "ethnic_series = ethnea_df['Ethnea'].str.lower()\n",
    "pub_series = ethnea_df.PubCountry.str.lower()\n",
    "ethnic_keys = list(np.unique(ethnic_series.values))\n",
    "pub_keys = list(np.unique(pub_series.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pub_keys)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# separate train and training set\n",
    "trainIndex, testIndex, trainY, testY = train_test_split(range(len(first_name_struct)),[ethnic_keys.index(x) for x in ethnic_series],test_size = 0.2)\n",
    "with open('train_test_full_index.pickle','wb') as f:\n",
    "    pickle.dump((trainIndex,testIndex, trainY, testY),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load test train data\n",
    "with open('train_test_full_index.pickle', 'rb') as f:\n",
    "    trainIndex,testIndex,trainY,testY = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform the dataset into structure\n",
    "def transform_structure(name_struct):\n",
    "    list_structure = []\n",
    "    for x in name_struct:\n",
    "        try:\n",
    "            list_structure.append(struct_dict_keys.index(x)+1)\n",
    "        except BaseException:\n",
    "            list_structure.append(0)\n",
    "    #add pading 0 for structure less than num_input\n",
    "    #for i in range(len(list_structure),timesteps):\n",
    "    #    list_structure.append(0)\n",
    "    return list_structure   \n",
    "    #return [*map(lambda x:struct_dict_keys.index(x)+1, name_struct)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# make data source creation more efficient\n",
    "batch_size = 10000\n",
    "#first_name_ds_mat = np.zeros((len(ethnic_series),len(struct_dict_keys)),dtype=np.int32)\n",
    "#last_name_ds_mat = np.zeros((len(ethnic_series),len(struct_dict_keys)),dtype=np.int32)\n",
    "first_name_ds_mat = np.zeros((batch_size,len(struct_dict_keys)),dtype=np.int32)\n",
    "last_name_ds_mat = np.zeros((batch_size,len(struct_dict_keys)),dtype=np.int32)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    x = first_name_struct.iloc[i]\n",
    "    for y in x:\n",
    "        first_name_ds_mat[i,struct_dict_keys.index(y)]+=1\n",
    "    x = last_name_struct.iloc[i]\n",
    "    for y in x:\n",
    "        last_name_ds_mat[i,struct_dict_keys.index(y)]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_structure(name_struct):\n",
    "    list_structure = []\n",
    "    for x in name_struct:\n",
    "        try:\n",
    "            list_structure.append(struct_dict_keys.index(x)+1)\n",
    "        except BaseException:\n",
    "            list_structure.append(0)\n",
    "    #add pading 0 for structure less than num_input\n",
    "    #for i in range(len(list_structure),timesteps):\n",
    "    #    list_structure.append(0)\n",
    "    return list_structure   \n",
    "    #return [*map(lambda x:struct_dict_keys.index(x)+1, name_struct)]\n",
    "\n",
    "#data_source = full_name_struct.apply(lambda x: transform_structure(x))\n",
    "\n",
    "def generate_batch(first_name, last_name, i, batch_size=10000):\n",
    "    len_name = len(first_name)\n",
    "    print(len_name)\n",
    "    start = i*batch_size\n",
    "    print(start)\n",
    "    end = start+batch_size if start+batch_size < len_name else len_name\n",
    "    len_mat = end - start\n",
    "    #first_name_ds_mat = np.zeros((len_mat,50),dtype=np.int32)\n",
    "    #last_name_ds_mat = np.zeros((len_mat,50),dtype=np.int32)\n",
    "    first_name_ds_mat = first_name[start:end]\n",
    "    last_name_ds_mat = last_name[start:end]\n",
    "    first_name_ds_mat = pad_sequences(first_name_ds_mat.apply(lambda x: transform_structure(x)),maxlen=50,value=0.)\n",
    "    first_name_ds_mat = first_name_ds_mat.reshape(first_name_ds_mat.shape[0],1,first_name_ds_mat.shape[1])\n",
    "    last_name_ds_mat = pad_sequences(last_name_ds_mat.apply(lambda x:transform_structure(x)),maxlen=50,value=0.)\n",
    "    last_name_ds_mat = last_name_ds_mat.reshape(last_name_ds_mat.shape[0],1,last_name_ds_mat.shape[1])\n",
    "\n",
    "    \"\"\"\n",
    "    for i in range(len_mat):\n",
    "        x = first_name_struct.iloc[i]\n",
    "        for y in x:\n",
    "            first_name_ds_mat[i,struct_dict_keys.index(y)]+=1\n",
    "        x = last_name_struct.iloc[i]\n",
    "        for y in x:\n",
    "            last_name_ds_mat[i,struct_dict_keys.index(y)]+=1\n",
    "    \"\"\"\n",
    "    return first_name_ds_mat,last_name_ds_mat, range(start,end)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "first_name_ds = first_name_struct.apply(lambda x:transform_structure(x))\n",
    "last_name_ds = last_name_struct.apply(lambda x:transform_structure(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(x,my_keys):\n",
    "    y = np.zeros(len(my_keys))\n",
    "    y[my_keys.index(x)]=1\n",
    "    return y\n",
    "\n",
    "\n",
    "labels = np.array(list(map(lambda x: transform_labels(x,ethnic_keys),ethnic_series)))\n",
    "#pub_country = np.array(list(map(lambda x: transform_labels(x,pub_keys),pub_series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `Input` not found.\n"
     ]
    }
   ],
   "source": [
    "?Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "\"\"\"\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(name_struct_keys)+1,embedding_vector_length,input_length=max_sequence))\n",
    "model.add(Conv1D(filters=embedding_vector_length,kernel_size=3,padding='same',activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=2))\n",
    "#model.add(LSTM(lstm_layer,dropout=0.8))\n",
    "model.add(Bidirectional(LSTM(max_sequence*2,return_sequences=False),input_shape=(max_sequence,1)))\n",
    "#model.add(TimeDistributed(keras.layers.Dense(len(ethnic_keys),activation='softmax')))\n",
    "model.add(keras.layers.Dense(len(ethnic_keys),activation='softmax'))\n",
    "#model.add(keras.layers.Dense(len(ethnic_keys),activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\"\"\"\n",
    "\n",
    "# max sequence length\n",
    "seq_length = 100\n",
    "# multi input with single output\n",
    "\n",
    "# first name input\n",
    "first_name_input = Input(shape=(1,50),name='first_name_input')\n",
    "last_name_input = Input(shape=(1,50),name='last_name_input')\n",
    "\n",
    "# pub input\n",
    "pub_input = Input(shape=(len(pub_keys),),name='pub_input')\n",
    "\n",
    "# first tensor for first name\n",
    "first_name_l = Bidirectional(LSTM(1000,return_sequences=False))(first_name_input)\n",
    "last_name_l = Bidirectional(LSTM(1000,return_sequences=False))(last_name_input)\n",
    "\n",
    "# merge the two layer together\n",
    "merge_first_last = keras.layers.concatenate([first_name_l,last_name_l])\n",
    "\n",
    "# stack dense network for memory\n",
    "full_conn_merge = Dense(1000, activation='relu')(merge_first_last)\n",
    "x = keras.layers.concatenate([full_conn_merge,pub_input])\n",
    "x = Dense(500, activation='relu')(x)\n",
    "output_l = Dense(len(ethnic_keys),activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[first_name_input, last_name_input,pub_input], outputs=[output_l])\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "first_name_input (InputLayer)    (None, 1, 50)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "last_name_input (InputLayer)     (None, 1, 50)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 2000)          8408000     first_name_input[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 2000)          8408000     last_name_input[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 4000)          0           bidirectional_1[0][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1000)          4001000     concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "pub_input (InputLayer)           (None, 275)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 1275)          0           dense_1[0][0]                    \n",
      "                                                                   pub_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 500)           638000      concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 26)            13026       dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 21,468,026\n",
      "Trainable params: 21,468,026\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "first_name_ds_mat = np.zeros((len(first_name_ds),len(struct_dict_keys)),dtype=np.int32)\n",
    "for i,x in enumerate(first_name_ds):\n",
    "    for y in x:\n",
    "        first_name_ds_mat[i,y-1]+=1\n",
    "last_name_ds_mat = np.zeros((len(last_name_ds),len(struct_dict_keys)),dtype=np.int32)\n",
    "for i,x in enumerate(last_name_ds):\n",
    "    for y in x:\n",
    "        last_name_ds_mat[i,y-1]+=1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "first_name_input (InputLayer)    (None, 1, 50)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "last_name_input (InputLayer)     (None, 1, 50)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 2000)          8408000     first_name_input[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 2000)          8408000     last_name_input[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 4000)          0           bidirectional_1[0][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1000)          4001000     concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "pub_input (InputLayer)           (None, 275)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 1275)          0           dense_1[0][0]                    \n",
      "                                                                   pub_input[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 500)           638000      concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 26)            13026       dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 21,468,026\n",
      "Trainable params: 21,468,026\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 45s - loss: 3.1224 - acc: 0.2315     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 2.2213 - acc: 0.3686     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 2.0202 - acc: 0.4331     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 1.9033 - acc: 0.4656     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 1.7903 - acc: 0.4840     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 1.7069 - acc: 0.5122     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 1.6376 - acc: 0.5259     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 1.5492 - acc: 0.5519     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 1.5071 - acc: 0.5635     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 1.4551 - acc: 0.5807     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 1.3799 - acc: 0.6036     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 1.3457 - acc: 0.6159     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 1.2921 - acc: 0.6246     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 1.2550 - acc: 0.6434     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 1.2393 - acc: 0.6425     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 1.2071 - acc: 0.6538     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 1.1875 - acc: 0.6595     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 1.1594 - acc: 0.6689     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 1.1439 - acc: 0.6744     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 1.1157 - acc: 0.6794     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 1.0980 - acc: 0.6874     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 1.0717 - acc: 0.6939     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 1.0669 - acc: 0.6915     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 1.0465 - acc: 0.7016     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 1.0498 - acc: 0.7009     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 1.0067 - acc: 0.7109     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.9924 - acc: 0.7170     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 1.0021 - acc: 0.7140     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.9882 - acc: 0.7190     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 1.0012 - acc: 0.7122     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 1.0010 - acc: 0.7115     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.9741 - acc: 0.7190     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.9870 - acc: 0.7174     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.9373 - acc: 0.7317     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.9317 - acc: 0.7331     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.9291 - acc: 0.7365     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.9311 - acc: 0.7339     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.9206 - acc: 0.7310     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.9261 - acc: 0.7340     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.9272 - acc: 0.7358     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.8998 - acc: 0.7416     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.9009 - acc: 0.7428     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.8926 - acc: 0.7453     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.8894 - acc: 0.7466     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.8759 - acc: 0.7524     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.8923 - acc: 0.7442     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7982 - acc: 0.7719     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.8085 - acc: 0.7670     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7976 - acc: 0.7710     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7865 - acc: 0.7724     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7793 - acc: 0.7772     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7799 - acc: 0.7747     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7681 - acc: 0.7815     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7759 - acc: 0.7773     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7762 - acc: 0.7743     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7745 - acc: 0.7782     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7708 - acc: 0.7784     \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 41s - loss: 0.7723 - acc: 0.7765     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7678 - acc: 0.7769     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7681 - acc: 0.7791     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7512 - acc: 0.7848     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7639 - acc: 0.7778     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7570 - acc: 0.7809     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7708 - acc: 0.7730     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7668 - acc: 0.7761     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7754 - acc: 0.7750     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7734 - acc: 0.7743     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7454 - acc: 0.7840     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7535 - acc: 0.7794     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7574 - acc: 0.7833     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7448 - acc: 0.7811     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7457 - acc: 0.7834     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7484 - acc: 0.7811     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7298 - acc: 0.7894     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7407 - acc: 0.7844     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7214 - acc: 0.7916     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7268 - acc: 0.7900     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7423 - acc: 0.7855     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7357 - acc: 0.7874     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7225 - acc: 0.7889     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7290 - acc: 0.7916     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7203 - acc: 0.7916     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7375 - acc: 0.7859     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7392 - acc: 0.7882     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7443 - acc: 0.7843     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7373 - acc: 0.7848     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7196 - acc: 0.7934     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7029 - acc: 0.7970     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7084 - acc: 0.7933     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7097 - acc: 0.7938     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7070 - acc: 0.7952     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7172 - acc: 0.7911     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7083 - acc: 0.7923     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7123 - acc: 0.7952     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7044 - acc: 0.7976     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7128 - acc: 0.7928     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7087 - acc: 0.7972     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.7053 - acc: 0.7955     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6973 - acc: 0.7966     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.6958 - acc: 0.8027     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6884 - acc: 0.8027     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6945 - acc: 0.7970     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6871 - acc: 0.8019     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6907 - acc: 0.7980     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6958 - acc: 0.7981     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6869 - acc: 0.8005     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6954 - acc: 0.7988     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6937 - acc: 0.7993     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6850 - acc: 0.8002     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6807 - acc: 0.8043     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.6789 - acc: 0.8011     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6880 - acc: 0.8017     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6744 - acc: 0.8001     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.7061 - acc: 0.7930     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.6760 - acc: 0.8042     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6967 - acc: 0.7990     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.6791 - acc: 0.8028     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.6741 - acc: 0.8053     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6662 - acc: 0.8050     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6733 - acc: 0.8025     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.6726 - acc: 0.8042     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6897 - acc: 0.7967     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6921 - acc: 0.7998     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.6682 - acc: 0.8056     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6705 - acc: 0.8044     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6658 - acc: 0.8071     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6701 - acc: 0.8061     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.6606 - acc: 0.8079     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.6717 - acc: 0.8023     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6734 - acc: 0.8029     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6617 - acc: 0.8082     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6508 - acc: 0.8127     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.6660 - acc: 0.8088     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6592 - acc: 0.8098     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6648 - acc: 0.8073     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6625 - acc: 0.8054     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6693 - acc: 0.8058     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6678 - acc: 0.8052     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6619 - acc: 0.8071     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.6470 - acc: 0.8134     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6557 - acc: 0.8122     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6572 - acc: 0.8109     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.6592 - acc: 0.8074     \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 41s - loss: 0.6490 - acc: 0.8124     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.6463 - acc: 0.8117     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6535 - acc: 0.8108     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.6348 - acc: 0.8160     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.6517 - acc: 0.8105     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.6406 - acc: 0.8106     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 41s - loss: 0.6489 - acc: 0.8110     \n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 42s - loss: 0.6393 - acc: 0.8162     \n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "first_trainX = first_name_struct[trainIndex]\n",
    "first_testX = first_name_struct[testIndex]\n",
    "last_trainX = last_name_struct[trainIndex]\n",
    "last_testX = last_name_struct[testIndex]\n",
    "trainY = labels[trainIndex]\n",
    "testY = labels[testIndex]\n",
    "\n",
    "#trainX =np.array([to_categorical(x,nb_classes=len(struct_dict_keys)+1) for x in trainX])\n",
    "#testX =np.array([to_categorical(x,nb_classes=len(struct_dict_keys)+1) for x in testX])\n",
    "\n",
    "mini_batch_size = 20000\n",
    "len_mini_batch = round(len(trainY)/mini_batch_size)\n",
    "batch_size = 5000\n",
    "\n",
    "for x in range(1):\n",
    "    for y in range(len_mini_batch):\n",
    "        y_first_trainX, y_last_trainX, batch_range =  generate_batch(first_trainX,last_trainX,y,mini_batch_size)\n",
    "        pub_country = np.array(list(map(lambda x: transform_labels(x,pub_keys),pub_series[trainIndex].iloc[batch_range])))\n",
    "        model.fit([y_first_trainX, y_last_trainX, pub_country],trainY[batch_range],epochs=1,batch_size=batch_size)\n",
    "        #scores = model.evaluate([first_testX, last_testX],testY,verbose=0)\n",
    "        #print(\"Accuracy: %.2f%%\" %(scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(3520000, 3540000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_range\n",
    "#pub_series[trainIndex][list(batch_range)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(firstX, lastX, testY, testIndex, mini_batch_size=20000):\n",
    "    test_len = len(testY)\n",
    "    len_mini_batch = round(test_len/mini_batch_size)\n",
    "    scores = np.zeros(len_mini_batch)\n",
    "    for i in range(len_mini_batch):\n",
    "        y_first_trainX,y_last_trainX,batch_range = generate_batch(firstX,lastX,i,mini_batch_size)\n",
    "        pub_country = np.array(list(map(lambda x: transform_labels(x,pub_keys), pub_series[testIndex].iloc[batch_range])))\n",
    "        scores[i] = model.evaluate([y_first_trainX,y_last_trainX,pub_country],testY[batch_range])[1]\n",
    "        print(scores[i])\n",
    "    return np.average(scores)\n",
    "    #scores = model.evaluate([first_testX, last_testX],testY,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "886817\n",
      "0\n",
      "20000/20000 [==============================] - 30s    \n",
      "0.81265\n",
      "886817\n",
      "20000\n",
      "20000/20000 [==============================] - 30s    \n",
      "0.81075\n",
      "886817\n",
      "40000\n",
      " 3072/20000 [===>..........................] - ETA: 25s"
     ]
    }
   ],
   "source": [
    "evaluate(first_testX,last_testX, testY, testIndex)\n",
    "#test\n",
    "#y_first_trainX,y_last_trainX,batch_range = generate_batch(first_testX,last_testX,1,mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'china'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pub_series[testIndex].iloc[batch_range]\n",
    "pub_series[testIndex].iloc[20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3415 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3498 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3490 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3557 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3430 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3428 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3461 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3501 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3491 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3422 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3318 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3446 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3456 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3455 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3472 - acc: 0.3236    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3430 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3367 - acc: 0.3294    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3515 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3367 - acc: 0.3292    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 15s - loss: 2.3417 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3449 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3443 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3426 - acc: 0.3251    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3411 - acc: 0.3276    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3455 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3505 - acc: 0.3220    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3414 - acc: 0.3269    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3437 - acc: 0.3252    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3439 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3535 - acc: 0.3212    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3427 - acc: 0.3271    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3431 - acc: 0.3284    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3463 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3493 - acc: 0.3242    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3397 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3363 - acc: 0.3241    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3406 - acc: 0.3279    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3457 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3443 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3426 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3412 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3468 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3409 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3474 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3476 - acc: 0.3222    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3478 - acc: 0.3227    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3488 - acc: 0.3230    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3471 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3446 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3408 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3407 - acc: 0.3286    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3415 - acc: 0.3287    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3397 - acc: 0.3280    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3402 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3401 - acc: 0.3295    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3486 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3482 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3439 - acc: 0.3254    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3497 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3451 - acc: 0.3245    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3438 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3430 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3430 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3338 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3470 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3507 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3464 - acc: 0.3239    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3403 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3474 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3374 - acc: 0.3297    \n",
      "Epoch 1/1\n",
      "47268/47268 [==============================] - 13s - loss: 2.3398 - acc: 0.3272    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3411 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3497 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3487 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3555 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3427 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3426 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3461 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3500 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3490 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3422 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3317 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3446 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3456 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3453 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3472 - acc: 0.3236    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3430 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3367 - acc: 0.3294    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3515 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3365 - acc: 0.3292    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3416 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3448 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3441 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3426 - acc: 0.3251    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3410 - acc: 0.3276    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3455 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3504 - acc: 0.3220    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3414 - acc: 0.3269    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3437 - acc: 0.3252    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3439 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3535 - acc: 0.3212    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3426 - acc: 0.3271    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3431 - acc: 0.3284    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3461 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3493 - acc: 0.3242    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3397 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3363 - acc: 0.3241    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3406 - acc: 0.3279    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3456 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3442 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3425 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3412 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3468 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3410 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3474 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3475 - acc: 0.3222    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3478 - acc: 0.3227    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3489 - acc: 0.3230    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3472 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3445 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3408 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3406 - acc: 0.3286    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3414 - acc: 0.3287    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3396 - acc: 0.3280    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3402 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3400 - acc: 0.3295    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3486 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3482 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3438 - acc: 0.3254    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3496 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3450 - acc: 0.3245    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3436 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 15s - loss: 2.3430 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3429 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3339 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3470 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3507 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3464 - acc: 0.3239    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3403 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3473 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3372 - acc: 0.3297    \n",
      "Epoch 1/1\n",
      "47268/47268 [==============================] - 13s - loss: 2.3399 - acc: 0.3272    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 15s - loss: 2.3412 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3497 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3486 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3556 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3426 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3427 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3459 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3500 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3489 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3420 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3316 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3445 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3455 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3452 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3471 - acc: 0.3236    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3429 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3366 - acc: 0.3294    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3514 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3365 - acc: 0.3292    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3416 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3448 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3441 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3425 - acc: 0.3251    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3410 - acc: 0.3276    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3455 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3504 - acc: 0.3220    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3415 - acc: 0.3269    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 15s - loss: 2.3436 - acc: 0.3252    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3437 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 62s - loss: 2.3534 - acc: 0.3212    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 59s - loss: 2.3427 - acc: 0.3271    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 14s - loss: 2.3430 - acc: 0.3284    \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 14s - loss: 2.3462 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 32s - loss: 2.3493 - acc: 0.3242    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3398 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3365 - acc: 0.3241    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 45s - loss: 2.3405 - acc: 0.3279    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 48s - loss: 2.3457 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 70s - loss: 2.3442 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3426 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3412 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 51s - loss: 2.3469 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 65s - loss: 2.3407 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3475 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3475 - acc: 0.3222    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 56s - loss: 2.3476 - acc: 0.3227    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3488 - acc: 0.3230    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3471 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3445 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 55s - loss: 2.3406 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3406 - acc: 0.3286    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3416 - acc: 0.3287    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3394 - acc: 0.3280    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 58s - loss: 2.3401 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3399 - acc: 0.3295    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3486 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3482 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 56s - loss: 2.3438 - acc: 0.3254    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 65s - loss: 2.3497 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3449 - acc: 0.3245    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3437 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 58s - loss: 2.3430 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 68s - loss: 2.3431 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3336 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3471 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 54s - loss: 2.3507 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3464 - acc: 0.3239    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3404 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3474 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 56s - loss: 2.3372 - acc: 0.3297    \n",
      "Epoch 1/1\n",
      "47268/47268 [==============================] - 65s - loss: 2.3399 - acc: 0.3272    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3410 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3498 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 56s - loss: 2.3487 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3555 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3425 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3428 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 59s - loss: 2.3461 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3501 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3489 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3420 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 56s - loss: 2.3310 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3445 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3454 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3453 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 58s - loss: 2.3471 - acc: 0.3236    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3429 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3367 - acc: 0.3294    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3514 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 56s - loss: 2.3365 - acc: 0.3292    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3415 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3447 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3444 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 58s - loss: 2.3426 - acc: 0.3251    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3410 - acc: 0.3276    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3455 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3504 - acc: 0.3220    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 55s - loss: 2.3414 - acc: 0.3269    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3438 - acc: 0.3252    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3439 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3535 - acc: 0.3212    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 59s - loss: 2.3427 - acc: 0.3271    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3428 - acc: 0.3285    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3462 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3492 - acc: 0.3242    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 55s - loss: 2.3396 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 64s - loss: 2.3362 - acc: 0.3241    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3405 - acc: 0.3278    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3455 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 59s - loss: 2.3440 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 64s - loss: 2.3425 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3413 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3467 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 57s - loss: 2.3408 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3473 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3474 - acc: 0.3222    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3477 - acc: 0.3227    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 58s - loss: 2.3487 - acc: 0.3230    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3470 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3444 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3407 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 59s - loss: 2.3407 - acc: 0.3286    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 62s - loss: 2.3416 - acc: 0.3287    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3397 - acc: 0.3280    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3403 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 60s - loss: 2.3396 - acc: 0.3295    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 62s - loss: 2.3487 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3483 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3437 - acc: 0.3254    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 60s - loss: 2.3497 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 61s - loss: 2.3451 - acc: 0.3245    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3438 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3430 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 60s - loss: 2.3432 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 62s - loss: 2.3337 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3470 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3506 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 60s - loss: 2.3463 - acc: 0.3239    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 62s - loss: 2.3404 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3474 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3372 - acc: 0.3297    \n",
      "Epoch 1/1\n",
      "47268/47268 [==============================] - 63s - loss: 2.3398 - acc: 0.3272    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3408 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3496 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3486 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 55s - loss: 2.3555 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 63s - loss: 2.3426 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3426 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3459 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3501 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3489 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3420 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3316 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 45s - loss: 2.3444 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3454 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3454 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3468 - acc: 0.3236    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3428 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3364 - acc: 0.3294    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3510 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 69s - loss: 2.3364 - acc: 0.3292    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3412 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3448 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3441 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3425 - acc: 0.3251    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3409 - acc: 0.3276    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3453 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3503 - acc: 0.3220    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3414 - acc: 0.3269    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 30s - loss: 2.3436 - acc: 0.3252    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3436 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3534 - acc: 0.3212    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3424 - acc: 0.3271    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3427 - acc: 0.3285    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3462 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3491 - acc: 0.3242    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3395 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3364 - acc: 0.3241    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3405 - acc: 0.3278    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3456 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3440 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3422 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3410 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3466 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3407 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3476 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3473 - acc: 0.3222    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3478 - acc: 0.3227    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3486 - acc: 0.3230    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3471 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3443 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3407 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3405 - acc: 0.3286    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3416 - acc: 0.3287    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3394 - acc: 0.3280    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3402 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 51s - loss: 2.3399 - acc: 0.3295    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 65s - loss: 2.3487 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3481 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3435 - acc: 0.3254    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3498 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3449 - acc: 0.3245    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3438 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3428 - acc: 0.3264    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3432 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3336 - acc: 0.3282    \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 66s - loss: 2.3469 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3504 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 36s - loss: 2.3459 - acc: 0.3239    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3405 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3475 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3372 - acc: 0.3297    \n",
      "Epoch 1/1\n",
      "47268/47268 [==============================] - 63s - loss: 2.3396 - acc: 0.3272    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 65s - loss: 2.3407 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 65s - loss: 2.3495 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3485 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3552 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3426 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3424 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3462 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3500 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3491 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3421 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3315 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3445 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3451 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3453 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3468 - acc: 0.3236    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3429 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3364 - acc: 0.3294    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3510 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3364 - acc: 0.3292    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 48s - loss: 2.3413 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3449 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3439 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3427 - acc: 0.3251    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3409 - acc: 0.3276    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3453 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3500 - acc: 0.3220    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3412 - acc: 0.3269    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3435 - acc: 0.3252    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3437 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3532 - acc: 0.3212    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3422 - acc: 0.3271    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3428 - acc: 0.3284    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3461 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3493 - acc: 0.3242    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 50s - loss: 2.3396 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 65s - loss: 2.3363 - acc: 0.3241    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3405 - acc: 0.3279    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3456 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 67s - loss: 2.3440 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3423 - acc: 0.3247    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3411 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3465 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 69s - loss: 2.3408 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3471 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3474 - acc: 0.3222    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3480 - acc: 0.3227    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 36s - loss: 2.3487 - acc: 0.3230    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3469 - acc: 0.3257    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3444 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3410 - acc: 0.3263    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3405 - acc: 0.3286    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3414 - acc: 0.3287    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3394 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3401 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3397 - acc: 0.3295    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3485 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3480 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3437 - acc: 0.3253    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3496 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3448 - acc: 0.3245    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3435 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3430 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3431 - acc: 0.3270    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3335 - acc: 0.3282    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3469 - acc: 0.3219    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3504 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3460 - acc: 0.3239    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3401 - acc: 0.3265    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3474 - acc: 0.3233    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3373 - acc: 0.3297    \n",
      "Epoch 1/1\n",
      "47268/47268 [==============================] - 55s - loss: 2.3395 - acc: 0.3272    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3407 - acc: 0.3281    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3494 - acc: 0.3248    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3488 - acc: 0.3256    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3555 - acc: 0.3225    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3424 - acc: 0.3260    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3425 - acc: 0.3277    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 59s - loss: 2.3459 - acc: 0.3250    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 62s - loss: 2.3497 - acc: 0.3234    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3491 - acc: 0.3231    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3422 - acc: 0.3267    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3318 - acc: 0.3275    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3443 - acc: 0.3249    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3456 - acc: 0.3266    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3452 - acc: 0.3243    \n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 66s - loss: 2.3470 - acc: 0.3236    \n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-9dcd0069fede>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_mini_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0my_first_trainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_last_trainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_range\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_trainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlast_trainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_first_trainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_last_trainX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m#scores = model.evaluate([first_testX, last_testX],testY,verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#print(\"Accuracy: %.2f%%\" %(scores[1]*100))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nnikolaus/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mini_batch_size = 50000\n",
    "len_mini_batch = round(len(trainY)/mini_batch_size)\n",
    "batch_size = 5000\n",
    "\n",
    "for x in range(10):\n",
    "    for y in range(len_mini_batch):\n",
    "        y_first_trainX, y_last_trainX, batch_range =  generate_batch(first_trainX,last_trainX,y,mini_batch_size)\n",
    "        model.fit([y_first_trainX, y_last_trainX],trainY[batch_range],epochs=1,batch_size=batch_size)\n",
    "        #scores = model.evaluate([first_testX, last_testX],testY,verbose=0)\n",
    "        #print(\"Accuracy: %.2f%%\" %(scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1, 50)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_first_trainX.reshape(y_first_trainX.shape[0],1,y_first_trainX.shape[1]).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save this model atlhouth there is something wrong with the features\n",
    "# I did not lowercase the text :(\n",
    "model_json = model.to_json()\n",
    "with open('model-keras-w3-bigram.json','w') as f:\n",
    "    f.write(model_json)\n",
    "#save the last weight\n",
    "model.save_weights('model-keras-w3-bigram-10.h5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trans_name(name):\n",
    "    name = name.lower()\n",
    "    # transform space into underscore\n",
    "    name = '_'+name.replace(' ','_')+'_'\n",
    "    #transform the name into sequence structure\n",
    "    ext_name = extract_structure(name)\n",
    "    trans_name = transform_structure(ext_name)\n",
    "    #name_ds_mat = np.zeros((1,len(struct_dict_keys)),dtype=np.int32)\n",
    "    #for i,x in enumerate(trans_name):\n",
    "    #    name_ds_mat[0,x-1]+=1    \n",
    "    trans_name = pad_sequences([trans_name], maxlen=50,value=0.)\n",
    "    trans_name = trans_name.reshape(trans_name.shape[0],1,trans_name.shape[1])\n",
    "\n",
    "    return trans_name\n",
    "\n",
    "def predict_ethnicity(fname,lname):\n",
    "    # lower case the name\n",
    "    fnamex = trans_name(fname)\n",
    "    lnamex = trans_name(lname)\n",
    "    pred = model.predict([np.array(fnamex),np.array(lnamex)])\n",
    "    pred_class = np.argsort(pred[0])[::-1]\n",
    "    return_item = []\n",
    "    for x in np.argsort(pred[0])[::-1]:\n",
    "        return_item.append((ethnic_keys[x],pred[0][x]))\n",
    "    return return_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arab', 0.69823658),\n",
       " ('hispanic', 0.057908442),\n",
       " ('french', 0.055705793),\n",
       " ('english', 0.048168365),\n",
       " ('israeli', 0.02471127),\n",
       " ('slav', 0.021401808),\n",
       " ('italian', 0.018654032),\n",
       " ('nordic', 0.017046874),\n",
       " ('indian', 0.016055189),\n",
       " ('turkish', 0.014107338),\n",
       " ('german', 0.007924147),\n",
       " ('chinese', 0.0059029222),\n",
       " ('romanian', 0.0056299986),\n",
       " ('african', 0.0029451125),\n",
       " ('japanese', 0.0026210765),\n",
       " ('baltic', 0.00069294788),\n",
       " ('dutch', 0.00057027361),\n",
       " ('thai', 0.00046238682),\n",
       " ('vietnamese', 0.00037071199),\n",
       " ('indonesian', 0.00027243383),\n",
       " ('greek', 0.00022111264),\n",
       " ('hungarian', 0.00022082729),\n",
       " ('korean', 0.000101094),\n",
       " ('caribbean', 3.0067344e-05),\n",
       " ('mongolian', 2.4259991e-05),\n",
       " ('polynesian', 1.4907315e-05)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ethnicity('Hari','Cahyono')\n",
    "#trans_name('Nikolaus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedd the structure vocabulary using text embedding and reduce the dimensionality\n",
    "\n",
    "# convert the names into word structure vector\n",
    "struct_dict_keys = list(struct_dict.keys())\n",
    "\n",
    "def transform_structure(name_struct):\n",
    "    list_structure = []\n",
    "    for x in name_struct:\n",
    "        try:\n",
    "            list_structure.append(struct_dict_keys.index(x)+1)\n",
    "        except BaseException:\n",
    "            list_structure.append(0)\n",
    "    #add pading 0 for structure less than num_input\n",
    "    #for i in range(len(list_structure),timesteps):\n",
    "    #    list_structure.append(0)\n",
    "    return list_structure   \n",
    "    #return [*map(lambda x:struct_dict_keys.index(x)+1, name_struct)]\n",
    "\n",
    "#data_source = full_name_struct.apply(lambda x: transform_structure(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_labels(x):\n",
    "    y = np.zeros(len(ethnic_keys))\n",
    "    y[ethnic_keys.index(x)]=1\n",
    "    return y\n",
    "\n",
    "labels = np.array(list(map(lambda x: transform_labels(x),ethnic_series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using tflearn make the graph creation simple\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# separate train and training set\n",
    "trainX, testX, trainY, testY = train_test_split(data_source,[ethnic_keys.index(x) for x in ethnic_series],test_size = 0.2)\n",
    "\n",
    "trainX = pad_sequences(trainX, maxlen=50,value=0.)\n",
    "testX = pad_sequences(testX,maxlen=50,value=0.)\n",
    "# Converting labels to binary vectors\n",
    "trainY = to_categorical(trainY,nb_classes=len(ethnic_keys))\n",
    "testY = to_categorical(testY,nb_classes=len(ethnic_keys))    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pickle\n",
    "with open('train_test_fix.pickle','wb') as f:\n",
    "    pickle.dump((trainX,trainY,testX,testY,ethnic_keys,struct_dict_keys),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('train_test_fix.pickle', 'rb') as f:\n",
    "    trainX,trainY,testX,testY,ethnic_keys,struct_dict_keys = pickle.load(f)\n",
    "    #aha = pickle.load(f)\n",
    "\n",
    "#with open('traintest-smote.pickle','rb') as f:\n",
    "#    train_res,test_res = pickle.load(f)\n",
    "\n",
    "with open('ethnic_keys.pickle','rb') as f:\n",
    "    name_struct_keys,ethnic_keys = pickle.load(f)\n",
    "        \n",
    "embedding_vector_length = 1000\n",
    "lstm_layer = 1000\n",
    "max_sequence = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert categorical to binary crossentropy\n",
    "#trainY = np.array([np.where(x>0)[0][0] for x in trainY])\n",
    "#testY = np.array([np.where(x>0)[0][0] for x in testY])\n",
    "\n",
    "#test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 1000)          62696000  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 50, 1000)          3001000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               880800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 23)                4623      \n",
      "=================================================================\n",
      "Total params: 66,582,423\n",
      "Trainable params: 66,582,423\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 778s - loss: 1.6879 - acc: 0.5265    \n",
      "Accuracy: 75.88%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 796s - loss: 0.5197 - acc: 0.8582    \n",
      "Accuracy: 84.37%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 817s - loss: 0.2142 - acc: 0.9437    \n",
      "Accuracy: 85.12%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 779s - loss: 0.0989 - acc: 0.9755    \n",
      "Accuracy: 85.57%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 29131s - loss: 0.0476 - acc: 0.9905   \n",
      "Accuracy: 85.63%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 4804s - loss: 0.0258 - acc: 0.9953   \n",
      "Accuracy: 85.55%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 35875s - loss: 0.0138 - acc: 0.9980   \n",
      "Accuracy: 85.79%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 745s - loss: 0.0090 - acc: 0.9988    \n",
      "Accuracy: 85.69%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 742s - loss: 0.0068 - acc: 0.9992    \n",
      "Accuracy: 85.82%\n",
      "Epoch 1/1\n",
      "35653/35653 [==============================] - 741s - loss: 0.0048 - acc: 0.9996    \n",
      "Accuracy: 85.62%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(name_struct_keys)+1,embedding_vector_length,input_length=max_sequence))\n",
    "model.add(Conv1D(filters=embedding_vector_length,kernel_size=3,padding='same',activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=2))\n",
    "#model.add(LSTM(lstm_layer,dropout=0.8))\n",
    "model.add(Bidirectional(LSTM(max_sequence*2,return_sequences=False),input_shape=(max_sequence,1)))\n",
    "#model.add(TimeDistributed(keras.layers.Dense(len(ethnic_keys),activation='softmax')))\n",
    "model.add(keras.layers.Dense(len(ethnic_keys),activation='softmax'))\n",
    "#model.add(keras.layers.Dense(len(ethnic_keys),activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "for x in range(10):\n",
    "    model.fit(trainX,trainY,epochs=1,batch_size=1000)\n",
    "    scores = model.evaluate(testX,testY,verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" %(scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  112,  1331,  1332, ...,     0,     0,     0],\n",
       "       [  251,   252,   232, ...,     0,     0,     0],\n",
       "       [ 4633, 11731, 23123, ...,     0,     0,     0],\n",
       "       ..., \n",
       "       [  696,   697,   698, ...,     0,     0,     0],\n",
       "       [   25,    26,  5195, ...,     0,     0,     0],\n",
       "       [ 8671,  8672,  6568, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# save this model atlhouth there is something wrong with the features\n",
    "# I did not lowercase the text :(\n",
    "model_json = model.to_json()\n",
    "with open('model-keras-embed-bilstm-womaxpool.json','w') as f:\n",
    "    f.write(model_json)\n",
    "#save the last weight\n",
    "model.save_weights('model-keras-embed-bilstm-womaxpool-10.h5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "\n",
    "# compute the accuracy\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "    #print(precision)\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "    #print(recall)\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    recall = c1 / c3\n",
    "\n",
    "    return recall\n",
    "\n",
    "# load model\n",
    "# load json and create model\n",
    "json_file = open('model-keras-embed-bilstm-womaxpool.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "#                               ,custom_objects= {'f1_score': f1_score})\n",
    "loaded_model.load_weights(\"model-keras-embed-bilstm-womaxpool-10.h5\")\n",
    "\n",
    "loaded_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy',f1_score,precision,recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = loaded_model.evaluate(testX,testY,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8561812878216707, F1: 0.8601400889915978, Precision: 0.8757469038779302, Recal: 0.845523895023381\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {}, F1: {}, Precision: {}, Recal: {}'.format(scores[1],scores[2],scores[3],scores[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "\n",
    "# transform prediction\n",
    "# given name compute the prediction\n",
    "def predict_ethnicity(name):\n",
    "    # lower case the name\n",
    "    name = name.lower()\n",
    "    # transform space into underscore\n",
    "    name = '_'+name.replace(' ','_')+'_'\n",
    "    #transform the name into sequence structure\n",
    "    ext_name = extract_structure(name)\n",
    "    trans_name = transform_structure(ext_name)\n",
    "    trans_name = pad_sequences([trans_name], maxlen=50,value=0.)\n",
    "    pred = loaded_model.predict(trans_name)\n",
    "    pred_class = np.argsort(pred[0])[::-1]\n",
    "    return_item = []\n",
    "    for x in np.argsort(pred[0])[::-1]:\n",
    "        return_item.append((ethnic_keys[x],pred[0][x]))\n",
    "    return return_item\n",
    "\n",
    "name='helen lamothe'\n",
    "ext_name = extract_structure(name)\n",
    "#print(ext_name)\n",
    "trans_name = transform_structure(ext_name)\n",
    "#trans_name\n",
    "pad_sequences([trans_name], maxlen=50,value=0.)\n",
    "#extract_structure('Nikolaus Nova')\n",
    "#transform_structure('Robert Nova')\n",
    "ethnic_prob = predict_ethnicity('Filho  Elias Abdalla')\n",
    "#ethnic_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.22544670e-03,   2.45175033e-05,   5.54050894e-06,\n",
       "          3.56604069e-05,   1.31730601e-04,   1.58663862e-03,\n",
       "          9.95586514e-01,   3.65224201e-04,   4.18016425e-05,\n",
       "          1.64734403e-04,   3.29397808e-05,   1.42851750e-05,\n",
       "          5.77516516e-07,   3.13429664e-05,   1.98912196e-04,\n",
       "          5.65968139e-06,   4.63458673e-06,   4.45792568e-04,\n",
       "          5.28864875e-05,   2.15678101e-05,   8.53615438e-06,\n",
       "          4.57901763e-07,   1.44505730e-05]], dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = loaded_model.predict(trainX[10].reshape(1,50))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  5,  0, 17,  7, 14,  9,  4, 18,  8,  3, 10, 13,  1, 19, 22, 11,\n",
       "       20, 15,  2, 16, 12, 21])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(test)[0][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6]),)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(trainY[10]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,  2304,  2305,     0,     0,     0,     0,\n",
       "         4088, 15559,  2523, 15102,     0,     0,     0,    24,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name='_helen__lamothe_'\n",
    "ext_name = extract_structure(name)\n",
    "#print(ext_name)\n",
    "trans_name = transform_structure(ext_name)\n",
    "#trans_name\n",
    "pad_sequences([trans_name], maxlen=50,value=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ITALIAN', 0.52735686),\n",
       " ('INDIAN', 0.30269179),\n",
       " ('KOREAN', 0.097788125),\n",
       " ('JAPANESE', 0.017470013),\n",
       " ('ROMANIAN', 0.015210837),\n",
       " ('TURKISH', 0.014220745),\n",
       " ('HISPANIC', 0.0092350421),\n",
       " ('GERMAN', 0.0046770978),\n",
       " ('BALTIC', 0.0032188322),\n",
       " ('ARAB', 0.0028918688),\n",
       " ('ISRAELI', 0.0022467086),\n",
       " ('GREEK', 0.0014537659),\n",
       " ('SLAV', 0.00046977124),\n",
       " ('NORDIC', 0.00024971511),\n",
       " ('HUNGARIAN', 0.0002173665),\n",
       " ('DUTCH', 0.00021363674),\n",
       " ('INDONESIAN', 0.00019762212),\n",
       " ('VIETNAMESE', 9.9613972e-05),\n",
       " ('AFRICAN', 3.4369114e-05),\n",
       " ('CHINESE', 3.3702971e-05),\n",
       " ('ENGLISH', 1.7328795e-05),\n",
       " ('FRENCH', 4.1550811e-06),\n",
       " ('THAI', 9.4588233e-07)]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ethnicity('harry potter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ISRAELI'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethnic_keys[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           _Elias_Abdalla__Filho_HISPANIC\n",
       "1                                  _Jad__Bou_Abdallah_ARAB\n",
       "2                                  _Ayman__Abdel_Aziz_ARAB\n",
       "3                                  _Salma__Abdelmoula_ARAB\n",
       "4                                     _Ibrahim__Abdou_ARAB\n",
       "5                             _Hazem__Abou_El_Fettouh_ARAB\n",
       "6                                     _Rola__Aboutaam_ARAB\n",
       "7        _Aida_Alexandra__Alvim_de_Abreu_Silva_Rodrigue...\n",
       "8             _Isabel__Cristina_Affonso_Scaletsky_HISPANIC\n",
       "9                                _Tsiri__Agbenyega_AFRICAN\n",
       "10                     _Jose__Maria_Aguado_Garcia_HISPANIC\n",
       "11                      _Manuela__Aguilar_Guisado_HISPANIC\n",
       "12                         _Arturo__Aguillon_Luna_HISPANIC\n",
       "13                                   _Ali__Ahmadzadeh_ARAB\n",
       "14                                    _Ahmed__Ibrahim_ARAB\n",
       "15                                    _Ahmed__Letaief_ARAB\n",
       "16                           _Mohammed__Shakeel_Ahmed_ARAB\n",
       "17                              _Alev__Aksoy_Dogan_TURKISH\n",
       "18                    _Mitsumi__Nakatsukasa_Akune_JAPANESE\n",
       "19                                   _Filiz__Akyuz_TURKISH\n",
       "20                                _Omaimah__Al_Gohary_ARAB\n",
       "21                                  _Jassim__Al_Jedah_ARAB\n",
       "22                                  _Shadi__Al_Khatib_ARAB\n",
       "23                                   _Azmi__Al_Najjar_ARAB\n",
       "24                           _Emilio__Alba_Conejo_HISPANIC\n",
       "25                     _Sheila_Alcantara__Llaguno_HISPANIC\n",
       "26                               _Marcos__Alcocer_HISPANIC\n",
       "27                       _Maria__Grazia_Alessandri_ITALIAN\n",
       "28                                  _Randa__Ali_Labib_ARAB\n",
       "29                        _Shameez__Allie_Hamdulay_AFRICAN\n",
       "                               ...                        \n",
       "44537                                _Zhihong__Liu_CHINESE\n",
       "44538                              _Zhen_ying__Liu_CHINESE\n",
       "44539                                    _Zhu__Liu_CHINESE\n",
       "44540                               _Zhengjie__Liu_CHINESE\n",
       "44541                              _Zhengchun__Liu_CHINESE\n",
       "44542                                _Ziliang__Liu_CHINESE\n",
       "44543                                    _Zhe__Liu_CHINESE\n",
       "44544                                 _Zulian__Liu_CHINESE\n",
       "44545                               _Zun_chun__Liu_CHINESE\n",
       "44546                                _Ze_zhou__Liu_CHINESE\n",
       "44547                               _Zhaochun__Liu_CHINESE\n",
       "44548                                 _ziying__Liu_CHINESE\n",
       "44549                                 _zhenyu__Liu_CHINESE\n",
       "44550                              _kenzo__Tanaka_JAPANESE\n",
       "44551                             _kimiko__Tanaka_JAPANESE\n",
       "44552                           _kazuhiro__Tanaka_JAPANESE\n",
       "44553                               _koji__Tanaka_JAPANESE\n",
       "44554                             _Kousei__Tanaka_JAPANESE\n",
       "44555                              _Kyoko__Tanaka_JAPANESE\n",
       "44556                              _Kenji__Tanaka_JAPANESE\n",
       "44557                              _keiji__Tanaka_JAPANESE\n",
       "44558                              _keiji__Tanaka_JAPANESE\n",
       "44559                                _Kei__Tanaka_JAPANESE\n",
       "44560                            _kiyoshi__Tanaka_JAPANESE\n",
       "44561                            _takashi__Sasaki_JAPANESE\n",
       "44562                            _toyoshi__Sasaki_JAPANESE\n",
       "44563                             _tetsuo__Sasaki_JAPANESE\n",
       "44564                             _takayo__Sasaki_JAPANESE\n",
       "44565                                    _may__Haddad_ARAB\n",
       "44566                             _Marianne__Haddad_FRENCH\n",
       "Length: 44567, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethnea_df['First']+ethnea_df['Last']+ethnea_df['Ethnea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
